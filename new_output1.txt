


Command: attack3.py --dataset sst2 --split test --loss cos --n_inputs 100 -b 1 --coeff_perplexity 0.2 --coeff_reg 1 --lr 0.01 --lr_decay 0.89 --bert_path /hdd1/jianwei/workspace/lamp/models/bert-base-finetuned-sst2 --n_steps 500 





Attacking..

Running input #0 of 100.
reference: 
========================
slightly disappointed 
========================
*********************************
*********************************
average of cosine similarity 0.9992715259422633
highest_index [0]
highest [0.9992715259422633]
Debug: ids_shape = 4, pads = [4]
Debug: input ids = tensor([[ 101, 3621, 9364,  102]], device='cuda:0')
Debug: ref = ['[CLS] slightly disappointed [SEP]']
[Init] best rec loss: 1.9897326231002808 for ['[CLS] study showing [SEP]']
[Init] best rec loss: 1.6931339502334595 for ['[CLS]wall same [SEP]']
[Init] best rec loss: 1.5862441062927246 for ['[CLS] kennedy west [SEP]']
[Init] best rec loss: 1.5641170740127563 for ['[CLS] never suspension [SEP]']
[Init] best rec loss: 1.3241164684295654 for ['[CLS] panel officer [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.590 (perp=11.621, rec=0.255, cos=0.011), tot_loss_proj:2.656 [t=0.30s]
prediction: ['[CLS] severe disappointed [SEP]']
Attempt swap
[ 100/ 500] tot_loss=2.228 (perp=10.251, rec=0.174, cos=0.004), tot_loss_proj:2.113 [t=0.31s]
prediction: ['[CLS] slightly disappointed [SEP]']
[ 150/ 500] tot_loss=2.152 (perp=10.251, rec=0.101, cos=0.001), tot_loss_proj:2.117 [t=0.29s]
prediction: ['[CLS] slightly disappointed [SEP]']
Attempt swap
[ 200/ 500] tot_loss=2.128 (perp=10.251, rec=0.077, cos=0.001), tot_loss_proj:2.124 [t=0.26s]
prediction: ['[CLS] slightly disappointed [SEP]']
Attempt swap
[ 250/ 500] tot_loss=2.120 (perp=10.251, rec=0.069, cos=0.001), tot_loss_proj:2.114 [t=0.26s]
prediction: ['[CLS] slightly disappointed [SEP]']
[ 300/ 500] tot_loss=2.121 (perp=10.251, rec=0.071, cos=0.001), tot_loss_proj:2.109 [t=0.26s]
prediction: ['[CLS] slightly disappointed [SEP]']
Attempt swap
[ 350/ 500] tot_loss=2.120 (perp=10.251, rec=0.069, cos=0.001), tot_loss_proj:2.116 [t=0.26s]
prediction: ['[CLS] slightly disappointed [SEP]']
Attempt swap
[ 400/ 500] tot_loss=2.117 (perp=10.251, rec=0.066, cos=0.001), tot_loss_proj:2.107 [t=0.25s]
prediction: ['[CLS] slightly disappointed [SEP]']
[ 450/ 500] tot_loss=2.114 (perp=10.251, rec=0.063, cos=0.001), tot_loss_proj:2.128 [t=0.26s]
prediction: ['[CLS] slightly disappointed [SEP]']
Attempt swap
[ 500/ 500] tot_loss=2.100 (perp=10.251, rec=0.049, cos=0.001), tot_loss_proj:2.123 [t=0.26s]
prediction: ['[CLS] slightly disappointed [SEP]']
Done with input #0 of 100.
reference: 
========================
[CLS] slightly disappointed [SEP]
========================
predicted: 
========================
[CLS] slightly disappointed [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

input #0 time: 0:03:35 | total time: 0:03:35


Running input #1 of 100.
reference: 
========================
splendidly 
========================
*********************************
*********************************
average of cosine similarity 0.9993548278348494
highest_index [0]
highest [0.9993548278348494]
Debug: ids_shape = 4, pads = [4]
Debug: input ids = tensor([[  101, 21459,  2135,   102]], device='cuda:0')
Debug: ref = ['[CLS] splendidly [SEP]']
[Init] best rec loss: 1.8751615285873413 for ['[CLS] church sat [SEP]']
[Init] best rec loss: 1.8574330806732178 for ['[CLS] collectiontail [SEP]']
[Init] best rec loss: 1.8238334655761719 for ['[CLS] football package [SEP]']
[Init] best rec loss: 1.7409363985061646 for ['[CLS] cas giants [SEP]']
[Init] best rec loss: 1.6608680486679077 for ['[CLS] conducted predator [SEP]']
[Init] best rec loss: 1.6287647485733032 for ['[CLS] j native [SEP]']
[Init] best rec loss: 1.5730043649673462 for ['[CLS] dated glazed [SEP]']
[Init] best rec loss: 1.5161365270614624 for ['[CLS] there correspondent [SEP]']
[Init] best rec loss: 1.271039366722107 for ['[CLS] finally relative [SEP]']
[Init] best rec loss: 1.120924472808838 for ['[CLS] course characters [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.319 (perp=10.543, rec=0.208, cos=0.002), tot_loss_proj:2.459 [t=0.25s]
prediction: ['[CLS] splendid splendid [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=1.931 (perp=9.171, rec=0.095, cos=0.001), tot_loss_proj:1.903 [t=0.26s]
prediction: ['[CLS] splendidly [SEP]']
[ 150/ 500] tot_loss=1.906 (perp=9.171, rec=0.070, cos=0.001), tot_loss_proj:1.903 [t=0.27s]
prediction: ['[CLS] splendidly [SEP]']
Attempt swap
[ 200/ 500] tot_loss=1.898 (perp=9.171, rec=0.062, cos=0.001), tot_loss_proj:1.894 [t=0.25s]
prediction: ['[CLS] splendidly [SEP]']
Attempt swap
[ 250/ 500] tot_loss=1.894 (perp=9.171, rec=0.058, cos=0.001), tot_loss_proj:1.885 [t=0.26s]
prediction: ['[CLS] splendidly [SEP]']
[ 300/ 500] tot_loss=1.893 (perp=9.171, rec=0.057, cos=0.001), tot_loss_proj:1.899 [t=0.26s]
prediction: ['[CLS] splendidly [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.899 (perp=9.171, rec=0.063, cos=0.001), tot_loss_proj:1.913 [t=0.25s]
prediction: ['[CLS] splendidly [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.896 (perp=9.171, rec=0.061, cos=0.001), tot_loss_proj:1.897 [t=0.27s]
prediction: ['[CLS] splendidly [SEP]']
[ 450/ 500] tot_loss=1.902 (perp=9.171, rec=0.067, cos=0.001), tot_loss_proj:1.894 [t=0.25s]
prediction: ['[CLS] splendidly [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.893 (perp=9.171, rec=0.058, cos=0.001), tot_loss_proj:1.906 [t=0.29s]
prediction: ['[CLS] splendidly [SEP]']
Done with input #1 of 100.
reference: 
========================
[CLS] splendidly [SEP]
========================
predicted: 
========================
[CLS] splendidly [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

input #1 time: 0:03:27 | total time: 0:07:03


Running input #2 of 100.
reference: 
========================
gaining much momentum 
========================
*********************************
*********************************
average of cosine similarity 0.9994340582458305
highest_index [0]
highest [0.9994340582458305]
Debug: ids_shape = 5, pads = [5]
Debug: input ids = tensor([[  101,  8550,  2172, 11071,   102]], device='cuda:0')
Debug: ref = ['[CLS] gaining much momentum [SEP]']
[Init] best rec loss: 1.3203822374343872 for ['[CLS] wash〜 at [SEP]']
[Init] best rec loss: 1.2476643323898315 for ['[CLS] we working would [SEP]']
[Init] best perm rec loss: 1.2300113439559937 for ['[CLS] we would working [SEP]']
[Init] best perm rec loss: 1.2278040647506714 for ['[CLS] would we working [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.585 (perp=10.605, rec=0.443, cos=0.021), tot_loss_proj:2.863 [t=0.25s]
prediction: ['[CLS] very momentum growth [SEP]']
Attempt swap
Put prefix at the end
[ 100/ 500] tot_loss=1.907 (perp=8.515, rec=0.200, cos=0.004), tot_loss_proj:1.800 [t=0.26s]
prediction: ['[CLS] gaining much momentum [SEP]']
[ 150/ 500] tot_loss=1.802 (perp=8.515, rec=0.098, cos=0.001), tot_loss_proj:1.782 [t=0.26s]
prediction: ['[CLS] gaining much momentum [SEP]']
Attempt swap
[ 200/ 500] tot_loss=1.794 (perp=8.515, rec=0.090, cos=0.001), tot_loss_proj:1.794 [t=0.28s]
prediction: ['[CLS] gaining much momentum [SEP]']
Attempt swap
[ 250/ 500] tot_loss=1.782 (perp=8.515, rec=0.079, cos=0.001), tot_loss_proj:1.790 [t=0.26s]
prediction: ['[CLS] gaining much momentum [SEP]']
[ 300/ 500] tot_loss=1.776 (perp=8.515, rec=0.073, cos=0.001), tot_loss_proj:1.795 [t=0.25s]
prediction: ['[CLS] gaining much momentum [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.770 (perp=8.515, rec=0.066, cos=0.001), tot_loss_proj:1.798 [t=0.27s]
prediction: ['[CLS] gaining much momentum [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.767 (perp=8.515, rec=0.063, cos=0.001), tot_loss_proj:1.804 [t=0.26s]
prediction: ['[CLS] gaining much momentum [SEP]']
[ 450/ 500] tot_loss=1.777 (perp=8.515, rec=0.074, cos=0.001), tot_loss_proj:1.787 [t=0.27s]
prediction: ['[CLS] gaining much momentum [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.772 (perp=8.515, rec=0.068, cos=0.001), tot_loss_proj:1.795 [t=0.26s]
prediction: ['[CLS] gaining much momentum [SEP]']
Done with input #2 of 100.
reference: 
========================
[CLS] gaining much momentum [SEP]
========================
predicted: 
========================
[CLS] gaining much momentum [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

input #2 time: 0:03:28 | total time: 0:10:31


Running input #3 of 100.
reference: 
========================
flawless film 
========================
*********************************
*********************************
average of cosine similarity 0.9993001481006445
highest_index [0]
highest [0.9993001481006445]
Debug: ids_shape = 4, pads = [4]
Debug: input ids = tensor([[  101, 27503,  2143,   102]], device='cuda:0')
Debug: ref = ['[CLS] flawless film [SEP]']
[Init] best rec loss: 1.7515515089035034 for ['[CLS] fund integration [SEP]']
[Init] best rec loss: 1.515498399734497 for ['[CLS] pot reaction [SEP]']
[Init] best rec loss: 1.510279893875122 for ['[CLS] committee deportivo [SEP]']
[Init] best rec loss: 1.5031687021255493 for ['[CLS] carry including [SEP]']
[Init] best rec loss: 1.4859191179275513 for ['[CLS] has block [SEP]']
[Init] best rec loss: 1.312357783317566 for ['[CLS] anton laughed [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.664 (perp=11.851, rec=0.286, cos=0.008), tot_loss_proj:2.812 [t=0.27s]
prediction: ['[CLS] flawless successful [SEP]']
Attempt swap
Moved token
[ 100/ 500] tot_loss=2.293 (perp=10.476, rec=0.195, cos=0.003), tot_loss_proj:2.451 [t=0.25s]
prediction: ['[CLS] flawless flawless [SEP]']
[ 150/ 500] tot_loss=1.768 (perp=8.385, rec=0.090, cos=0.001), tot_loss_proj:1.760 [t=0.27s]
prediction: ['[CLS] flawless film [SEP]']
Attempt swap
[ 200/ 500] tot_loss=1.741 (perp=8.385, rec=0.063, cos=0.001), tot_loss_proj:1.759 [t=0.29s]
prediction: ['[CLS] flawless film [SEP]']
Attempt swap
[ 250/ 500] tot_loss=1.742 (perp=8.385, rec=0.064, cos=0.001), tot_loss_proj:1.755 [t=0.27s]
prediction: ['[CLS] flawless film [SEP]']
[ 300/ 500] tot_loss=1.745 (perp=8.385, rec=0.067, cos=0.001), tot_loss_proj:1.745 [t=0.27s]
prediction: ['[CLS] flawless film [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.742 (perp=8.385, rec=0.064, cos=0.001), tot_loss_proj:1.754 [t=0.27s]
prediction: ['[CLS] flawless film [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.737 (perp=8.385, rec=0.059, cos=0.001), tot_loss_proj:1.762 [t=0.28s]
prediction: ['[CLS] flawless film [SEP]']
[ 450/ 500] tot_loss=1.739 (perp=8.385, rec=0.061, cos=0.001), tot_loss_proj:1.757 [t=0.27s]
prediction: ['[CLS] flawless film [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.743 (perp=8.385, rec=0.065, cos=0.001), tot_loss_proj:1.750 [t=0.28s]
prediction: ['[CLS] flawless film [SEP]']
Done with input #3 of 100.
reference: 
========================
[CLS] flawless film [SEP]
========================
predicted: 
========================
[CLS] flawless film [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

input #3 time: 0:03:32 | total time: 0:14:03


Running input #4 of 100.
reference: 
========================
tiresomely 
========================
*********************************
*********************************
average of cosine similarity 0.9992301171062454
highest_index [0]
highest [0.9992301171062454]
Debug: ids_shape = 5, pads = [5]
Debug: input ids = tensor([[  101, 13310,  8462,  2135,   102]], device='cuda:0')
Debug: ref = ['[CLS] tiresomely [SEP]']
[Init] best rec loss: 1.8612022399902344 for ['[CLS] before departmentssh [SEP]']
[Init] best rec loss: 1.8395576477050781 for ['[CLS] stay squeak mean [SEP]']
[Init] best rec loss: 1.7808197736740112 for ['[CLS] watch joint weekly [SEP]']
[Init] best rec loss: 1.7083849906921387 for ['[CLS] religious tip seat [SEP]']
[Init] best rec loss: 1.4955319166183472 for ['[CLS] differenceparts bad [SEP]']
[Init] best rec loss: 1.2935547828674316 for ['[CLS] fatedss jack [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.807 (perp=12.580, rec=0.283, cos=0.008), tot_loss_proj:3.146 [t=0.28s]
prediction: ['[CLS] tires uglyistle [SEP]']
Attempt swap
Put prefix at the end
[ 100/ 500] tot_loss=1.838 (perp=8.428, rec=0.150, cos=0.003), tot_loss_proj:1.877 [t=0.27s]
prediction: ['[CLS]ly tiresome [SEP]']
[ 150/ 500] tot_loss=1.782 (perp=8.428, rec=0.096, cos=0.001), tot_loss_proj:1.886 [t=0.29s]
prediction: ['[CLS]ly tiresome [SEP]']
Attempt swap
Moved token
[ 200/ 500] tot_loss=1.586 (perp=7.516, rec=0.082, cos=0.001), tot_loss_proj:1.575 [t=0.26s]
prediction: ['[CLS] tiresomely [SEP]']
Attempt swap
[ 250/ 500] tot_loss=1.573 (perp=7.516, rec=0.069, cos=0.001), tot_loss_proj:1.572 [t=0.27s]
prediction: ['[CLS] tiresomely [SEP]']
[ 300/ 500] tot_loss=1.565 (perp=7.516, rec=0.061, cos=0.001), tot_loss_proj:1.561 [t=0.28s]
prediction: ['[CLS] tiresomely [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.574 (perp=7.516, rec=0.070, cos=0.001), tot_loss_proj:1.572 [t=0.27s]
prediction: ['[CLS] tiresomely [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.569 (perp=7.516, rec=0.065, cos=0.001), tot_loss_proj:1.577 [t=0.28s]
prediction: ['[CLS] tiresomely [SEP]']
[ 450/ 500] tot_loss=1.561 (perp=7.516, rec=0.057, cos=0.001), tot_loss_proj:1.574 [t=0.28s]
prediction: ['[CLS] tiresomely [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.565 (perp=7.516, rec=0.061, cos=0.001), tot_loss_proj:1.572 [t=0.25s]
prediction: ['[CLS] tiresomely [SEP]']
Done with input #4 of 100.
reference: 
========================
[CLS] tiresomely [SEP]
========================
predicted: 
========================
[CLS] tiresomely [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

input #4 time: 0:03:33 | total time: 0:17:36


Running input #5 of 100.
reference: 
========================
enjoyable ease 
========================
*********************************
*********************************
average of cosine similarity 0.9993649146105028
highest_index [0]
highest [0.9993649146105028]
Debug: ids_shape = 4, pads = [4]
Debug: input ids = tensor([[  101, 22249,  7496,   102]], device='cuda:0')
Debug: ref = ['[CLS] enjoyable ease [SEP]']
[Init] best rec loss: 1.7795944213867188 for ['[CLS] pays country [SEP]']
[Init] best rec loss: 1.6453757286071777 for ['[CLS] works flow [SEP]']
[Init] best rec loss: 1.6053582429885864 for ['[CLS] sol liked [SEP]']
[Init] best rec loss: 1.5769269466400146 for ['[CLS] em madame [SEP]']
[Init] best rec loss: 1.4209589958190918 for ['[CLS] eye central [SEP]']
[Init] best perm rec loss: 1.41153085231781 for ['[CLS] central eye [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=3.349 (perp=15.495, rec=0.244, cos=0.006), tot_loss_proj:4.873 [t=0.30s]
prediction: ['[CLS] cy ease [SEP]']
Attempt swap
Put prefix at the end
[ 100/ 500] tot_loss=2.053 (perp=9.390, rec=0.172, cos=0.003), tot_loss_proj:3.411 [t=0.28s]
prediction: ['[CLS] ease industries [SEP]']
[ 150/ 500] tot_loss=2.174 (perp=10.180, rec=0.135, cos=0.003), tot_loss_proj:3.370 [t=0.25s]
prediction: ['[CLS] ease remix [SEP]']
Attempt swap
[ 200/ 500] tot_loss=2.171 (perp=10.180, rec=0.132, cos=0.003), tot_loss_proj:3.375 [t=0.25s]
prediction: ['[CLS] ease remix [SEP]']
Attempt swap
[ 250/ 500] tot_loss=2.167 (perp=10.180, rec=0.128, cos=0.002), tot_loss_proj:3.377 [t=0.26s]
prediction: ['[CLS] ease remix [SEP]']
[ 300/ 500] tot_loss=2.276 (perp=10.724, rec=0.129, cos=0.002), tot_loss_proj:3.398 [t=0.25s]
prediction: ['[CLS] ease tales [SEP]']
Attempt swap
[ 350/ 500] tot_loss=2.268 (perp=10.724, rec=0.121, cos=0.002), tot_loss_proj:3.398 [t=0.25s]
prediction: ['[CLS] ease tales [SEP]']
Attempt swap
[ 400/ 500] tot_loss=2.480 (perp=11.854, rec=0.107, cos=0.002), tot_loss_proj:2.565 [t=0.26s]
prediction: ['[CLS] ease enjoyable [SEP]']
[ 450/ 500] tot_loss=2.500 (perp=11.854, rec=0.127, cos=0.002), tot_loss_proj:2.570 [t=0.26s]
prediction: ['[CLS] ease enjoyable [SEP]']
Attempt swap
[ 500/ 500] tot_loss=2.485 (perp=11.854, rec=0.112, cos=0.002), tot_loss_proj:2.570 [t=0.25s]
prediction: ['[CLS] ease enjoyable [SEP]']
Done with input #5 of 100.
reference: 
========================
[CLS] enjoyable ease [SEP]
========================
predicted: 
========================
[CLS] ease enjoyable [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 0.000 | p: 0.000 | r: 0.000
rougeL     | fm: 75.000 | p: 75.000 | r: 75.000
rougeLsum  | fm: 75.000 | p: 75.000 | r: 75.000
r1fm+r2fm = 100.000

[Aggregate metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 83.333 | p: 83.333 | r: 83.333
rougeL     | fm: 95.833 | p: 95.833 | r: 95.833
rougeLsum  | fm: 95.833 | p: 95.833 | r: 95.833
r1fm+r2fm = 183.333

input #5 time: 0:03:29 | total time: 0:21:06


Running input #6 of 100.
reference: 
========================
grayish 
========================
*********************************
*********************************
average of cosine similarity 0.9992504694447222
highest_index [0]
highest [0.9992504694447222]
Debug: ids_shape = 4, pads = [4]
Debug: input ids = tensor([[ 101, 3897, 4509,  102]], device='cuda:0')
Debug: ref = ['[CLS] grayish [SEP]']
[Init] best rec loss: 1.902685284614563 for ['[CLS] cassidyiot [SEP]']
[Init] best rec loss: 1.7486958503723145 for ['[CLS] values ballad [SEP]']
[Init] best rec loss: 1.7067688703536987 for ['[CLS] gifted frequently [SEP]']
[Init] best rec loss: 1.5019043684005737 for ['[CLS] handed canteen [SEP]']
[Init] best rec loss: 1.2771662473678589 for ['[CLS] air little [SEP]']
[Init] best rec loss: 1.2549055814743042 for ['[CLS] just endemic [SEP]']
[Init] best rec loss: 1.249065637588501 for ['[CLS] brooklyn darren [SEP]']
[Init] best rec loss: 1.124966025352478 for ['[CLS] double deep [SEP]']
[Init] best rec loss: 1.1203830242156982 for ['[CLS] too u2 [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.093 (perp=8.874, rec=0.306, cos=0.012), tot_loss_proj:2.841 [t=0.29s]
prediction: ['[CLS] evil gray [SEP]']
Attempt swap
[ 100/ 500] tot_loss=1.578 (perp=6.813, rec=0.212, cos=0.004), tot_loss_proj:2.720 [t=0.30s]
prediction: ['[CLS] gray gray [SEP]']
[ 150/ 500] tot_loss=1.557 (perp=6.813, rec=0.191, cos=0.003), tot_loss_proj:2.716 [t=0.29s]
prediction: ['[CLS] gray gray [SEP]']
Attempt swap
[ 200/ 500] tot_loss=2.218 (perp=10.460, rec=0.124, cos=0.002), tot_loss_proj:2.468 [t=0.30s]
prediction: ['[CLS]ish gray [SEP]']
Attempt swap
Moved token
[ 250/ 500] tot_loss=1.726 (perp=8.089, rec=0.107, cos=0.001), tot_loss_proj:1.693 [t=0.29s]
prediction: ['[CLS] grayish [SEP]']
[ 300/ 500] tot_loss=1.702 (perp=8.089, rec=0.084, cos=0.001), tot_loss_proj:1.708 [t=0.29s]
prediction: ['[CLS] grayish [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.677 (perp=8.089, rec=0.058, cos=0.001), tot_loss_proj:1.699 [t=0.30s]
prediction: ['[CLS] grayish [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.688 (perp=8.089, rec=0.069, cos=0.001), tot_loss_proj:1.683 [t=0.29s]
prediction: ['[CLS] grayish [SEP]']
[ 450/ 500] tot_loss=1.676 (perp=8.089, rec=0.057, cos=0.001), tot_loss_proj:1.690 [t=0.30s]
prediction: ['[CLS] grayish [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.698 (perp=8.089, rec=0.079, cos=0.001), tot_loss_proj:1.685 [t=0.29s]
prediction: ['[CLS] grayish [SEP]']
Done with input #6 of 100.
reference: 
========================
[CLS] grayish [SEP]
========================
predicted: 
========================
[CLS] grayish [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 85.714 | p: 85.714 | r: 85.714
rougeL     | fm: 96.429 | p: 96.429 | r: 96.429
rougeLsum  | fm: 96.429 | p: 96.429 | r: 96.429
r1fm+r2fm = 185.714

input #6 time: 0:03:56 | total time: 0:25:02


Running input #7 of 100.
reference: 
========================
no cute factor here ... not that i mind ugly ; the problem is he has no character , loveable or otherwise . 
========================
*********************************
*********************************
average of cosine similarity 0.9991768686555791
highest_index [0]
highest [0.9991768686555791]
Debug: ids_shape = 28, pads = [28]
Debug: input ids = tensor([[  101,  2053, 10140,  5387,  2182,  1012,  1012,  1012,  2025,  2008,
          1045,  2568,  9200,  1025,  1996,  3291,  2003,  2002,  2038,  2053,
          2839,  1010,  2293,  3085,  2030,  4728,  1012,   102]],
       device='cuda:0')
Debug: ref = ['[CLS] no cute factor here... not that i mind ugly ; the problem is he has no character, loveable or otherwise. [SEP]']
[Init] best rec loss: 1.8152823448181152 for ['[CLS] emperor show lowry speechded onesgrin paste connection event complex fair signsuously sparkling comedycarriage each death channels cross sooner dayuo petals clan [SEP]']
[Init] best rec loss: 1.3960720300674438 for ['[CLS] pilotden split cherokeecarriage minutephy runaway generation ruth episode grandmaster reddy mode disguise usual driveway par snowbound class parallel pouring mira rifles appeal [SEP]']
[Init] best rec loss: 1.3859511613845825 for ['[CLS] vance golf belt handyolved fl researchtium anonymousina me man murphyoof bearing zetavocationtellrti american autopsy that lie amongₑ free [SEP]']
[Init] best rec loss: 1.2355339527130127 for ['[CLS]nies pacific disease life animal alec none mukherjee moffat on consisting ran battalionzed gold depending 17th blow classics career main manual madagascar tourismide sony [SEP]']
[Init] best rec loss: 1.1908916234970093 for ['[CLS] cod dock # openר sat both grande flow hs most purpose baby beings comppia jenny infants part end pay exactly conference moths median [SEP]']
[Init] best perm rec loss: 1.1874291896820068 for ['[CLS] cod conference dock moths most com baby open hs median purpose flow exactly part end bothp jennypia # sat infants payר beings grande [SEP]']
[Init] best perm rec loss: 1.1840190887451172 for ['[CLS] cod pay part mothsר most hs baby infants grande both sat dockpia beings exactly purpose flow end conference comp jenny open median # [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.913 (perp=12.360, rec=0.438, cos=0.003), tot_loss_proj:3.374 [t=0.28s]
prediction: ['[CLS] employee poor problem prison problem no losernoless worst. problems maybe loserlike terrorist excuse fucking walls enforcementpor.no shit shit hiv [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.404 (perp=10.267, rec=0.347, cos=0.004), tot_loss_proj:3.023 [t=0.26s]
prediction: ['[CLS] no poor problem prison problem no problemno ugly nothing. ignore maybe ugly seems nor. any typesopor. problem homo or character [SEP]']
[ 150/ 500] tot_loss=2.221 (perp=9.650, rec=0.289, cos=0.002), tot_loss_proj:2.837 [t=0.26s]
prediction: ['[CLS] no poor problem military problem no problem ugly ugly nothing. ignore aka ugly seems nor. about ortchpor. problem bug or character [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.166 (perp=9.385, rec=0.287, cos=0.002), tot_loss_proj:2.954 [t=0.28s]
prediction: ['[CLS] no poor problem military problem no crisis pretty ugly nothing. popularity caring ugly no gr seems any character endangeredes. problem sex or character [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=2.142 (perp=9.486, rec=0.243, cos=0.001), tot_loss_proj:3.406 [t=0.25s]
prediction: ['[CLS] no wrong problem military problem noam no ugly nothing. has caring ugly no problem seems any character least org. character boo is character [SEP]']
[ 300/ 500] tot_loss=2.250 (perp=10.145, rec=0.221, cos=0.000), tot_loss_proj:3.558 [t=0.26s]
prediction: ['[CLS] no wrong problemckle problem everyam no ugly nothing. is caring he no problem characters was characterwashedes. or becca is character [SEP]']
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=2.045 (perp=9.182, rec=0.207, cos=0.001), tot_loss_proj:3.709 [t=0.25s]
prediction: ['[CLS] no wrong problemckle not is no no ugly nothing. is caring he no problem characters is character want or. or boo has character [SEP]']
Attempt swap
Moved sequence
[ 400/ 500] tot_loss=2.005 (perp=9.074, rec=0.190, cos=0.000), tot_loss_proj:3.580 [t=0.26s]
prediction: ['[CLS] has wrong problem vampires not is no not ugly caring nothing. ; he no problem characters is character want ;. or boo has character [SEP]']
[ 450/ 500] tot_loss=1.955 (perp=8.887, rec=0.177, cos=0.000), tot_loss_proj:3.489 [t=0.26s]
prediction: ['[CLS] has wrong problem vampires no is no mind ugly love nothing. ; he no problem characters is characterable ;. or boo is character [SEP]']
Attempt swap
Swapped tokens
[ 500/ 500] tot_loss=1.820 (perp=8.292, rec=0.161, cos=0.000), tot_loss_proj:3.526 [t=0.27s]
prediction: ['[CLS] has no problem vampires wrong is no mind ugly love nothing. ; he no problem characters is characterable ;. or boo is character [SEP]']
Done with input #7 of 100.
reference: 
========================
[CLS] no cute factor here... not that i mind ugly ; the problem is he has no character, loveable or otherwise. [SEP]
========================
predicted: 
========================
[CLS] has no problem vampires wrong is no mind ugly love nothing. ; he no problem characters is characterable ;. or boo is character [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 54.545 | p: 52.174 | r: 57.143
rouge2     | fm: 9.524 | p: 9.091 | r: 10.000
rougeL     | fm: 36.364 | p: 34.783 | r: 38.095
rougeLsum  | fm: 36.364 | p: 34.783 | r: 38.095
r1fm+r2fm = 64.069

[Aggregate metrics]:
rouge1     | fm: 94.318 | p: 94.022 | r: 94.643
rouge2     | fm: 76.190 | p: 76.136 | r: 76.250
rougeL     | fm: 88.920 | p: 88.723 | r: 89.137
rougeLsum  | fm: 88.920 | p: 88.723 | r: 89.137
r1fm+r2fm = 170.509

input #7 time: 0:03:29 | total time: 0:28:32


Running input #8 of 100.
reference: 
========================
's a frightful vanity film that , no doubt , pays off what debt miramax felt they owed to benigni 
========================
*********************************
*********************************
average of cosine similarity 0.9994032830483026
highest_index [0]
highest [0.9994032830483026]
Debug: ids_shape = 26, pads = [26]
Debug: input ids = tensor([[  101,  1005,  1055,  1037, 25966,  3993, 18736,  2143,  2008,  1010,
          2053,  4797,  1010, 12778,  2125,  2054,  7016, 18062, 17848,  2371,
          2027, 12232,  2000, 28378,  2072,   102]], device='cuda:0')
Debug: ref = ["[CLS]'s a frightful vanity film that, no doubt, pays off what debt miramax felt they owed to benigni [SEP]"]
[Init] best rec loss: 1.3184598684310913 for ['[CLS] challenges pa over computer taught lea eye clubs their paris alloy table currently fitting southern bargain luxury learning cinema those band gr across passage [SEP]']
[Init] best rec loss: 1.3084146976470947 for ['[CLS] thunder edge tender youngaz 505acality forced cheers bed lochley concacaf pm some commercial eliot advocate flow bubble habits ease bless [SEP]']
[Init] best rec loss: 1.29958176612854 for ['[CLS] slate conducted maps charlierocity these leaked too count search cody misery cannon vet vote prior foreign. ratio atm boys trafficrating 1st [SEP]']
[Init] best rec loss: 1.2899292707443237 for ['[CLS] descent caughtway holiday riding fist stages died pi focusedョeis anywhere half vine tarzan center sunlight away broughteda held reserved website [SEP]']
[Init] best perm rec loss: 1.2864528894424438 for ['[CLS] reserved focused away stages riding tarzanway caught center anywhere brought pi sunlight website holiday fist half held diedeis descenteda vineョ [SEP]']
[Init] best perm rec loss: 1.2788499593734741 for ['[CLS] stages holiday descent sunlightway half center fist away vine died tarzan riding focused website pi anywhere broughtedaeis heldョ reserved caught [SEP]']
[Init] best perm rec loss: 1.2772839069366455 for ['[CLS] anywhere stages riding halfeis away died center held focusedway brought tarzan sunlight holiday reservededa pi descentョ caught fist website vine [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=3.152 (perp=13.600, rec=0.424, cos=0.007), tot_loss_proj:3.957 [t=0.28s]
prediction: ['[CLS] cursedques airport selfish kappa sought industry designed film patronsr honneur medal societe club department torment sought insight give colby mountain cosmeticschal [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.912 (perp=12.774, rec=0.354, cos=0.003), tot_loss_proj:4.051 [t=0.27s]
prediction: ['[CLS] black translated novels selfish vanity ignored worthy vanity film patrons effort smugdora prepare seater filmlling pays training wow penny troyerated indies [SEP]']
[ 150/ 500] tot_loss=3.026 (perp=13.521, rec=0.320, cos=0.002), tot_loss_proj:3.839 [t=0.26s]
prediction: ['[CLS] designedios wealthy vanity vanity ignored debt vanity film debters fright jackson please that film dissolution pays training pays trait risingა indies [SEP]']
Attempt swap
Put prefix at the end
[ 200/ 500] tot_loss=2.773 (perp=12.451, rec=0.281, cos=0.002), tot_loss_proj:3.879 [t=0.25s]
prediction: ['[CLS] that film dissolution pays film paysiere mor dracula indies morigue debt vanity vanity ignored debt vanity film debters fright behind debts [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=2.762 (perp=12.482, rec=0.264, cos=0.001), tot_loss_proj:3.974 [t=0.27s]
prediction: ['[CLS] that film cyrus pays where indies quite campbell dracula dexly postage film vanity vanity ignored debt vanity film debters fright hidden debts [SEP]']
[ 300/ 500] tot_loss=2.630 (perp=11.983, rec=0.232, cos=0.002), tot_loss_proj:3.968 [t=0.25s]
prediction: ['[CLS] that doubt doubt pays what indies quite campbell vanity narrowedly s film vanity vanity what debt vanity film debters fright quentin debts [SEP]']
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=2.559 (perp=11.729, rec=0.212, cos=0.001), tot_loss_proj:3.979 [t=0.26s]
prediction: ['[CLS] that doubt doubt pays whatly tooth films vanity lack products s film vanity vanity what debt vanity film debters fright quentin debts [SEP]']
Attempt swap
Moved token
[ 400/ 500] tot_loss=2.469 (perp=11.388, rec=0.190, cos=0.002), tot_loss_proj:3.838 [t=0.26s]
prediction: ['[CLS] that doubt doubt pays offlyaging films vanity products lack s film vanity vanity what debt vanity film debt film fright quentin debts [SEP]']
[ 450/ 500] tot_loss=2.468 (perp=11.441, rec=0.177, cos=0.003), tot_loss_proj:3.755 [t=0.25s]
prediction: ['[CLS] that doubt doubt pays off vanity malice amara minerva films lack s film vanity vanity what debt vanity film debt film fright believed owed [SEP]']
Attempt swap
Moved token
[ 500/ 500] tot_loss=2.289 (perp=10.643, rec=0.160, cos=0.000), tot_loss_proj:3.634 [t=0.26s]
prediction: ['[CLS] that doubt doubt pays off malice films nominee films lack s a film vanity vanity what debt vanity film debt film fright believed owed [SEP]']
Done with input #8 of 100.
reference: 
========================
[CLS]'s a frightful vanity film that, no doubt, pays off what debt miramax felt they owed to benigni [SEP]
========================
predicted: 
========================
[CLS] that doubt doubt pays off malice films nominee films lack s a film vanity vanity what debt vanity film debt film fright believed owed [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 56.522 | p: 50.000 | r: 65.000
rouge2     | fm: 22.727 | p: 20.000 | r: 26.316
rougeL     | fm: 39.130 | p: 34.615 | r: 45.000
rougeLsum  | fm: 39.130 | p: 34.615 | r: 45.000
r1fm+r2fm = 79.249

[Aggregate metrics]:
rouge1     | fm: 90.119 | p: 89.130 | r: 91.349
rouge2     | fm: 70.250 | p: 69.899 | r: 70.702
rougeL     | fm: 83.388 | p: 82.711 | r: 84.233
rougeLsum  | fm: 83.388 | p: 82.720 | r: 84.233
r1fm+r2fm = 160.369

input #8 time: 0:03:30 | total time: 0:32:03


Running input #9 of 100.
reference: 
========================
of softheaded metaphysical claptrap 
========================
*********************************
*********************************
average of cosine similarity 0.9993981275486598
highest_index [0]
highest [0.9993981275486598]
Debug: ids_shape = 10, pads = [10]
Debug: input ids = tensor([[  101,  1997,  3730,  4974,  2098, 29081, 28618,  6494,  2361,   102]],
       device='cuda:0')
Debug: ref = ['[CLS] of softheaded metaphysical claptrap [SEP]']
[Init] best rec loss: 1.700202226638794 for ['[CLS] 2 definitely alert just heard 2005 p [MASK] [SEP]']
[Init] best rec loss: 1.27736234664917 for ['[CLS] reaction pleasure throatably thereafter state attachedoto [SEP]']
[Init] best rec loss: 1.1396721601486206 for ['[CLS] imp fbution specialising ste " lip nearby [SEP]']
[Init] best rec loss: 1.0866336822509766 for ['[CLS] video guys glass stilldleulf explorer eva [SEP]']
[Init] best perm rec loss: 1.0833972692489624 for ['[CLS] explorerdle video guys glass evaulf still [SEP]']
[Init] best perm rec loss: 1.0814344882965088 for ['[CLS] video guysulf still glassdle explorer eva [SEP]']
[Init] best perm rec loss: 1.08087956905365 for ['[CLS] evadle guys video glass still explorerulf [SEP]']
[Init] best perm rec loss: 1.079574704170227 for ['[CLS] still guys explorer videoulf evadle glass [SEP]']
[Init] best perm rec loss: 1.079067349433899 for ['[CLS] glass explorerulfdle still guys video eva [SEP]']
[Init] best perm rec loss: 1.0773389339447021 for ['[CLS] videoulf guys explorer stilldle glass eva [SEP]']
[Init] best perm rec loss: 1.075656533241272 for ['[CLS] still eva explorerdle glass guysulf video [SEP]']
[Init] best perm rec loss: 1.075609564781189 for ['[CLS]dle stillulf glass video eva guys explorer [SEP]']
[Init] best perm rec loss: 1.0754793882369995 for ['[CLS] still evaulf guys video explorer glassdle [SEP]']
[Init] best perm rec loss: 1.0741891860961914 for ['[CLS] glass still eva guys videodle explorerulf [SEP]']
[Init] best perm rec loss: 1.0710862874984741 for ['[CLS] eva video still glass explorer guysdleulf [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.976 (perp=13.033, rec=0.359, cos=0.010), tot_loss_proj:3.735 [t=0.26s]
prediction: ['[CLS] bean artists name clap protestant bread tomb symbolism [SEP]']
Attempt swap
Moved token
[ 100/ 500] tot_loss=2.495 (perp=11.043, rec=0.268, cos=0.018), tot_loss_proj:3.443 [t=0.26s]
prediction: ['[CLS] metaphysical trick of claptan bread tomb briefs [SEP]']
[ 150/ 500] tot_loss=2.348 (perp=10.789, rec=0.186, cos=0.004), tot_loss_proj:3.678 [t=0.25s]
prediction: ['[CLS] metaphysical soft of claptra softhead molded [SEP]']
Attempt swap
Moved sequence
[ 200/ 500] tot_loss=2.002 (perp=9.081, rec=0.178, cos=0.008), tot_loss_proj:2.730 [t=0.25s]
prediction: ['[CLS] metaphysical claptra soft of softheaded [SEP]']
Attempt swap
Moved token
[ 250/ 500] tot_loss=1.897 (perp=8.811, rec=0.134, cos=0.001), tot_loss_proj:2.749 [t=0.27s]
prediction: ['[CLS] metaphysical claptra of softheaded soft [SEP]']
[ 300/ 500] tot_loss=2.002 (perp=9.424, rec=0.116, cos=0.001), tot_loss_proj:2.673 [t=0.25s]
prediction: ['[CLS] metaphysical claptra of softheaded controlled [SEP]']
Attempt swap
[ 350/ 500] tot_loss=2.320 (perp=11.067, rec=0.105, cos=0.002), tot_loss_proj:2.982 [t=0.25s]
prediction: ['[CLS] metaphysical claptra of softheadp metaphysical [SEP]']
Attempt swap
Moved sequence
[ 400/ 500] tot_loss=1.814 (perp=8.569, rec=0.099, cos=0.001), tot_loss_proj:2.489 [t=0.27s]
prediction: ['[CLS] metaphysical claptrap of softhead metaphysical [SEP]']
[ 450/ 500] tot_loss=1.535 (perp=7.224, rec=0.090, cos=0.000), tot_loss_proj:1.907 [t=0.25s]
prediction: ['[CLS] metaphysical claptrap of softheaded [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.525 (perp=7.224, rec=0.080, cos=0.000), tot_loss_proj:1.920 [t=0.26s]
prediction: ['[CLS] metaphysical claptrap of softheaded [SEP]']
Done with input #9 of 100.
reference: 
========================
[CLS] of softheaded metaphysical claptrap [SEP]
========================
predicted: 
========================
[CLS] metaphysical claptrap of softheaded [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 40.000 | p: 40.000 | r: 40.000
rougeL     | fm: 66.667 | p: 66.667 | r: 66.667
rougeLsum  | fm: 66.667 | p: 66.667 | r: 66.667
r1fm+r2fm = 140.000

[Aggregate metrics]:
rouge1     | fm: 91.107 | p: 90.217 | r: 92.214
rouge2     | fm: 67.225 | p: 66.909 | r: 67.632
rougeL     | fm: 81.970 | p: 81.106 | r: 82.810
rougeLsum  | fm: 81.716 | p: 81.106 | r: 82.476
r1fm+r2fm = 158.332

input #9 time: 0:03:28 | total time: 0:35:32


Running input #10 of 100.
reference: 
========================
ably balances real-time rhythms with propulsive incident . 
========================
*********************************
*********************************
average of cosine similarity 0.9991472052153931
highest_index [0]
highest [0.9991472052153931]
Debug: ids_shape = 15, pads = [15]
Debug: input ids = tensor([[  101, 11113,  2135,  5703,  2015,  2613,  1011,  2051, 17900,  2007,
         17678, 23004,  5043,  1012,   102]], device='cuda:0')
Debug: ref = ['[CLS] ably balances real - time rhythms with propulsive incident. [SEP]']
[Init] best rec loss: 1.8671343326568604 for ['[CLS] upon mora citizen genus crambidaeset devils roll association refersudged dea appropriate [SEP]']
[Init] best rec loss: 1.8420976400375366 for ['[CLS] true double constructive tribune foreman! rfc corbin glory reverse about grow generally [SEP]']
[Init] best rec loss: 1.4058338403701782 for ['[CLS] uporic blessed level sheep sound common themes places totallyblood order angel [SEP]']
[Init] best perm rec loss: 1.3968875408172607 for ['[CLS] themes up sheep blessedblood angel common orderoric places totally level sound [SEP]']
[Init] best perm rec loss: 1.3953274488449097 for ['[CLS]blood common angeloric up sheep order blessed sound level totally places themes [SEP]']
[Init] best perm rec loss: 1.3951473236083984 for ['[CLS] themesoric soundblood sheep up places common blessed level order totally angel [SEP]']
[Init] best perm rec loss: 1.3938393592834473 for ['[CLS] angel up level order totally themes placesblood sheeporic sound blessed common [SEP]']
[Init] best perm rec loss: 1.3935538530349731 for ['[CLS] totally up common level places sheep order sound blessed themes angeloricblood [SEP]']
[Init] best perm rec loss: 1.3933236598968506 for ['[CLS] sheep places common sound themes totally order blessed level upbloodoric angel [SEP]']
[Init] best perm rec loss: 1.3882167339324951 for ['[CLS] places sheep totally level uporic blessed order angel sound common themesblood [SEP]']
[Init] best perm rec loss: 1.387595295906067 for ['[CLS]bloodoric blessed totally up themes places angel sheep common sound order level [SEP]']
[Init] best perm rec loss: 1.3872023820877075 for ['[CLS] sound blessedblood common up sheep places order level totally themes angeloric [SEP]']
[Init] best perm rec loss: 1.3837432861328125 for ['[CLS] sound places angel common totally level sheep order blessed uporic themesblood [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.631 (perp=11.254, rec=0.372, cos=0.007), tot_loss_proj:3.345 [t=0.25s]
prediction: ['[CLS] mmmplicity studies onto differences focuses optimistic. special spirit and the stations [SEP]']
Attempt swap
Moved sequence
[ 100/ 500] tot_loss=2.474 (perp=10.931, rec=0.285, cos=0.003), tot_loss_proj:3.572 [t=0.26s]
prediction: ['[CLS]ctuatedplicity balance with philosophical. special spiritef and tracking based automatically [SEP]']
[ 150/ 500] tot_loss=2.391 (perp=10.792, rec=0.231, cos=0.002), tot_loss_proj:3.974 [t=0.27s]
prediction: ['[CLS] routineplicity balance with philosophical. secret spirit into. rhythms based blindly [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.384 (perp=10.907, rec=0.201, cos=0.002), tot_loss_proj:3.846 [t=0.26s]
prediction: ['[CLS] ab penalties balance with philosophical.. spirit into secret rhythms kilometers ab [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=2.324 (perp=10.608, rec=0.199, cos=0.003), tot_loss_proj:3.495 [t=0.25s]
prediction: ['[CLS] abulsive balance with perspective.. fatally springs super rhythmsly ab [SEP]']
[ 300/ 500] tot_loss=2.311 (perp=10.668, rec=0.172, cos=0.006), tot_loss_proj:3.867 [t=0.25s]
prediction: ['[CLS] abulsive balance with perspective.. fatally incidentscopic rhythmsly ab [SEP]']
Attempt swap
Moved token
[ 350/ 500] tot_loss=2.010 (perp=9.313, rec=0.147, cos=0.001), tot_loss_proj:3.336 [t=0.25s]
prediction: ['[CLS] abulsive balance with perspective.. fatally incidentulsive rhythms ably [SEP]']
Attempt swap
Moved sequence
[ 400/ 500] tot_loss=2.093 (perp=9.630, rec=0.164, cos=0.003), tot_loss_proj:3.471 [t=0.25s]
prediction: ['[CLS] ab balance with facts -.lyulsive incident injury rhythms ably [SEP]']
[ 450/ 500] tot_loss=2.029 (perp=9.488, rec=0.130, cos=0.001), tot_loss_proj:2.749 [t=0.25s]
prediction: ['[CLS]ly balances real -.lyulsive incidentulsive rhythms ably [SEP]']
Attempt swap
Moved sequence
[ 500/ 500] tot_loss=1.879 (perp=8.452, rec=0.178, cos=0.010), tot_loss_proj:2.577 [t=0.26s]
prediction: ['[CLS]ly balances real -.ulsive propulsive incident rhythms ably [SEP]']
Done with input #10 of 100.
reference: 
========================
[CLS] ably balances real - time rhythms with propulsive incident. [SEP]
========================
predicted: 
========================
[CLS]ly balances real -.ulsive propulsive incident rhythms ably [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 80.000 | p: 80.000 | r: 80.000
rouge2     | fm: 22.222 | p: 22.222 | r: 22.222
rougeL     | fm: 60.000 | p: 60.000 | r: 60.000
rougeLsum  | fm: 60.000 | p: 60.000 | r: 60.000
r1fm+r2fm = 102.222

[Aggregate metrics]:
rouge1     | fm: 90.277 | p: 89.486 | r: 91.364
rouge2     | fm: 63.134 | p: 62.847 | r: 63.743
rougeL     | fm: 79.866 | p: 79.188 | r: 80.433
rougeLsum  | fm: 80.117 | p: 79.662 | r: 81.039
r1fm+r2fm = 153.411

input #10 time: 0:03:27 | total time: 0:38:59


Running input #11 of 100.
reference: 
========================
was being attempted here that stubbornly refused to gel 
========================
*********************************
*********************************
average of cosine similarity 0.9992849825911105
highest_index [0]
highest [0.9992849825911105]
Debug: ids_shape = 12, pads = [12]
Debug: input ids = tensor([[  101,  2001,  2108,  4692,  2182,  2008, 14205,  2135,  4188,  2000,
         21500,   102]], device='cuda:0')
Debug: ref = ['[CLS] was being attempted here that stubbornly refused to gel [SEP]']
[Init] best rec loss: 1.8228942155838013 for ['[CLS] whitney keseion label lane attributed kaplankling today [SEP]']
[Init] best rec loss: 1.7478306293487549 for ['[CLS] fluidigh ron doctor karma client climateweed native eggs [SEP]']
[Init] best rec loss: 1.3220566511154175 for ['[CLS] platform tal drawn inland mileture familiarvd megu [SEP]']
[Init] best perm rec loss: 1.311123251914978 for ['[CLS] me talvd familiar inland mileturegu drawn platform [SEP]']
[Init] best perm rec loss: 1.3074727058410645 for ['[CLS]ture inlandgu me tal platform familiarvd mile drawn [SEP]']
[Init] best perm rec loss: 1.2857089042663574 for ['[CLS] inlandture drawn tal platform megu familiarvd mile [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.823 (perp=12.116, rec=0.395, cos=0.005), tot_loss_proj:3.441 [t=0.26s]
prediction: ['[CLS] which trouble away♣ springs attempted debris _ed project [SEP]']
Attempt swap
Moved token
[ 100/ 500] tot_loss=2.750 (perp=12.177, rec=0.312, cos=0.003), tot_loss_proj:3.938 [t=0.26s]
prediction: ['[CLS] that tried refused especially insisted activists stubborn geled refused [SEP]']
[ 150/ 500] tot_loss=2.595 (perp=11.868, rec=0.220, cos=0.002), tot_loss_proj:3.153 [t=0.25s]
prediction: ['[CLS] was attempted refused where stubborn copies stubborn geled gel [SEP]']
Attempt swap
Moved token
[ 200/ 500] tot_loss=2.175 (perp=10.031, rec=0.168, cos=0.001), tot_loss_proj:2.906 [t=0.27s]
prediction: ['[CLS] was attempted that stubbornly stubborn refused geled gel [SEP]']
Attempt swap
Moved token
[ 250/ 500] tot_loss=2.275 (perp=10.610, rec=0.151, cos=0.001), tot_loss_proj:2.926 [t=0.25s]
prediction: ['[CLS] was attempted that stubborn stubborn here refused gel to gel [SEP]']
[ 300/ 500] tot_loss=2.255 (perp=10.610, rec=0.132, cos=0.001), tot_loss_proj:2.921 [t=0.25s]
prediction: ['[CLS] was attempted that stubborn stubborn here refused gel to gel [SEP]']
Attempt swap
Moved token
[ 350/ 500] tot_loss=1.926 (perp=9.078, rec=0.109, cos=0.001), tot_loss_proj:2.922 [t=0.25s]
prediction: ['[CLS] here was attempted that stubborn stubborn refused gelly to [SEP]']
Attempt swap
Moved token
[ 400/ 500] tot_loss=1.822 (perp=8.566, rec=0.108, cos=0.001), tot_loss_proj:2.680 [t=0.27s]
prediction: ['[CLS] here was attempted that stubborn stubborn refused to gelly [SEP]']
[ 450/ 500] tot_loss=1.799 (perp=8.566, rec=0.085, cos=0.000), tot_loss_proj:2.685 [t=0.26s]
prediction: ['[CLS] here was attempted that stubborn stubborn refused to gelly [SEP]']
Attempt swap
Moved token
[ 500/ 500] tot_loss=1.661 (perp=7.834, rec=0.093, cos=0.001), tot_loss_proj:2.099 [t=0.26s]
prediction: ['[CLS] here was attempted that stubbornly stubborn refused to gel [SEP]']
Done with input #11 of 100.
reference: 
========================
[CLS] was being attempted here that stubbornly refused to gel [SEP]
========================
predicted: 
========================
[CLS] here was attempted that stubborn stubborn refused to gelly [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 72.727 | p: 72.727 | r: 72.727
rouge2     | fm: 10.000 | p: 10.000 | r: 10.000
rougeL     | fm: 63.636 | p: 63.636 | r: 63.636
rougeLsum  | fm: 63.636 | p: 63.636 | r: 63.636
r1fm+r2fm = 82.727

[Aggregate metrics]:
rouge1     | fm: 88.814 | p: 87.955 | r: 89.848
rouge2     | fm: 58.728 | p: 58.443 | r: 59.230
rougeL     | fm: 78.409 | p: 77.874 | r: 79.023
rougeLsum  | fm: 78.600 | p: 78.195 | r: 79.036
r1fm+r2fm = 147.543

input #11 time: 0:03:27 | total time: 0:42:27


Running input #12 of 100.
reference: 
========================
that will be seen to better advantage on cable , especially considering its barely 
========================
*********************************
*********************************
average of cosine similarity 0.9993439176315202
highest_index [0]
highest [0.9993439176315202]
Debug: ids_shape = 16, pads = [16]
Debug: input ids = tensor([[ 101, 2008, 2097, 2022, 2464, 2000, 2488, 5056, 2006, 5830, 1010, 2926,
         6195, 2049, 4510,  102]], device='cuda:0')
Debug: ref = ['[CLS] that will be seen to better advantage on cable, especially considering its barely [SEP]']
[Init] best rec loss: 1.8480900526046753 for ['[CLS] beausse japan forbid the colleen success wireless file ryan mhz seeks black pilgrimage [SEP]']
[Init] best rec loss: 1.794683814048767 for ['[CLS] i stars embankment good fitted obeeborg cole incorporated relative : alone sans cad [SEP]']
[Init] best rec loss: 1.4141982793807983 for ['[CLS] hurdlesseazi tax clause asia read rose againyeh pu jace universe team [SEP]']
[Init] best rec loss: 1.3857673406600952 for ['[CLS]cape release beyond doctors native dig legs seven meansnessy la delivery president jam [SEP]']
[Init] best rec loss: 1.319607138633728 for ['[CLS] whatever prince time junior craigsal can grew late parade clues find given but [SEP]']
[Init] best rec loss: 1.2739667892456055 for ['[CLS] few lay series margin shades office translate victornctionyn haitas beth guess [SEP]']
[Init] best rec loss: 1.2710332870483398 for ['[CLS] command married coma lux me brook fuel specifically containing deaf missing firm prospects track [SEP]']
[Init] best perm rec loss: 1.2705339193344116 for ['[CLS] prospects command me brook married lux containing fuel coma track firm missing deaf specifically [SEP]']
[Init] best perm rec loss: 1.2703341245651245 for ['[CLS] coma deaf lux containing married brook prospects missing me firm specifically command fuel track [SEP]']
[Init] best perm rec loss: 1.2677892446517944 for ['[CLS] command brook deaf firm missing prospects specifically coma containing track married me lux fuel [SEP]']
[Init] best perm rec loss: 1.2655885219573975 for ['[CLS] track deaf containing specifically firm prospects coma command brook fuel lux missing me married [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.806 (perp=12.068, rec=0.390, cos=0.002), tot_loss_proj:3.465 [t=0.25s]
prediction: ['[CLS] if weakly off apparently rub hearing suspect least having limit cable probably weak attack [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.358 (perp=10.339, rec=0.285, cos=0.005), tot_loss_proj:2.935 [t=0.26s]
prediction: ['[CLS] than barely for better rebellion to to being laid endangered cable cable lacking cable [SEP]']
[ 150/ 500] tot_loss=2.087 (perp=9.330, rec=0.221, cos=0.001), tot_loss_proj:2.897 [t=0.26s]
prediction: ['[CLS] than barely on better rolled to will being seen about cable cable hearing cable [SEP]']
Attempt swap
Moved token
[ 200/ 500] tot_loss=2.313 (perp=10.636, rec=0.184, cos=0.002), tot_loss_proj:3.030 [t=0.25s]
prediction: ['[CLS] than barely better rub to will being seen on advantage advantage cable certain cable [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=2.097 (perp=9.646, rec=0.166, cos=0.001), tot_loss_proj:2.745 [t=0.26s]
prediction: ['[CLS] that barely better rub to will considering seen on certain advantage cable advantage cable [SEP]']
[ 300/ 500] tot_loss=2.074 (perp=9.646, rec=0.144, cos=0.001), tot_loss_proj:2.743 [t=0.26s]
prediction: ['[CLS] that barely better rub to will considering seen on certain advantage cable advantage cable [SEP]']
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=2.108 (perp=9.896, rec=0.129, cos=0.000), tot_loss_proj:2.862 [t=0.27s]
prediction: ['[CLS] that barely better will to rub considering seen on certain advantage cable advantage despite [SEP]']
Attempt swap
Swapped tokens
[ 400/ 500] tot_loss=1.999 (perp=9.349, rec=0.128, cos=0.001), tot_loss_proj:2.801 [t=0.26s]
prediction: ['[CLS] that barely better will to rub on seen considering certain advantage cable advantage despite [SEP]']
[ 450/ 500] tot_loss=2.105 (perp=9.974, rec=0.110, cos=0.000), tot_loss_proj:2.957 [t=0.25s]
prediction: ['[CLS] that barely better will to to on seen considering certain advantage cable advantage its [SEP]']
Attempt swap
Swapped tokens
[ 500/ 500] tot_loss=2.047 (perp=9.698, rec=0.107, cos=0.000), tot_loss_proj:2.826 [t=0.26s]
prediction: ['[CLS] that barely better will to to on seen considering especially advantage its advantage cable [SEP]']
Done with input #12 of 100.
reference: 
========================
[CLS] that will be seen to better advantage on cable, especially considering its barely [SEP]
========================
predicted: 
========================
[CLS] that barely better will to to on seen considering certain advantage its advantage cable [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 83.871 | p: 81.250 | r: 86.667
rouge2     | fm: 6.897 | p: 6.667 | r: 7.143
rougeL     | fm: 51.613 | p: 50.000 | r: 53.333
rougeLsum  | fm: 51.613 | p: 50.000 | r: 53.333
r1fm+r2fm = 90.768

[Aggregate metrics]:
rouge1     | fm: 88.577 | p: 87.585 | r: 89.780
rouge2     | fm: 54.899 | p: 54.615 | r: 55.162
rougeL     | fm: 76.363 | p: 75.731 | r: 77.038
rougeLsum  | fm: 76.021 | p: 75.391 | r: 76.628
r1fm+r2fm = 143.476

input #12 time: 0:03:26 | total time: 0:45:53


Running input #13 of 100.
reference: 
========================
point at things that explode into flame 
========================
*********************************
*********************************
average of cosine similarity 0.9992538394516878
highest_index [0]
highest [0.9992538394516878]
Debug: ids_shape = 9, pads = [9]
Debug: input ids = tensor([[  101,  2391,  2012,  2477,  2008, 15044,  2046,  8457,   102]],
       device='cuda:0')
Debug: ref = ['[CLS] point at things that explode into flame [SEP]']
[Init] best rec loss: 1.4737218618392944 for ['[CLS] mini gray inside mumbai atnies havoc [SEP]']
[Init] best rec loss: 1.4235199689865112 for ['[CLS] iona favorable vamp garrett nu pathetic miranda [SEP]']
[Init] best rec loss: 1.406105875968933 for ['[CLS] pdf along timing started mal practical prevailed [SEP]']
[Init] best rec loss: 1.390662670135498 for ['[CLS] clay young demon every rolesorestation bill [SEP]']
[Init] best rec loss: 1.3708242177963257 for ['[CLS]gled speaker finish eh asxy do [SEP]']
[Init] best rec loss: 1.3614050149917603 for ['[CLS] dame recess ak latter threshold po illness [SEP]']
[Init] best rec loss: 1.3537973165512085 for ['[CLS] squat what names set fence thin received [SEP]']
[Init] best rec loss: 1.3353148698806763 for ['[CLS] dressed fraternity colonial round et bahn heads [SEP]']
[Init] best rec loss: 1.3023403882980347 for ['[CLS] furnace card double experiment working corruption without [SEP]']
[Init] best rec loss: 1.261353611946106 for ['[CLS] arm permanent rowe precision cardinal defeational [SEP]']
[Init] best perm rec loss: 1.2592674493789673 for ['[CLS] permanent defeat arm cardinal roweional precision [SEP]']
[Init] best perm rec loss: 1.2589128017425537 for ['[CLS] defeat cardinal arm roweional precision permanent [SEP]']
[Init] best perm rec loss: 1.256705403327942 for ['[CLS] permanent cardinalional arm precision rowe defeat [SEP]']
[Init] best perm rec loss: 1.2559808492660522 for ['[CLS] permanentional cardinal defeat rowe precision arm [SEP]']
[Init] best perm rec loss: 1.2554824352264404 for ['[CLS] defeat cardinalional arm permanent precision rowe [SEP]']
[Init] best perm rec loss: 1.2538996934890747 for ['[CLS] cardinal defeational arm precision rowe permanent [SEP]']
[Init] best perm rec loss: 1.2521754503250122 for ['[CLS] permanent rowe cardinal armional precision defeat [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.500 (perp=10.752, rec=0.344, cos=0.005), tot_loss_proj:3.458 [t=0.26s]
prediction: ['[CLS] accusations you must flames really steel took [SEP]']
Attempt swap
Put prefix at the end
[ 100/ 500] tot_loss=2.506 (perp=11.010, rec=0.299, cos=0.005), tot_loss_proj:3.779 [t=0.26s]
prediction: ['[CLS] when flame mean candy into point something [SEP]']
[ 150/ 500] tot_loss=2.051 (perp=9.149, rec=0.219, cos=0.002), tot_loss_proj:3.487 [t=0.26s]
prediction: ['[CLS] when flame point things into point things [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.055 (perp=9.323, rec=0.189, cos=0.001), tot_loss_proj:3.459 [t=0.25s]
prediction: ['[CLS] at flame point things into point when [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=1.966 (perp=8.847, rec=0.193, cos=0.003), tot_loss_proj:3.240 [t=0.25s]
prediction: ['[CLS] things flame at point into things when [SEP]']
[ 300/ 500] tot_loss=2.062 (perp=9.574, rec=0.147, cos=0.001), tot_loss_proj:3.659 [t=0.25s]
prediction: ['[CLS] things flame at point into things moments [SEP]']
Attempt swap
Moved sequence
[ 350/ 500] tot_loss=1.991 (perp=9.337, rec=0.122, cos=0.001), tot_loss_proj:2.959 [t=0.26s]
prediction: ['[CLS] things flame point into things at explode [SEP]']
Attempt swap
Moved sequence
[ 400/ 500] tot_loss=1.384 (perp=6.098, rec=0.160, cos=0.004), tot_loss_proj:2.964 [t=0.27s]
prediction: ['[CLS] things flame into things at that point [SEP]']
[ 450/ 500] tot_loss=1.640 (perp=7.651, rec=0.110, cos=0.001), tot_loss_proj:2.578 [t=0.26s]
prediction: ['[CLS] things flame explode things at that point [SEP]']
Attempt swap
Swapped tokens
[ 500/ 500] tot_loss=1.453 (perp=6.717, rec=0.109, cos=0.000), tot_loss_proj:2.330 [t=0.28s]
prediction: ['[CLS] things flame things explode at that point [SEP]']
Done with input #13 of 100.
reference: 
========================
[CLS] point at things that explode into flame [SEP]
========================
predicted: 
========================
[CLS] things flame point into things at that [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 88.889 | p: 88.889 | r: 88.889
rouge2     | fm: 0.000 | p: 0.000 | r: 0.000
rougeL     | fm: 55.556 | p: 55.556 | r: 55.556
rougeLsum  | fm: 55.556 | p: 55.556 | r: 55.556
r1fm+r2fm = 88.889

[Aggregate metrics]:
rouge1     | fm: 88.549 | p: 87.737 | r: 89.554
rouge2     | fm: 50.300 | p: 50.069 | r: 50.698
rougeL     | fm: 74.894 | p: 74.293 | r: 75.464
rougeLsum  | fm: 74.804 | p: 74.227 | r: 75.331
r1fm+r2fm = 138.850

input #13 time: 0:03:26 | total time: 0:49:20


Running input #14 of 100.
reference: 
========================
undeniably intriguing film 
========================
*********************************
*********************************
average of cosine similarity 0.9993219682705545
highest_index [0]
highest [0.9993219682705545]
Debug: ids_shape = 7, pads = [7]
Debug: input ids = tensor([[  101,  6151, 19825,  6321, 23824,  2143,   102]], device='cuda:0')
Debug: ref = ['[CLS] undeniably intriguing film [SEP]']
[Init] best rec loss: 1.9555590152740479 for ['[CLS] library falls children tierney espn [SEP]']
[Init] best rec loss: 1.90388023853302 for ['[CLS] olivia temple will kerr whom [SEP]']
[Init] best rec loss: 1.7101174592971802 for ['[CLS] junior touching itpton ; [SEP]']
[Init] best rec loss: 1.6756500005722046 for ['[CLS] brooks mentioninianame nothing [SEP]']
[Init] best rec loss: 1.649327278137207 for ['[CLS] quiver federation maddie sacramentoboard [SEP]']
[Init] best rec loss: 1.459338903427124 for ['[CLS] myers harold sprayed [MASK] tom [SEP]']
[Init] best perm rec loss: 1.455399990081787 for ['[CLS] myers harold tom [MASK] sprayed [SEP]']
[Init] best perm rec loss: 1.4520574808120728 for ['[CLS] harold myers [MASK] tom sprayed [SEP]']
[Init] best perm rec loss: 1.4497400522232056 for ['[CLS] myers harold [MASK] tom sprayed [SEP]']
[Init] best perm rec loss: 1.4491201639175415 for ['[CLS] harold tom myers [MASK] sprayed [SEP]']
[Init] best perm rec loss: 1.4477119445800781 for ['[CLS] [MASK] harold myers tom sprayed [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.695 (perp=12.128, rec=0.266, cos=0.003), tot_loss_proj:2.881 [t=0.27s]
prediction: ['[CLS]bly intriguing intriguing uncanny intriguing [SEP]']
Attempt swap
Moved token
[ 100/ 500] tot_loss=2.284 (perp=10.606, rec=0.161, cos=0.002), tot_loss_proj:2.481 [t=0.26s]
prediction: ['[CLS]bly intriguingbly intriguing film [SEP]']
[ 150/ 500] tot_loss=2.621 (perp=12.582, rec=0.103, cos=0.001), tot_loss_proj:2.882 [t=0.26s]
prediction: ['[CLS]bly intriguingenia intriguing film [SEP]']
Attempt swap
Moved sequence
[ 200/ 500] tot_loss=1.686 (perp=7.971, rec=0.090, cos=0.001), tot_loss_proj:1.909 [t=0.25s]
prediction: ['[CLS] intriguingeniably intriguing film [SEP]']
Attempt swap
[ 250/ 500] tot_loss=1.678 (perp=7.971, rec=0.083, cos=0.001), tot_loss_proj:1.909 [t=0.25s]
prediction: ['[CLS] intriguingeniably intriguing film [SEP]']
[ 300/ 500] tot_loss=1.682 (perp=7.971, rec=0.087, cos=0.001), tot_loss_proj:1.918 [t=0.26s]
prediction: ['[CLS] intriguingeniably intriguing film [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.427 (perp=6.728, rec=0.080, cos=0.001), tot_loss_proj:1.407 [t=0.25s]
prediction: ['[CLS] undeniably intriguing film [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.441 (perp=6.728, rec=0.094, cos=0.001), tot_loss_proj:1.416 [t=0.26s]
prediction: ['[CLS] undeniably intriguing film [SEP]']
[ 450/ 500] tot_loss=1.427 (perp=6.728, rec=0.080, cos=0.001), tot_loss_proj:1.413 [t=0.25s]
prediction: ['[CLS] undeniably intriguing film [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.437 (perp=6.728, rec=0.090, cos=0.001), tot_loss_proj:1.416 [t=0.27s]
prediction: ['[CLS] undeniably intriguing film [SEP]']
Done with input #14 of 100.
reference: 
========================
[CLS] undeniably intriguing film [SEP]
========================
predicted: 
========================
[CLS] undeniably intriguing film [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 89.342 | p: 88.627 | r: 90.212
rouge2     | fm: 53.425 | p: 53.101 | r: 53.772
rougeL     | fm: 76.515 | p: 76.003 | r: 77.066
rougeLsum  | fm: 76.143 | p: 75.760 | r: 76.716
r1fm+r2fm = 142.767

input #14 time: 0:03:26 | total time: 0:52:46


Running input #15 of 100.
reference: 
========================
efficient , suitably anonymous chiller . 
========================
*********************************
*********************************
average of cosine similarity 0.9992724874303516
highest_index [0]
highest [0.9992724874303516]
Debug: ids_shape = 10, pads = [10]
Debug: input ids = tensor([[  101,  8114,  1010,  4848,  8231, 10812, 10720,  2121,  1012,   102]],
       device='cuda:0')
Debug: ref = ['[CLS] efficient, suitably anonymous chiller. [SEP]']
[Init] best rec loss: 1.96763014793396 for ['[CLS] rope accepting the truth few having sikh music [SEP]']
[Init] best rec loss: 1.925123929977417 for ['[CLS] ¹⁄₂ lds bay simple 19 client utc congestion [SEP]']
[Init] best rec loss: 1.9198782444000244 for ['[CLS] property par coming kincaid pulling node reid wild [SEP]']
[Init] best rec loss: 1.8514207601547241 for ['[CLS] light over lear hodges second base twinned ecstasy [SEP]']
[Init] best rec loss: 1.7387832403182983 for ['[CLS] clubs peaceerated enclosed davis 事 timestle [SEP]']
[Init] best rec loss: 1.7253295183181763 for ['[CLS] tor casey decembergrate doua developed logic [SEP]']
[Init] best rec loss: 1.7060983180999756 for ['[CLS] du resign what pet system jazz aschurch [SEP]']
[Init] best rec loss: 1.7037686109542847 for ['[CLS] madeline discovery ocean truss stations chance ledge waiting [SEP]']
[Init] best rec loss: 1.6834999322891235 for ['[CLS] child indian setting launched and today returns becker [SEP]']
[Init] best rec loss: 1.5785369873046875 for ['[CLS] winner french badminton harperrdial missed ex fond [SEP]']
[Init] best perm rec loss: 1.575916051864624 for ['[CLS] frenchrdial badminton winner ex missed fond harper [SEP]']
[Init] best perm rec loss: 1.5737770795822144 for ['[CLS] french harper fondrdial badminton missed ex winner [SEP]']
[Init] best perm rec loss: 1.5736515522003174 for ['[CLS] missed winner ex frenchrdial fond harper badminton [SEP]']
[Init] best perm rec loss: 1.569117546081543 for ['[CLS]rdial ex harper fond missed french winner badminton [SEP]']
[Init] best perm rec loss: 1.568715214729309 for ['[CLS] french ex harper fondrdial missed winner badminton [SEP]']
[Init] best perm rec loss: 1.5686029195785522 for ['[CLS] harperrdial fond winner french ex missed badminton [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.615 (perp=11.550, rec=0.301, cos=0.004), tot_loss_proj:3.140 [t=0.27s]
prediction: ['[CLS]ablyably for directors fittingally efficient precision [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.573 (perp=11.747, rec=0.222, cos=0.002), tot_loss_proj:3.021 [t=0.25s]
prediction: ['[CLS]ablyably successfully efficient efficient chill efficient chill [SEP]']
[ 150/ 500] tot_loss=2.264 (perp=10.447, rec=0.173, cos=0.001), tot_loss_proj:2.549 [t=0.26s]
prediction: ['[CLS] suitably. efficient efficient chill efficient chill [SEP]']
Attempt swap
Moved sequence
[ 200/ 500] tot_loss=2.160 (perp=10.114, rec=0.135, cos=0.002), tot_loss_proj:2.609 [t=0.26s]
prediction: ['[CLS] suitably anonymous chill. anonymous efficient chill [SEP]']
Attempt swap
Moved token
[ 250/ 500] tot_loss=2.167 (perp=10.315, rec=0.103, cos=0.001), tot_loss_proj:2.996 [t=0.25s]
prediction: ['[CLS] suitably anonymous chill. anonymouser efficient [SEP]']
[ 300/ 500] tot_loss=2.131 (perp=10.246, rec=0.081, cos=0.001), tot_loss_proj:3.060 [t=0.26s]
prediction: ['[CLS] suitably anonymous chill.,er efficient [SEP]']
Attempt swap
Moved token
[ 350/ 500] tot_loss=1.767 (perp=8.487, rec=0.069, cos=0.001), tot_loss_proj:2.173 [t=0.25s]
prediction: ['[CLS] suitably anonymous., chiller efficient [SEP]']
Attempt swap
Swapped tokens
[ 400/ 500] tot_loss=1.598 (perp=7.639, rec=0.069, cos=0.001), tot_loss_proj:1.801 [t=0.25s]
prediction: ['[CLS] suitably anonymous. efficient chiller, [SEP]']
[ 450/ 500] tot_loss=1.604 (perp=7.639, rec=0.075, cos=0.001), tot_loss_proj:1.797 [t=0.25s]
prediction: ['[CLS] suitably anonymous. efficient chiller, [SEP]']
Attempt swap
Swapped tokens
[ 500/ 500] tot_loss=1.491 (perp=7.125, rec=0.066, cos=0.001), tot_loss_proj:1.643 [t=0.25s]
prediction: ['[CLS] suitably anonymous, efficient chiller. [SEP]']
Done with input #15 of 100.
reference: 
========================
[CLS] efficient, suitably anonymous chiller. [SEP]
========================
predicted: 
========================
[CLS] suitably anonymous, efficient chiller. [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 40.000 | p: 40.000 | r: 40.000
rougeL     | fm: 83.333 | p: 83.333 | r: 83.333
rougeLsum  | fm: 83.333 | p: 83.333 | r: 83.333
r1fm+r2fm = 140.000

[Aggregate metrics]:
rouge1     | fm: 90.077 | p: 89.447 | r: 90.967
rouge2     | fm: 53.144 | p: 52.970 | r: 53.398
rougeL     | fm: 76.854 | p: 76.367 | r: 77.487
rougeLsum  | fm: 76.725 | p: 76.281 | r: 77.304
r1fm+r2fm = 143.221

input #15 time: 0:03:26 | total time: 0:56:13


Running input #16 of 100.
reference: 
========================
all of this , and more 
========================
*********************************
*********************************
average of cosine similarity 0.9993411698074052
highest_index [0]
highest [0.9993411698074052]
Debug: ids_shape = 8, pads = [8]
Debug: input ids = tensor([[ 101, 2035, 1997, 2023, 1010, 1998, 2062,  102]], device='cuda:0')
Debug: ref = ['[CLS] all of this, and more [SEP]']
[Init] best rec loss: 1.9508296251296997 for ['[CLS] experienced dev candidate giantstel pregnant [SEP]']
[Init] best rec loss: 1.8343384265899658 for ['[CLS] drops min s concerning dal cannon [SEP]']
[Init] best rec loss: 1.6890870332717896 for ['[CLS]athi lord alta slowly film various [SEP]']
[Init] best rec loss: 1.6304024457931519 for ['[CLS] ages mall mean influential len twain [SEP]']
[Init] best rec loss: 1.5867127180099487 for ['[CLS] ruling downcaster robot got focus [SEP]']
[Init] best rec loss: 1.525856614112854 for ['[CLS] legsyen t sharon camp ro [SEP]']
[Init] best rec loss: 1.5195893049240112 for ['[CLS]encia olgazy edit areas sounding [SEP]']
[Init] best rec loss: 1.3733412027359009 for ['[CLS] a clubs slayer trophy affected residents [SEP]']
[Init] best rec loss: 1.3456374406814575 for ['[CLS] ultra backpack tallest you downstream map [SEP]']
[Init] best perm rec loss: 1.337275505065918 for ['[CLS] tallest downstream map you backpack ultra [SEP]']
[Init] best perm rec loss: 1.336099624633789 for ['[CLS] map tallest ultra downstream backpack you [SEP]']
[Init] best perm rec loss: 1.3238879442214966 for ['[CLS] tallest ultra map backpack you downstream [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=3.255 (perp=13.939, rec=0.460, cos=0.008), tot_loss_proj:4.581 [t=0.25s]
prediction: ['[CLS] inch until results maxwell evengeny [SEP]']
Attempt swap
Put prefix at the end
[ 100/ 500] tot_loss=2.381 (perp=9.273, rec=0.514, cos=0.012), tot_loss_proj:3.285 [t=0.26s]
prediction: ['[CLS] make that that that walls and [SEP]']
[ 150/ 500] tot_loss=2.078 (perp=8.645, rec=0.347, cos=0.002), tot_loss_proj:3.219 [t=0.25s]
prediction: ['[CLS] this that all that walls and [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=1.734 (perp=7.157, rec=0.301, cos=0.002), tot_loss_proj:2.626 [t=0.25s]
prediction: ['[CLS] more and all this everything that [SEP]']
Attempt swap
Moved token
[ 250/ 500] tot_loss=1.487 (perp=6.160, rec=0.252, cos=0.003), tot_loss_proj:2.170 [t=0.26s]
prediction: ['[CLS] more all this and everything that [SEP]']
[ 300/ 500] tot_loss=1.341 (perp=5.551, rec=0.228, cos=0.004), tot_loss_proj:1.851 [t=0.28s]
prediction: ['[CLS] more all this and all that [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.306 (perp=5.385, rec=0.226, cos=0.003), tot_loss_proj:2.237 [t=0.25s]
prediction: ['[CLS] of all this and of that [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.424 (perp=6.152, rec=0.191, cos=0.002), tot_loss_proj:1.869 [t=0.27s]
prediction: ['[CLS] of all this and of more [SEP]']
[ 450/ 500] tot_loss=1.421 (perp=6.152, rec=0.186, cos=0.004), tot_loss_proj:1.878 [t=0.26s]
prediction: ['[CLS] of all this and of more [SEP]']
Attempt swap
Moved token
[ 500/ 500] tot_loss=1.237 (perp=5.368, rec=0.161, cos=0.002), tot_loss_proj:1.565 [t=0.26s]
prediction: ['[CLS] of all of this and more [SEP]']
Done with input #16 of 100.
reference: 
========================
[CLS] all of this, and more [SEP]
========================
predicted: 
========================
[CLS] of all of this and more [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 93.333 | p: 87.500 | r: 100.000
rouge2     | fm: 76.923 | p: 71.429 | r: 83.333
rougeL     | fm: 93.333 | p: 87.500 | r: 100.000
rougeLsum  | fm: 93.333 | p: 87.500 | r: 100.000
r1fm+r2fm = 170.256

[Aggregate metrics]:
rouge1     | fm: 90.208 | p: 89.083 | r: 91.354
rouge2     | fm: 54.511 | p: 54.077 | r: 54.978
rougeL     | fm: 78.551 | p: 77.684 | r: 79.311
rougeLsum  | fm: 77.687 | p: 76.934 | r: 78.704
r1fm+r2fm = 144.719

input #16 time: 0:03:26 | total time: 0:59:40


Running input #17 of 100.
reference: 
========================
want to think too much about what 's going on 
========================
*********************************
*********************************
average of cosine similarity 0.9992301552937229
highest_index [0]
highest [0.9992301552937229]
Debug: ids_shape = 13, pads = [13]
Debug: input ids = tensor([[ 101, 2215, 2000, 2228, 2205, 2172, 2055, 2054, 1005, 1055, 2183, 2006,
          102]], device='cuda:0')
Debug: ref = ["[CLS] want to think too much about what's going on [SEP]"]
[Init] best rec loss: 1.6595710515975952 for ['[CLS] one yuri alter slot roll again kanyefield await mourning benefit [SEP]']
[Init] best rec loss: 1.6428885459899902 for ['[CLS] friend agency todd wi dirty percent milesamp... vietsive [SEP]']
[Init] best rec loss: 1.6136316061019897 for ['[CLS] us junk " pete separate lost did eventriated air each [SEP]']
[Init] best rec loss: 1.576501488685608 for ['[CLS] confines gracie spit modern name slip loire service again recall gate [SEP]']
[Init] best rec loss: 1.5198489427566528 for ['[CLS] highestlt short ad relation minority black bunch marine above different [SEP]']
[Init] best rec loss: 1.513322114944458 for ['[CLS] name standardfoldieg names result diesfulds suggest mystery [SEP]']
[Init] best rec loss: 1.4297772645950317 for ['[CLS] leadute ti aria shooter atislav levi average garde attitude [SEP]']
[Init] best rec loss: 1.3880505561828613 for ['[CLS] general sensation water consecrated affairvudran bethany then religious threatened [SEP]']
[Init] best rec loss: 1.366730809211731 for ['[CLS] georgian kilometers following fatal conversion station arch with gene goddess [SEP]']
[Init] best perm rec loss: 1.3620654344558716 for ['[CLS] gene station fatalh conversion following goddess kilometers with georgian arc [SEP]']
[Init] best perm rec loss: 1.361521601676941 for ['[CLS] arc gene goddess following with georgian conversionh kilometers fatal station [SEP]']
[Init] best perm rec loss: 1.361145257949829 for ['[CLS] fatal with goddess geneh georgian conversion kilometers arc station following [SEP]']
[Init] best perm rec loss: 1.3605331182479858 for ['[CLS] kilometers gene following fatal with goddess georgian arc conversion stationh [SEP]']
[Init] best perm rec loss: 1.360186219215393 for ['[CLS] geneh conversion station with kilometers following georgian arc fatal goddess [SEP]']
[Init] best perm rec loss: 1.3596504926681519 for ['[CLS]h kilometers conversion fatal following station goddess with georgian gene arc [SEP]']
[Init] best perm rec loss: 1.3583624362945557 for ['[CLS] kilometers with stationh conversion fatal goddess arc gene following georgian [SEP]']
[Init] best perm rec loss: 1.3579857349395752 for ['[CLS] georgian geneh station with fatal conversion kilometers following arc goddess [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.566 (perp=10.810, rec=0.398, cos=0.006), tot_loss_proj:3.790 [t=0.25s]
prediction: ['[CLS] can thinkth monsters mercy : repertoire gained full cent, [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.152 (perp=9.119, rec=0.317, cos=0.011), tot_loss_proj:2.951 [t=0.28s]
prediction: ["[CLS]'want away too too : think talent want too about [SEP]"]
[ 150/ 500] tot_loss=1.868 (perp=8.158, rec=0.231, cos=0.005), tot_loss_proj:2.491 [t=0.25s]
prediction: ["[CLS]'want too too too - think about want too much [SEP]"]
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=1.657 (perp=7.450, rec=0.164, cos=0.003), tot_loss_proj:2.353 [t=0.26s]
prediction: ['[CLS] too want too too on - think about want too much [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=1.631 (perp=7.495, rec=0.130, cos=0.002), tot_loss_proj:2.427 [t=0.26s]
prediction: ['[CLS] too too want too on john think about want too much [SEP]']
[ 300/ 500] tot_loss=1.732 (perp=8.117, rec=0.108, cos=0.000), tot_loss_proj:2.705 [t=0.26s]
prediction: ['[CLS] too too want we on what think about want too much [SEP]']
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=1.776 (perp=8.362, rec=0.103, cos=0.000), tot_loss_proj:2.588 [t=0.25s]
prediction: ['[CLS] too too want what on what think about want too much [SEP]']
Attempt swap
Swapped tokens
[ 400/ 500] tot_loss=1.646 (perp=7.764, rec=0.093, cos=0.000), tot_loss_proj:2.523 [t=0.25s]
prediction: ['[CLS] too want too what on what think about want too much [SEP]']
[ 450/ 500] tot_loss=1.651 (perp=7.790, rec=0.093, cos=0.000), tot_loss_proj:2.502 [t=0.28s]
prediction: ['[CLS] too want too what to what think about want too much [SEP]']
Attempt swap
Moved token
[ 500/ 500] tot_loss=1.457 (perp=6.743, rec=0.107, cos=0.000), tot_loss_proj:2.255 [t=0.26s]
prediction: ['[CLS] too want too what to think about want too much what [SEP]']
Done with input #17 of 100.
reference: 
========================
[CLS] want to think too much about what's going on [SEP]
========================
predicted: 
========================
[CLS] too want too what to what think about want too much [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 72.000 | p: 69.231 | r: 75.000
rouge2     | fm: 8.696 | p: 8.333 | r: 9.091
rougeL     | fm: 56.000 | p: 53.846 | r: 58.333
rougeLsum  | fm: 56.000 | p: 53.846 | r: 58.333
r1fm+r2fm = 80.696

[Aggregate metrics]:
rouge1     | fm: 89.033 | p: 87.946 | r: 90.365
rouge2     | fm: 52.256 | p: 51.669 | r: 52.881
rougeL     | fm: 77.346 | p: 76.458 | r: 78.366
rougeLsum  | fm: 76.598 | p: 75.732 | r: 77.593
r1fm+r2fm = 141.289

input #17 time: 0:03:26 | total time: 1:03:07


Running input #18 of 100.
reference: 
========================
invigorating 
========================
*********************************
*********************************
average of cosine similarity 0.9993194147820215
highest_index [0]
highest [0.9993194147820215]
Debug: ids_shape = 6, pads = [6]
Debug: input ids = tensor([[  101,  1999,  5737, 20255,  5844,   102]], device='cuda:0')
Debug: ref = ['[CLS] invigorating [SEP]']
[Init] best rec loss: 1.974354863166809 for ['[CLS] roadhosis purple couple [SEP]']
[Init] best rec loss: 1.9697707891464233 for ['[CLS] death laytonning segregation [SEP]']
[Init] best rec loss: 1.9564363956451416 for ['[CLS] bound dvd lead grace [SEP]']
[Init] best rec loss: 1.8697879314422607 for ['[CLS]ments vs olsen gaelic [SEP]']
[Init] best rec loss: 1.7662303447723389 for ['[CLS] says oh dynasty watershed [SEP]']
[Init] best rec loss: 1.67368483543396 for ['[CLS] disappointednce secret running [SEP]']
[Init] best rec loss: 1.5851483345031738 for ['[CLS] with thy commission operating [SEP]']
[Init] best rec loss: 1.3104640245437622 for ['[CLS] dual circle duodle [SEP]']
[Init] best rec loss: 1.2881578207015991 for ['[CLS] canellant replication calm [SEP]']
[Init] best perm rec loss: 1.2752639055252075 for ['[CLS] replicationellant can calm [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=3.087 (perp=13.650, rec=0.351, cos=0.006), tot_loss_proj:3.368 [t=0.26s]
prediction: ['[CLS] treasure visual basedising [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.550 (perp=11.332, rec=0.280, cos=0.003), tot_loss_proj:4.169 [t=0.25s]
prediction: ['[CLS] considered based visualating [SEP]']
[ 150/ 500] tot_loss=1.784 (perp=7.761, rec=0.229, cos=0.003), tot_loss_proj:2.208 [t=0.26s]
prediction: ['[CLS] presentvigorating [SEP]']
Attempt swap
[ 200/ 500] tot_loss=1.311 (perp=5.588, rec=0.191, cos=0.002), tot_loss_proj:1.212 [t=0.25s]
prediction: ['[CLS] invigorating [SEP]']
Attempt swap
[ 250/ 500] tot_loss=1.210 (perp=5.588, rec=0.091, cos=0.001), tot_loss_proj:1.199 [t=0.25s]
prediction: ['[CLS] invigorating [SEP]']
[ 300/ 500] tot_loss=1.182 (perp=5.588, rec=0.063, cos=0.001), tot_loss_proj:1.194 [t=0.26s]
prediction: ['[CLS] invigorating [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.173 (perp=5.588, rec=0.055, cos=0.001), tot_loss_proj:1.179 [t=0.25s]
prediction: ['[CLS] invigorating [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.186 (perp=5.588, rec=0.068, cos=0.001), tot_loss_proj:1.199 [t=0.26s]
prediction: ['[CLS] invigorating [SEP]']
[ 450/ 500] tot_loss=1.183 (perp=5.588, rec=0.064, cos=0.001), tot_loss_proj:1.188 [t=0.26s]
prediction: ['[CLS] invigorating [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.181 (perp=5.588, rec=0.062, cos=0.001), tot_loss_proj:1.188 [t=0.27s]
prediction: ['[CLS] invigorating [SEP]']
Done with input #18 of 100.
reference: 
========================
[CLS] invigorating [SEP]
========================
predicted: 
========================
[CLS] invigorating [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 89.632 | p: 88.561 | r: 90.900
rouge2     | fm: 54.693 | p: 54.273 | r: 55.397
rougeL     | fm: 78.136 | p: 77.309 | r: 79.001
rougeLsum  | fm: 77.976 | p: 77.195 | r: 78.922
r1fm+r2fm = 144.325

input #18 time: 0:03:28 | total time: 1:06:35


Running input #19 of 100.
reference: 
========================
to infamy 
========================
*********************************
*********************************
average of cosine similarity 0.9993635209107408
highest_index [0]
highest [0.9993635209107408]
Debug: ids_shape = 6, pads = [6]
Debug: input ids = tensor([[ 101, 2000, 1999, 7011, 8029,  102]], device='cuda:0')
Debug: ref = ['[CLS] to infamy [SEP]']
[Init] best rec loss: 1.458032250404358 for ['[CLS] elevator swings 1941 ser [SEP]']
[Init] best rec loss: 1.306396722793579 for ['[CLS] [MASK] bishop split jay [SEP]']
[Init] best rec loss: 1.226775884628296 for ['[CLS] really is horse cast [SEP]']
[Init] best rec loss: 1.1880154609680176 for ['[CLS] interstate selectedzuki symmetry [SEP]']
[Init] best rec loss: 1.1041380167007446 for ['[CLS] intra raf soviet events [SEP]']
[Init] best perm rec loss: 1.1023863554000854 for ['[CLS] raf soviet events intra [SEP]']
[Init] best perm rec loss: 1.099515676498413 for ['[CLS] intra events soviet raf [SEP]']
[Init] best perm rec loss: 1.0991603136062622 for ['[CLS] soviet intra events raf [SEP]']
[Init] best perm rec loss: 1.0963892936706543 for ['[CLS] soviet events intra raf [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.540 (perp=10.628, rec=0.397, cos=0.017), tot_loss_proj:3.631 [t=0.25s]
prediction: ['[CLS] eastng episode actress [SEP]']
Attempt swap
Moved sequence
[ 100/ 500] tot_loss=2.184 (perp=9.390, rec=0.299, cos=0.006), tot_loss_proj:3.106 [t=0.26s]
prediction: ['[CLS] tofamyfa [SEP]']
[ 150/ 500] tot_loss=2.118 (perp=9.749, rec=0.166, cos=0.002), tot_loss_proj:3.173 [t=0.25s]
prediction: ['[CLS] tofamymy [SEP]']
Attempt swap
[ 200/ 500] tot_loss=2.068 (perp=9.749, rec=0.114, cos=0.004), tot_loss_proj:3.183 [t=0.28s]
prediction: ['[CLS] tofamymy [SEP]']
Attempt swap
[ 250/ 500] tot_loss=2.062 (perp=9.749, rec=0.107, cos=0.006), tot_loss_proj:3.186 [t=0.26s]
prediction: ['[CLS] tofamymy [SEP]']
[ 300/ 500] tot_loss=2.059 (perp=9.749, rec=0.109, cos=0.001), tot_loss_proj:3.189 [t=0.25s]
prediction: ['[CLS] tofamymy [SEP]']
Attempt swap
[ 350/ 500] tot_loss=2.051 (perp=9.749, rec=0.100, cos=0.001), tot_loss_proj:3.182 [t=0.27s]
prediction: ['[CLS] tofamymy [SEP]']
Attempt swap
[ 400/ 500] tot_loss=2.055 (perp=9.749, rec=0.105, cos=0.001), tot_loss_proj:3.192 [t=0.25s]
prediction: ['[CLS] tofamymy [SEP]']
[ 450/ 500] tot_loss=2.052 (perp=9.749, rec=0.101, cos=0.002), tot_loss_proj:3.188 [t=0.25s]
prediction: ['[CLS] tofamymy [SEP]']
Attempt swap
[ 500/ 500] tot_loss=2.056 (perp=9.749, rec=0.106, cos=0.001), tot_loss_proj:3.192 [t=0.25s]
prediction: ['[CLS] tofamymy [SEP]']
Done with input #19 of 100.
reference: 
========================
[CLS] to infamy [SEP]
========================
predicted: 
========================
[CLS] tofamymy [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 57.143 | p: 66.667 | r: 50.000
rouge2     | fm: 0.000 | p: 0.000 | r: 0.000
rougeL     | fm: 57.143 | p: 66.667 | r: 50.000
rougeLsum  | fm: 57.143 | p: 66.667 | r: 50.000
r1fm+r2fm = 57.143

[Aggregate metrics]:
rouge1     | fm: 88.023 | p: 87.457 | r: 88.800
rouge2     | fm: 52.124 | p: 51.712 | r: 52.638
rougeL     | fm: 77.254 | p: 77.075 | r: 77.793
rougeLsum  | fm: 76.744 | p: 76.483 | r: 77.335
r1fm+r2fm = 140.147

input #19 time: 0:03:26 | total time: 1:10:02


Running input #20 of 100.
reference: 
========================
the perverse pleasure 
========================
*********************************
*********************************
average of cosine similarity 0.9992466071766843
highest_index [0]
highest [0.9992466071766843]
Debug: ids_shape = 6, pads = [6]
Debug: input ids = tensor([[  101,  1996,  2566, 16070,  5165,   102]], device='cuda:0')
Debug: ref = ['[CLS] the perverse pleasure [SEP]']
[Init] best rec loss: 1.7585136890411377 for ['[CLS] beast sometimes appearance lying [SEP]']
[Init] best rec loss: 1.6632047891616821 for ['[CLS] map your included adventure [SEP]']
[Init] best rec loss: 1.4528199434280396 for ['[CLS] flashed totalrricular women [SEP]']
[Init] best rec loss: 1.4066518545150757 for ['[CLS] trialuce tai sweet [SEP]']
[Init] best rec loss: 1.3769547939300537 for ['[CLS]nst 2018 principles arguing [SEP]']
[Init] best rec loss: 1.322018027305603 for ['[CLS] jensen eden blackwell is [SEP]']
[Init] best rec loss: 1.2753173112869263 for ['[CLS] storylineness [CLS]xi [SEP]']
[Init] best perm rec loss: 1.2749077081680298 for ['[CLS]ness storylinexi [CLS] [SEP]']
[Init] best perm rec loss: 1.2715117931365967 for ['[CLS] storylinexiness [CLS] [SEP]']
[Init] best perm rec loss: 1.2714813947677612 for ['[CLS] storylinenessxi [CLS] [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.560 (perp=10.348, rec=0.465, cos=0.026), tot_loss_proj:3.670 [t=0.28s]
prediction: ['[CLS] great darkness rewarded [SEP] [SEP]']
Attempt swap
Moved sequence
[ 100/ 500] tot_loss=2.643 (perp=11.460, rec=0.335, cos=0.016), tot_loss_proj:3.395 [t=0.28s]
prediction: ['[CLS] economic theverse pleasure [SEP]']
[ 150/ 500] tot_loss=1.962 (perp=8.761, rec=0.206, cos=0.004), tot_loss_proj:2.785 [t=0.25s]
prediction: ['[CLS] above theverse pleasure [SEP]']
Attempt swap
Moved token
[ 200/ 500] tot_loss=2.058 (perp=9.438, rec=0.168, cos=0.003), tot_loss_proj:3.002 [t=0.26s]
prediction: ['[CLS]verse the pleasureverse [SEP]']
Attempt swap
Moved token
[ 250/ 500] tot_loss=2.325 (perp=10.498, rec=0.216, cos=0.010), tot_loss_proj:3.062 [t=0.25s]
prediction: ['[CLS] the pleasure¨verse [SEP]']
[ 300/ 500] tot_loss=2.258 (perp=10.498, rec=0.156, cos=0.003), tot_loss_proj:3.054 [t=0.27s]
prediction: ['[CLS] the pleasure¨verse [SEP]']
Attempt swap
Moved token
[ 350/ 500] tot_loss=1.858 (perp=8.617, rec=0.133, cos=0.002), tot_loss_proj:2.776 [t=0.25s]
prediction: ['[CLS] the pleasure pleasureverse [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.857 (perp=8.617, rec=0.132, cos=0.002), tot_loss_proj:2.771 [t=0.25s]
prediction: ['[CLS] the pleasure pleasureverse [SEP]']
[ 450/ 500] tot_loss=1.830 (perp=8.617, rec=0.105, cos=0.002), tot_loss_proj:2.780 [t=0.25s]
prediction: ['[CLS] the pleasure pleasureverse [SEP]']
Attempt swap
[ 500/ 500] tot_loss=2.506 (perp=12.001, rec=0.104, cos=0.002), tot_loss_proj:3.681 [t=0.29s]
prediction: ['[CLS] per¨ pleasureverse [SEP]']
Done with input #20 of 100.
reference: 
========================
[CLS] the perverse pleasure [SEP]
========================
predicted: 
========================
[CLS] the¨ pleasureverse [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 66.667 | p: 75.000 | r: 60.000
rouge2     | fm: 28.571 | p: 33.333 | r: 25.000
rougeL     | fm: 66.667 | p: 75.000 | r: 60.000
rougeLsum  | fm: 66.667 | p: 75.000 | r: 60.000
r1fm+r2fm = 95.238

[Aggregate metrics]:
rouge1     | fm: 86.993 | p: 86.793 | r: 87.426
rouge2     | fm: 50.705 | p: 50.537 | r: 51.182
rougeL     | fm: 76.641 | p: 76.914 | r: 76.880
rougeLsum  | fm: 76.302 | p: 76.494 | r: 76.597
r1fm+r2fm = 137.698

input #20 time: 0:03:27 | total time: 1:13:29


Running input #21 of 100.
reference: 
========================
the way this all works out makes the women look more like stereotypical caretakers and moral teachers , instead of serious athletes . 
========================
*********************************
*********************************
average of cosine similarity 0.999323347023505
highest_index [0]
highest [0.999323347023505]
Debug: ids_shape = 27, pads = [27]
Debug: input ids = tensor([[  101,  1996,  2126,  2023,  2035,  2573,  2041,  3084,  1996,  2308,
          2298,  2062,  2066, 12991, 27086, 17600,  2015,  1998,  7191,  5089,
          1010,  2612,  1997,  3809,  7576,  1012,   102]], device='cuda:0')
Debug: ref = ['[CLS] the way this all works out makes the women look more like stereotypical caretakers and moral teachers, instead of serious athletes. [SEP]']
[Init] best rec loss: 1.8797924518585205 for ["[CLS] pride sufficient stakes favor background pronounced classic uncomfortable devlin binding instead hr inner paolo finished ld focused coincidencewave corporation'making perry manitoba diet [SEP]"]
[Init] best rec loss: 1.6756571531295776 for ['[CLS]rino moons first aslaw registered. according summer testified aid attacked artist power weekend texas total faced scoring drive c las ladies okay political [SEP]']
[Init] best rec loss: 1.6570099592208862 for ['[CLS] club life only involving drive quebec bain than v vary proceeding cave rebellion gabriel freedom intohmi set tour - copies light howeday grin [SEP]']
[Init] best rec loss: 1.6356710195541382 for ['[CLS] deepvao pit sports lines alter holding tight successfully aged logo merlin do rickrier involved place fancy the nay bomber kylie gp re which [SEP]']
[Init] best rec loss: 1.525413990020752 for ['[CLS] rising paperсhl language existence describednt clan yournnek formerly western miss whose withdrawal work rainbow boss true keith throughout statutes along [SEP]']
[Init] best rec loss: 1.5040524005889893 for ['[CLS] is waiter know performs ecolecaster public alta jess hub shower wrote do leaf la hyper delilah objectookure slit telecommunications rein or last [SEP]']
[Init] best rec loss: 1.4968369007110596 for ['[CLS] bonn university on ashe shot wearing rockerlica classification speed non burning glad california againstanding colt timing mouthigo gun machinery score liked seems [SEP]']
[Init] best rec loss: 1.2692524194717407 for ['[CLS] vii bent connecticut pose, golden there itemback baby dee size loan especially labor stonytaking according [UNK] side she fauna general situations rights [SEP]']
[Init] best perm rec loss: 1.2645514011383057 for ['[CLS] rights loan especially connecticut there bent labor, she size general situationsback item vii pose baby stony deetaking [UNK] golden fauna according side [SEP]']
[Init] best perm rec loss: 1.2639286518096924 for ['[CLS] pose bent rights vii labor loanback dee, itemtaking general fauna stony size she connecticut side according especially golden situations [UNK] baby there [SEP]']
[Init] best perm rec loss: 1.2572057247161865 for ['[CLS], situations connecticutback especially pose item benttaking dee golden rights general stony she loan size according [UNK] labor vii fauna there baby side [SEP]']
[Init] best perm rec loss: 1.2564804553985596 for ['[CLS] especiallytaking there she goldenback dee bent pose stony, situations loan size rights item [UNK] labor general baby connecticut vii fauna side according [SEP]']
[Init] best perm rec loss: 1.2533059120178223 for ['[CLS] general situations connecticut golden labor dee according bent especially baby there loan stonyback pose side [UNK] she vii item size rights,taking fauna [SEP]']
[Init] best perm rec loss: 1.2523531913757324 for ['[CLS] there baby loan vii fauna side she connecticut pose stony labor bent situations item according [UNK] especially deetaking golden, sizeback rights general [SEP]']
[Init] best perm rec loss: 1.2521893978118896 for ['[CLS] loan size baby, especially stony situations item bent there according connecticuttaking side fauna poseback dee golden general [UNK] she labor rights vii [SEP]']
[Init] best perm rec loss: 1.2520167827606201 for ['[CLS] rights golden according loan bent [UNK] situations connecticut stony there vii general dee size babyback labor she,taking fauna item especially side pose [SEP]']
[Init] best perm rec loss: 1.2508926391601562 for ['[CLS] fauna she vii [UNK] rightsback bent stony baby there connecticut item situations labor according golden size, side loantaking especially pose general dee [SEP]']
[Init] best perm rec loss: 1.2494364976882935 for ['[CLS] especially loanback vii [UNK] situations baby golden size bent connecticut dee according stony pose general labor rights fauna she item theretaking, side [SEP]']
[Init] best perm rec loss: 1.2471531629562378 for ['[CLS] she especially [UNK] item pose golden vii generalback there sidetaking rights loan, dee baby stony labor size according bent fauna connecticut situations [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.702 (perp=11.319, rec=0.435, cos=0.003), tot_loss_proj:3.272 [t=0.30s]
prediction: ['[CLS] that excuses australian sign bribe idiot saddam conduct flat § - section corruption label if tapelassified during laborhow caused wrong wrong newspaper witnesses [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.616 (perp=11.300, rec=0.352, cos=0.003), tot_loss_proj:3.108 [t=0.26s]
prediction: ['[CLS] thattypical australian signs - on security conduct.ducted rating title furiously abstract or american pills after laborhow explain instead wrong rate teachers [SEP]']
[ 150/ 500] tot_loss=2.519 (perp=11.031, rec=0.309, cos=0.003), tot_loss_proj:3.073 [t=0.27s]
prediction: ["[CLS] that typical australian signs'the rape conduct. oblast rating tube make abstract or reported gerais after this vietnam explain instead hands paid teachers [SEP]"]
Attempt swap
Moved sequence
[ 200/ 500] tot_loss=2.587 (perp=11.218, rec=0.340, cos=0.003), tot_loss_proj:3.133 [t=0.26s]
prediction: ['[CLS] how assume leave archdiocese - by closed dirty american cheated almost ¹⁄₂ vietnam. differential locked deathhdi committee or caused instead traffics teachers [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=2.514 (perp=11.031, rec=0.306, cos=0.002), tot_loss_proj:3.415 [t=0.26s]
prediction: ['[CLS] how assume not way - on closed dirty treaty gerais some our woman withoutchenko locked takehdi offenders or caused instead affairss women [SEP]']
[ 300/ 500] tot_loss=2.340 (perp=10.280, rec=0.282, cos=0.001), tot_loss_proj:3.073 [t=0.27s]
prediction: ['[CLS] how look con way - on closed dirty rules gerais some our woman. violation locked department makes offenders or looks instead off works women [SEP]']
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=2.430 (perp=10.794, rec=0.270, cos=0.000), tot_loss_proj:3.353 [t=0.25s]
prediction: ['[CLS] how dirty speech way - on closed look campaign gerais turned our women.typical locked department makes leaders or looks instead problems works women [SEP]']
Attempt swap
Swapped tokens
[ 400/ 500] tot_loss=2.341 (perp=10.357, rec=0.266, cos=0.004), tot_loss_proj:2.887 [t=0.26s]
prediction: ['[CLS] how circumstances speech way - killing out look ligue gerais turned all locked.typical women department makes leaders or looks instead off works women [SEP]']
[ 450/ 500] tot_loss=2.284 (perp=10.150, rec=0.252, cos=0.002), tot_loss_proj:2.894 [t=0.26s]
prediction: ['[CLS] how circumstances group way - killing out look women gerais sort all locked.typical woman faculty makes leaders or looks instead off serious women [SEP]']
Attempt swap
Moved token
[ 500/ 500] tot_loss=2.174 (perp=9.685, rec=0.237, cos=0.001), tot_loss_proj:3.070 [t=0.28s]
prediction: ['[CLS] how relief group way - a out look gerais women turns all locked.typical woman faculty makes teachers or looks instead off serious women [SEP]']
Done with input #21 of 100.
reference: 
========================
[CLS] the way this all works out makes the women look more like stereotypical caretakers and moral teachers, instead of serious athletes. [SEP]
========================
predicted: 
========================
[CLS] how relief group way - a out look gerais women turns all locked.typical woman faculty makes teachers or looks instead off serious women [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 45.833 | p: 44.000 | r: 47.826
rouge2     | fm: 0.000 | p: 0.000 | r: 0.000
rougeL     | fm: 33.333 | p: 32.000 | r: 34.783
rougeLsum  | fm: 33.333 | p: 32.000 | r: 34.783
r1fm+r2fm = 45.833

[Aggregate metrics]:
rouge1     | fm: 84.998 | p: 84.907 | r: 85.630
rouge2     | fm: 48.473 | p: 48.341 | r: 48.765
rougeL     | fm: 74.601 | p: 74.722 | r: 74.728
rougeLsum  | fm: 74.405 | p: 74.349 | r: 74.615
r1fm+r2fm = 133.471

input #21 time: 0:03:30 | total time: 1:16:59


Running input #22 of 100.
reference: 
========================
a successful adaptation and an enjoyable film in its own right 
========================
*********************************
*********************************
average of cosine similarity 0.9993366894184397
highest_index [0]
highest [0.9993366894184397]
Debug: ids_shape = 13, pads = [13]
Debug: input ids = tensor([[  101,  1037,  3144,  6789,  1998,  2019, 22249,  2143,  1999,  2049,
          2219,  2157,   102]], device='cuda:0')
Debug: ref = ['[CLS] a successful adaptation and an enjoyable film in its own right [SEP]']
[Init] best rec loss: 1.9521197080612183 for ['[CLS] followingaway leoong cyrusso class inside dane gothic major [SEP]']
[Init] best rec loss: 1.8905121088027954 for ['[CLS] shakespeare operation emerald hip year art mcdowell model apart league rate [SEP]']
[Init] best rec loss: 1.8834716081619263 for ['[CLS] pseudonym court flynn fed soxnight golden remains lloyd colon op [SEP]']
[Init] best rec loss: 1.881540298461914 for ['[CLS]berries thirds rounds exit whole reaction flux packages advertising dish habit [SEP]']
[Init] best rec loss: 1.849566102027893 for ['[CLS] mines ) organ yes deck sessions mainly steady introduction arson dates [SEP]']
[Init] best rec loss: 1.8134469985961914 for ['[CLS] av waitingalis reception pillar anna deal mentionedhl etc showers [SEP]']
[Init] best rec loss: 1.7512906789779663 for ['[CLS] cloud road hey wynn under diiny stalk seduce variousour [SEP]']
[Init] best rec loss: 1.7188242673873901 for ['[CLS] dialectotte [MASK] type became designing aired replacing piece dear travel [SEP]']
[Init] best rec loss: 1.711775779724121 for ['[CLS] immortal dos standing commentarytort placehim corporal full cruisers carrier [SEP]']
[Init] best rec loss: 1.6849783658981323 for ['[CLS] sans services downstairsgar arched take network before simply dean jurgen [SEP]']
[Init] best rec loss: 1.6268218755722046 for ['[CLS] kids function phoenix chinese set boarders over her schedule laughter [SEP]']
[Init] best perm rec loss: 1.6199898719787598 for ['[CLS] her function board kids phoenix laughter set over chineseers schedule [SEP]']
[Init] best perm rec loss: 1.6110483407974243 for ['[CLS] phoenix her laughter over function schedule chinese boarders kids set [SEP]']
[Init] best perm rec loss: 1.610538125038147 for ['[CLS] function over schedule phoenix herers chinese kids laughter board set [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=3.263 (perp=12.449, rec=0.755, cos=0.017), tot_loss_proj:3.991 [t=0.24s]
prediction: ['[CLS] earl. actuallyss symposium dead waste lamar ram accidentotho [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=3.046 (perp=11.887, rec=0.660, cos=0.009), tot_loss_proj:3.826 [t=0.25s]
prediction: ['[CLS] zane. penalpur debris else waste probability senate.otho [SEP]']
[ 150/ 500] tot_loss=3.235 (perp=13.050, rec=0.621, cos=0.003), tot_loss_proj:4.598 [t=0.26s]
prediction: ['[CLS] besides. complaintspur submitted hundred film crime gunfire behind expensive [SEP]']
Attempt swap
Moved token
[ 200/ 500] tot_loss=2.908 (perp=11.510, rec=0.600, cos=0.006), tot_loss_proj:4.188 [t=0.26s]
prediction: ['[CLS] survived complaintspur submitted hundred film crime gunfire of expensive. [SEP]']
Attempt swap
Moved sequence
[ 250/ 500] tot_loss=2.715 (perp=10.771, rec=0.557, cos=0.004), tot_loss_proj:4.065 [t=0.25s]
prediction: ['[CLS] survived complaintspur wong hundred film crime of minor freshwater. [SEP]']
[ 300/ 500] tot_loss=2.749 (perp=10.916, rec=0.537, cos=0.029), tot_loss_proj:4.186 [t=0.25s]
prediction: ['[CLS] survived productspur sense hundred adaptation crime of minor genre. [SEP]']
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=3.414 (perp=14.403, rec=0.530, cos=0.004), tot_loss_proj:4.857 [t=0.25s]
prediction: ['[CLS] avid productspur sense hundred adaptation fled prevented minor freshwater dozens [SEP]']
Attempt swap
Swapped tokens
[ 400/ 500] tot_loss=2.935 (perp=12.160, rec=0.501, cos=0.002), tot_loss_proj:4.143 [t=0.25s]
prediction: ['[CLS] recorded fledpur sense hundred adaptation products prevented a fun dozens [SEP]']
[ 450/ 500] tot_loss=3.041 (perp=12.790, rec=0.480, cos=0.004), tot_loss_proj:4.493 [t=0.27s]
prediction: ['[CLS] adopted fledpur sense interesting adaptation products prevented a fun dozens [SEP]']
Attempt swap
Swapped tokens
[ 500/ 500] tot_loss=2.931 (perp=12.107, rec=0.509, cos=0.001), tot_loss_proj:3.944 [t=0.27s]
prediction: ['[CLS] adopted fledpur despite smoothly adaptation products sense a fun multi [SEP]']
Done with input #22 of 100.
reference: 
========================
[CLS] a successful adaptation and an enjoyable film in its own right [SEP]
========================
predicted: 
========================
[CLS] adopted fledpur sense interesting adaptation products prevented a fun dozens [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 32.000 | p: 33.333 | r: 30.769
rouge2     | fm: 0.000 | p: 0.000 | r: 0.000
rougeL     | fm: 24.000 | p: 25.000 | r: 23.077
rougeLsum  | fm: 24.000 | p: 25.000 | r: 23.077
r1fm+r2fm = 32.000

[Aggregate metrics]:
rouge1     | fm: 82.898 | p: 82.808 | r: 83.282
rouge2     | fm: 46.512 | p: 46.185 | r: 46.840
rougeL     | fm: 72.630 | p: 72.713 | r: 72.743
rougeLsum  | fm: 72.290 | p: 72.284 | r: 72.433
r1fm+r2fm = 129.411

input #22 time: 0:03:27 | total time: 1:20:27


Running input #23 of 100.
reference: 
========================
while some will object to the idea of a vietnam picture with such a rah-rah , patriotic tone , soldiers ultimately achieves its main strategic objective : dramatizing the human cost of the conflict that came to define a generation . 
========================
*********************************
*********************************
average of cosine similarity 0.999235259621351
highest_index [0]
highest [0.999235259621351]
Debug: ids_shape = 50, pads = [50]
Debug: input ids = tensor([[  101,  2096,  2070,  2097,  4874,  2000,  1996,  2801,  1997,  1037,
          5148,  3861,  2007,  2107,  1037, 10958,  2232,  1011, 10958,  2232,
          1010, 14314,  4309,  1010,  3548,  4821,  6162,  2015,  2049,  2364,
          6143,  7863,  1024,  3689,  3775,  6774,  1996,  2529,  3465,  1997,
          1996,  4736,  2008,  2234,  2000,  9375,  1037,  4245,  1012,   102]],
       device='cuda:0')
Debug: ref = ['[CLS] while some will object to the idea of a vietnam picture with such a rah - rah, patriotic tone, soldiers ultimately achieves its main strategic objective : dramatizing the human cost of the conflict that came to define a generation. [SEP]']
[Init] best rec loss: 1.383732795715332 for ['[CLS] rank brazil郎 glasscutiologymark term hire reach soup pursuit respond township lagos notmute cum dusk once sex role have located down divers johnnieᴵ wanted goodness scent pattern met 6 baltic nes travel free stale westward julian towns mono katy incarnation jenks owens kind [SEP]']
[Init] best rec loss: 1.3736612796783447 for ['[CLS] jumped its ion [MASK] deep spirit tracks controls spun donated tape calendar ineligible martial airport breaths complexvas net straight vs # jake featurebal each roots record death share troubles chance scores mate frank holding quest exactlyul governments win far ap gathering toysience married club [SEP]']
[Init] best rec loss: 1.3212342262268066 for ['[CLS] stephens type specialty agedre vu disney resourcexide raeiansia are roadbbly feeling daphne amazon detail demon ii smartershi remembertained mad injunction israel personal network elbow conceptbbled president where rep example researchapple mr rangers resisted revival accessible self na express legion [SEP]']
[Init] best rec loss: 1.3039929866790771 for ['[CLS]cation oystermel though painfies aftermath faction micro ec decommissioned hole federation educated wi looking ban office mean creation positional vi morning envy fair ken goodwill sons no give aren para shops calendar concert beingsian you pl denmark love platform battle flags astronomy rome asking [SEP]']
[Init] best rec loss: 1.2455841302871704 for ['[CLS] talent cause skirt handled ⁴ anne pieces mine! caused safety tor goal fore 2014 residents chosen offering chiefs number sun consumergementni property riding dolphin exchequerada saw occupants trades vale course kind coronation ballroom dona village totally selena winds firstd sums manga square scholar [SEP]']
[Init] best rec loss: 1.2352474927902222 for ['[CLS] drugfl vienna chop ; wound shop wonder main added founded lennox bridge gel residential rich kilometers facing countries seal adults captain wet interstate tea saved mr hawk withdrawal indeed temperaly sent daily life ₱ rail era seasons bottom champion herselfsta? context teen ready airfield [SEP]']
[Init] best perm rec loss: 1.2314822673797607 for ['[CLS] indeed champion kilometers withdrawal gel seal tea drug wonder ; founded vienna era herself ₱ added residential daily bridge sent adults airfield captain mr ready contextsta bottom seasons shop rail wet life hawkaly countries teen facing richfl lennox saved main? interstate wound temper chop [SEP]']
[Init] best perm rec loss: 1.2305387258529663 for ['[CLS] sent shop wound airfield captain wetfl vienna countries mr ready main rail seasons daily drug facing seal teen saved ; bridge adults temper added context withdrawal champion wondersta rich era kilometers bottom hawk tea residential? foundedaly indeed ₱ interstate life chop gel lennox herself [SEP]']
[Init] best perm rec loss: 1.2290924787521362 for ['[CLS] indeed ₱ facing seasonsfl rich bridge main mr gel wonder vienna founded ready champion chop hawk wet context kilometers captain life teen sent bottom saved woundaly rail era lennox withdrawal tea drug interstate temper seal residential added shop airfield daily herself countries ;sta adults? [SEP]']
[Init] best perm rec loss: 1.2287770509719849 for ['[CLS] airfield rail added hawk context indeed countries withdrawal shop rich vienna seal captain champion mr sent life temper daily gel teen tea bridge ₱ residential adults saved drugsta bottom founded lennox ready facing ; interstate woundfl kilometers wet main wonder chop seasonsaly era herself? [SEP]']
[Init] best perm rec loss: 1.2277514934539795 for ['[CLS] residential lennoxfl rich hawk captain ₱aly tea daily main drug sent viennasta facing? ; gel wound context countries temper teen shop champion seal seasons wet herself airfield adults founded saved life ready wonder mr interstate bottom era indeed chop added kilometers withdrawal bridge rail [SEP]']
[Init] best perm rec loss: 1.2267485857009888 for ['[CLS] seal ready mr captainfl rich shop drug main rail residential era gel temper saved? countries kilometers tea bottom interstate added context life bridge lennox vienna champion herself daily facing chop withdrawal adults teen indeed ₱ ; airfield woundsta seasons hawk wonder wet founded sentaly [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.860 (perp=12.140, rec=0.429, cos=0.002), tot_loss_proj:3.626 [t=0.25s]
prediction: ['[CLS] outer detachment @ tunnel less ham + abandon downs have project open core editorial watershed critique earthgrowth check helping inner forcetag, pandora hemisphere vietnam project initiative moment bring focus central mission. contained steal education studied narrowingzh, americanstorm science kendra headlogic [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.654 (perp=11.577, rec=0.335, cos=0.003), tot_loss_proj:3.509 [t=0.25s]
prediction: ['[CLS] technical vietnam whose its objective siren + abandon da generation project closed core editorialvor parenting conferencesgrowthistle effective, forcetag in ajax ultimately vietnam project strategy liberation strategic focus withdrew objective : science strategic contained primary message dictionary ; american depth scientists rewards headflower [SEP]']
[ 150/ 500] tot_loss=2.664 (perp=11.731, rec=0.283, cos=0.036), tot_loss_proj:3.592 [t=0.26s]
prediction: ['[CLS] technical fight whose its objective siren thus abandon of matt project the conjunction editorialvorudged conferences compassionateistle effectively, strategictag in maritime ultimately vietnam initiative strategic liberation strategic focus withdrew objective : soldiers strategic strategic main message comparison domestic human war scientific awarded ja jim [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.535 (perp=11.201, rec=0.291, cos=0.004), tot_loss_proj:3.340 [t=0.28s]
prediction: ['[CLS] ancient fight whose its objective psycho though cho neo movementa the designk - sound minus project [CLS] world, film expedition and includes ultimately vietnam waters em drama, story consciousness objective : soldiers strategicject main narrative telling ( civil history european awarded jaل [SEP]']
Attempt swap
Moved token
[ 250/ 500] tot_loss=2.382 (perp=10.828, rec=0.215, cos=0.001), tot_loss_proj:3.275 [t=0.26s]
prediction: ['[CLS] argentina humanitarian whose of environmental strategic science while cho pete movement poster its itsh. music whose project teach world, picture expedition and ultimately vietnam significance strategic drama, story chi objective : soldiersized s main pictureizing ( civil conflict european awarded achieved 9 [SEP]']
[ 300/ 500] tot_loss=2.306 (perp=10.548, rec=0.193, cos=0.004), tot_loss_proj:3.365 [t=0.26s]
prediction: ['[CLS]like patriotic where of its strategic unfolded while ra wooden movement design its theh. music whose thought teach war, pictureh and ultimately vietnam strategic strategic drama, story chi objective : soldiersized s main picturezing ( every conflict european awarded achieve 9 [SEP]']
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=2.144 (perp=9.898, rec=0.162, cos=0.002), tot_loss_proj:3.030 [t=0.25s]
prediction: ['[CLS] away comic the a its strategic science while ra european ideaa its theh, vietnam whose awareness teach war, picture campaign, ultimately vietnam strategic of drama, story chi objective : soldiersized s main picturezing ( every conflict the awarded achieve 9 [SEP]']
Attempt swap
Swapped tokens
[ 400/ 500] tot_loss=2.124 (perp=9.846, rec=0.153, cos=0.001), tot_loss_proj:3.106 [t=0.26s]
prediction: ['[CLS] away comic chi a its strategic science while ra european ideaa its theh, vietnam the wherein teach (, picture campaign ) ultimately vietnam strategic strategic drama, story the objective : soldiersized s main picturezing ( every conflict the awarded achieve 9 [SEP]']
[ 450/ 500] tot_loss=2.126 (perp=9.915, rec=0.141, cos=0.001), tot_loss_proj:2.981 [t=0.25s]
prediction: ['[CLS] away advocatedsu a its strategic notation while ra the ideas its theh, vietnam the generation object (, picture campaign ) ultimately patriotic achieve strategic drama, story the objective : soldiersized s main picturezing ( the conflict the awarded achieve 9 [SEP]']
Attempt swap
Swapped tokens
[ 500/ 500] tot_loss=2.163 (perp=10.112, rec=0.140, cos=0.000), tot_loss_proj:3.087 [t=0.25s]
prediction: ['[CLS] away comic object a its strategic notation while ra the ideas its theh, vietnam the generationsu (, picture tone ) ultimately patriotic achieve strategic drama, story the objective : soldiersdicate s main picturezing ( the conflict the awarded achieve 9 [SEP]']
Done with input #23 of 100.
reference: 
========================
[CLS] while some will object to the idea of a vietnam picture with such a rah - rah, patriotic tone, soldiers ultimately achieves its main strategic objective : dramatizing the human cost of the conflict that came to define a generation. [SEP]
========================
predicted: 
========================
[CLS] away comic object a its strategic notation while ra the ideas its theh, vietnam the generationsu (, picture campaign ) ultimately patriotic achieve strategic drama, story the objective : soldiersdicate s main picturezing ( the conflict the awarded achieve 9 [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 43.590 | p: 44.737 | r: 42.500
rouge2     | fm: 2.632 | p: 2.703 | r: 2.564
rougeL     | fm: 28.205 | p: 28.947 | r: 27.500
rougeLsum  | fm: 28.205 | p: 28.947 | r: 27.500
r1fm+r2fm = 46.221

[Aggregate metrics]:
rouge1     | fm: 81.119 | p: 80.923 | r: 81.549
rouge2     | fm: 44.495 | p: 44.323 | r: 44.723
rougeL     | fm: 70.732 | p: 70.861 | r: 70.812
rougeLsum  | fm: 70.210 | p: 70.317 | r: 70.554
r1fm+r2fm = 125.614

input #23 time: 0:03:29 | total time: 1:23:56


Running input #24 of 100.
reference: 
========================
taken outside the context of the current political climate ( see : terrorists are more evil than ever ! ) 
========================
*********************************
*********************************
average of cosine similarity 0.9993537840940759
highest_index [0]
highest [0.9993537840940759]
Debug: ids_shape = 22, pads = [22]
Debug: input ids = tensor([[  101,  2579,  2648,  1996,  6123,  1997,  1996,  2783,  2576,  4785,
          1006,  2156,  1024, 15554,  2024,  2062,  4763,  2084,  2412,   999,
          1007,   102]], device='cuda:0')
Debug: ref = ['[CLS] taken outside the context of the current political climate ( see : terrorists are more evil than ever! ) [SEP]']
[Init] best rec loss: 1.825258493423462 for ['[CLS] trail floor cadillac partners african titlesin spelling offer van beaten grid managed diedsf came celtic had henry aggregator [SEP]']
[Init] best rec loss: 1.6625887155532837 for ['[CLS] folder 000 tell hurt cooking raised instead hosts z [CLS]ttered lowell breuning mode cells apartments ryel arliche [SEP]']
[Init] best rec loss: 1.662034034729004 for ['[CLS] % wholewell forgotten upon beginning hellolsoc only favor including trailer naval a difficult cards dragons foreign cars [SEP]']
[Init] best rec loss: 1.638895034790039 for ['[CLS] ladder sebastian separation ball ely associated warn bark basis medieval staff music trulycta ensured rick helicopter adjust resolve dia [SEP]']
[Init] best rec loss: 1.3410228490829468 for ['[CLS] lying afford rubbed media ة soft % _truct postage voices ind defending either morning then armeddder melanie return [SEP]']
[Init] best rec loss: 1.323185920715332 for ['[CLS] post la cited north soldiers jasperditional [SEP] shay singer hate male warrantritetype conditionative type above doorbell [SEP]']
[Init] best rec loss: 1.1163339614868164 for ['[CLS] mid play no bush attack arms youngeraneous snow ryu suffer emwyl unless port county village happy bond damned [SEP]']
[Init] best perm rec loss: 1.1137642860412598 for ['[CLS] bondwyl unless county em arms damned happy suffer play attack younger ryu midaneous bush snow village no port [SEP]']
[Init] best perm rec loss: 1.1134698390960693 for ['[CLS] unless play younger ryu attack village no mid damned portwyl happy suffer arms em snow bush bond countyaneous [SEP]']
[Init] best perm rec loss: 1.1096725463867188 for ['[CLS] attack unless em arms happywyl bond mid younger play snow villageaneous no county damned ryu suffer port bush [SEP]']
[Init] best perm rec loss: 1.1079434156417847 for ['[CLS] mid unless ryu play attack nowyl em younger countyaneous bond village happy damned snow bush port suffer arms [SEP]']
[Init] best perm rec loss: 1.107871174812317 for ['[CLS] suffer happy em younger play ryu no mid bond unless snow bush armsaneous damned attack countywyl village port [SEP]']
[Init] best perm rec loss: 1.1060388088226318 for ['[CLS] bond play damned unless county younger no midwyl em attack ryu village snow armsaneous happy bush suffer port [SEP]']
[Init] best perm rec loss: 1.1028209924697876 for ['[CLS] playwyl county em no mid arms bond bush damned snow unless happy ryu younger attack suffer village portaneous [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.774 (perp=11.723, rec=0.426, cos=0.004), tot_loss_proj:3.310 [t=0.25s]
prediction: ['[CLS] armedtated [SEP] stupid aids drug allegedly drug funds¤ against unlessat! coupa drug were hands armed [SEP]']
Attempt swap
Moved token
[ 100/ 500] tot_loss=2.762 (perp=12.045, rec=0.351, cos=0.001), tot_loss_proj:3.320 [t=0.25s]
prediction: ['[CLS] campeonato backwards [SEP] evil aids. countries drug judge¤ taken yard! terrorists victim government species the terrorists knew [SEP]']
[ 150/ 500] tot_loss=2.724 (perp=12.133, rec=0.295, cos=0.003), tot_loss_proj:3.311 [t=0.27s]
prediction: ['[CLS] sc taken outside evil political. subsequently drug slightly¤ taken innocent! terrorists those tech leader the legal knew [SEP]']
Attempt swap
Moved sequence
[ 200/ 500] tot_loss=2.429 (perp=10.809, rec=0.264, cos=0.004), tot_loss_proj:3.402 [t=0.26s]
prediction: ['[CLS] really taken outside evil political. region political slightly¤ taken innocent! the terrorists terrorists controversy the of surroundings [SEP]']
Attempt swap
Moved token
[ 250/ 500] tot_loss=2.179 (perp=9.810, rec=0.216, cos=0.001), tot_loss_proj:2.982 [t=0.25s]
prediction: ['[CLS] really taken outside evil political? context cause slightly outside taken (! the terrorists terrorists context the the context [SEP]']
[ 300/ 500] tot_loss=2.148 (perp=9.805, rec=0.187, cos=0.001), tot_loss_proj:2.967 [t=0.26s]
prediction: ['[CLS] really taken outside evil political. context political slightly outside taken (! the terrorists terrorists context the the context [SEP]']
Attempt swap
Moved token
[ 350/ 500] tot_loss=2.062 (perp=9.350, rec=0.187, cos=0.004), tot_loss_proj:2.809 [t=0.26s]
prediction: ['[CLS] : taken outside evil political context ) climate slightly outside taken (! the terrorists terrorists context the the context [SEP]']
Attempt swap
Moved sequence
[ 400/ 500] tot_loss=1.938 (perp=8.808, rec=0.176, cos=0.001), tot_loss_proj:2.660 [t=0.26s]
prediction: ['[CLS] : taken outside evil political context ) the climate somewhat outside taken (! the terrorists terrorists context the context [SEP]']
[ 450/ 500] tot_loss=1.935 (perp=8.808, rec=0.173, cos=0.001), tot_loss_proj:2.662 [t=0.26s]
prediction: ['[CLS] : taken outside evil political context ) the climate somewhat outside taken (! the terrorists terrorists context the context [SEP]']
Attempt swap
Moved sequence
[ 500/ 500] tot_loss=1.856 (perp=8.475, rec=0.160, cos=0.000), tot_loss_proj:2.524 [t=0.25s]
prediction: ['[CLS] : taken outside evil political context ) the climate current outside taken (! the terrorists context the terrorists context [SEP]']
Done with input #24 of 100.
reference: 
========================
[CLS] taken outside the context of the current political climate ( see : terrorists are more evil than ever! ) [SEP]
========================
predicted: 
========================
[CLS] : taken outside evil political context ) the climate current outside taken (! the terrorists context the terrorists context [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 66.667 | p: 66.667 | r: 66.667
rouge2     | fm: 11.765 | p: 11.765 | r: 11.765
rougeL     | fm: 44.444 | p: 44.444 | r: 44.444
rougeLsum  | fm: 44.444 | p: 44.444 | r: 44.444
r1fm+r2fm = 78.431

[Aggregate metrics]:
rouge1     | fm: 80.507 | p: 80.587 | r: 81.005
rouge2     | fm: 43.154 | p: 43.050 | r: 43.503
rougeL     | fm: 69.687 | p: 69.765 | r: 69.731
rougeLsum  | fm: 69.247 | p: 69.406 | r: 69.296
r1fm+r2fm = 123.660

input #24 time: 0:03:28 | total time: 1:27:25


Running input #25 of 100.
reference: 
========================
strange and beautiful film 
========================
*********************************
*********************************
average of cosine similarity 0.9993176191193096
highest_index [0]
highest [0.9993176191193096]
Debug: ids_shape = 6, pads = [6]
Debug: input ids = tensor([[ 101, 4326, 1998, 3376, 2143,  102]], device='cuda:0')
Debug: ref = ['[CLS] strange and beautiful film [SEP]']
[Init] best rec loss: 2.0016868114471436 for ['[CLS] excess concepcion occupation channel [SEP]']
[Init] best rec loss: 1.8594616651535034 for ['[CLS] memory within ; buy [SEP]']
[Init] best rec loss: 1.7197651863098145 for ['[CLS] merit delaney canoniary [SEP]']
[Init] best rec loss: 1.7120318412780762 for ['[CLS] executive into females he [SEP]']
[Init] best rec loss: 1.6727626323699951 for ['[CLS] james adding letters received [SEP]']
[Init] best rec loss: 1.6727566719055176 for ['[CLS] frequent gailez bane [SEP]']
[Init] best rec loss: 1.5321781635284424 for ['[CLS] mention acre old headline [SEP]']
[Init] best rec loss: 1.4170466661453247 for ['[CLS] mouth oblast cycle jury [SEP]']
[Init] best rec loss: 1.3436115980148315 for ['[CLS] hide a seal mess [SEP]']
[Init] best perm rec loss: 1.3427221775054932 for ['[CLS] seal a mess hide [SEP]']
[Init] best perm rec loss: 1.3401399850845337 for ['[CLS] mess hide a seal [SEP]']
[Init] best perm rec loss: 1.3389146327972412 for ['[CLS] mess a hide seal [SEP]']
[Init] best perm rec loss: 1.3344050645828247 for ['[CLS] mess hide seal a [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.786 (perp=12.281, rec=0.323, cos=0.006), tot_loss_proj:2.917 [t=0.27s]
prediction: ['[CLS] beautiful wow novel mysterious [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=1.884 (perp=8.327, rec=0.216, cos=0.002), tot_loss_proj:1.967 [t=0.26s]
prediction: ['[CLS] beautiful strange beautiful film [SEP]']
[ 150/ 500] tot_loss=1.807 (perp=8.327, rec=0.140, cos=0.002), tot_loss_proj:1.976 [t=0.26s]
prediction: ['[CLS] beautiful strange beautiful film [SEP]']
Attempt swap
Moved token
[ 200/ 500] tot_loss=1.583 (perp=7.298, rec=0.122, cos=0.002), tot_loss_proj:1.773 [t=0.26s]
prediction: ['[CLS] beautiful beautiful strange film [SEP]']
Attempt swap
[ 250/ 500] tot_loss=1.570 (perp=7.298, rec=0.108, cos=0.002), tot_loss_proj:1.767 [t=0.25s]
prediction: ['[CLS] beautiful beautiful strange film [SEP]']
[ 300/ 500] tot_loss=1.572 (perp=7.298, rec=0.111, cos=0.002), tot_loss_proj:1.757 [t=0.26s]
prediction: ['[CLS] beautiful beautiful strange film [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.564 (perp=7.298, rec=0.103, cos=0.002), tot_loss_proj:1.754 [t=0.26s]
prediction: ['[CLS] beautiful beautiful strange film [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.565 (perp=7.298, rec=0.104, cos=0.002), tot_loss_proj:1.757 [t=0.24s]
prediction: ['[CLS] beautiful beautiful strange film [SEP]']
[ 450/ 500] tot_loss=1.556 (perp=7.298, rec=0.095, cos=0.001), tot_loss_proj:1.763 [t=0.25s]
prediction: ['[CLS] beautiful beautiful strange film [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.546 (perp=7.298, rec=0.085, cos=0.001), tot_loss_proj:1.760 [t=0.27s]
prediction: ['[CLS] beautiful beautiful strange film [SEP]']
Done with input #25 of 100.
reference: 
========================
[CLS] strange and beautiful film [SEP]
========================
predicted: 
========================
[CLS] beautiful beautiful strange film [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 83.333 | p: 83.333 | r: 83.333
rouge2     | fm: 20.000 | p: 20.000 | r: 20.000
rougeL     | fm: 66.667 | p: 66.667 | r: 66.667
rougeLsum  | fm: 66.667 | p: 66.667 | r: 66.667
r1fm+r2fm = 103.333

[Aggregate metrics]:
rouge1     | fm: 80.572 | p: 80.536 | r: 81.001
rouge2     | fm: 42.270 | p: 42.301 | r: 42.559
rougeL     | fm: 69.404 | p: 69.549 | r: 69.548
rougeLsum  | fm: 69.037 | p: 69.145 | r: 69.165
r1fm+r2fm = 122.842

input #25 time: 0:03:27 | total time: 1:30:53


Running input #26 of 100.
reference: 
========================
this ) meandering and pointless french coming-of-age import from writer-director anne-sophie birot 
========================
*********************************
*********************************
average of cosine similarity 0.9992061826546765
highest_index [0]
highest [0.9992061826546765]
Debug: ids_shape = 25, pads = [25]
Debug: input ids = tensor([[  101,  2023,  1007,  2812,  4063,  2075,  1998, 23100,  2413,  2746,
          1011,  1997,  1011,  2287, 12324,  2013,  3213,  1011,  2472,  4776,
          1011,  8234, 12170, 21709,   102]], device='cuda:0')
Debug: ref = ['[CLS] this ) meandering and pointless french coming - of - age import from writer - director anne - sophie birot [SEP]']
[Init] best rec loss: 1.9023979902267456 for ['[CLS] pointed minister laid neither loan happening dirty lad unitedod hatemourwi is conflictadia either angels compliment shield advantage looksrana [SEP]']
[Init] best rec loss: 1.8645083904266357 for ['[CLS] warfare isle due could marriedgocroft legislative cream can allie were must lion lawsuits eireann amateur highland kings therefore lil model roughly [SEP]']
[Init] best rec loss: 1.7954801321029663 for ['[CLS] cards media decision batsman healthy always year garrettoid templeawa prime clearing agencynin radio return emission puerto motion worldsd breath [SEP]']
[Init] best rec loss: 1.7904412746429443 for ['[CLS] votes jane normanwaite heaving trouble peopletou tax were sud launch onesmin rear lovely guitarists distressed gain software seemedtort industry [SEP]']
[Init] best rec loss: 1.65139901638031 for ['[CLS] exchange specify blame hurlinghyllum r monarch bet prom scouting presented jamal residents 2018 macdonald glow officials manual residing free shield pinkructured [SEP]']
[Init] best rec loss: 1.6358462572097778 for ['[CLS]sed benedict later housing why surroundingpinegrave nat use amount cast thy scored pattern run unknown authority travellingflict quotes guest lucan [SEP]']
[Init] best rec loss: 1.62001371383667 for ['[CLS] inter disappointed fiveiel 3 the airline whisperingar kelsey score chi kept dvduting cubs really casedrop4 commons due hayes [SEP]']
[Init] best rec loss: 1.5167450904846191 for ['[CLS] scene nearby protected miriam pvia 1 studio all emphasizes liner debut nic furtherych think kick charlie ling shoes thatization joe [SEP]']
[Init] best rec loss: 1.514949917793274 for ['[CLS] stakekar passing rides 65 turns speak mutant montrose capacity rid fur unite button riceª ak occupied cher following fully igo [SEP]']
[Init] best rec loss: 1.5008208751678467 for ['[CLS] colonial merely four door usuallytead souls arrow constituenciesᆼ officers more s discipline theoretical iso octave shot fourth list polishkan death [SEP]']
[Init] best perm rec loss: 1.4985138177871704 for ['[CLS] door colonial shot isotead discipline merelyᆼkan list more arrow four souls polish death fourth theoretical usually constituencies octave s officers [SEP]']
[Init] best perm rec loss: 1.4953454732894897 for ['[CLS] theoretical list polishᆼ officers colonial s disciplinetead octave usually merely four souls arrowkan death constituencies shot iso more fourth door [SEP]']
[Init] best perm rec loss: 1.4886966943740845 for ['[CLS] colonial s four soulskan octavetead death moreᆼ list polish shot door arrow officers usually theoretical fourth discipline constituencies iso merely [SEP]']
[Init] best perm rec loss: 1.485183835029602 for ['[CLS] shot polish constituencies theoretical arrow merelyᆼ usually souls officerskan octave list door colonial fourth iso s death discipline four moretead [SEP]']
[Init] best perm rec loss: 1.4836591482162476 for ['[CLS] polish list merely fourkan octaveᆼ souls usually constituencies iso officers arrow s discipline shot colonial moretead death fourth door theoretical [SEP]']
[Init] best perm rec loss: 1.4816036224365234 for ['[CLS] s merely colonial doorkan fourth polish souls four officers death usually arrow shot moretead list octave iso constituencies discipline theoreticalᆼ [SEP]']
[Init] best perm rec loss: 1.47699773311615 for ['[CLS] fourᆼ polishtead shot usually octave discipline souls s death door merely officers list arrow more fourth iso constituencieskan colonial theoretical [SEP]']
[Init] best perm rec loss: 1.4744471311569214 for ['[CLS] iso death arrow s list polish usually four door more merelykan discipline colonial octave constituenciestead shot fourth officersᆼ souls theoretical [SEP]']
[Init] best perm rec loss: 1.4727554321289062 for ['[CLS] list shotᆼ death door iso constituencies fourth s officers four merely octave moreteadkan colonial arrow usually discipline polish souls theoretical [SEP]']
[Init] best perm rec loss: 1.4687553644180298 for ['[CLS] discipline constituencies shotkan merely arrowᆼ four polishtead iso s list officers death usually door octave fourth more souls colonial theoretical [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.906 (perp=12.836, rec=0.336, cos=0.002), tot_loss_proj:3.179 [t=0.25s]
prediction: ['[CLS] biased death italianing youtube under less franco drivers crap 2012 pointless issued check leaving empty sick inappropriatebi / money ranked dull [SEP]']
Attempt swap
Put prefix at the end
[ 100/ 500] tot_loss=2.943 (perp=13.332, rec=0.275, cos=0.001), tot_loss_proj:3.353 [t=0.28s]
prediction: ['[CLS] check coming empty leasterbi inability car category low sudden british italianing pointless ipod sorts franco drivers crap import pointless issued [SEP]']
[ 150/ 500] tot_loss=2.542 (perp=11.535, rec=0.233, cos=0.002), tot_loss_proj:2.991 [t=0.25s]
prediction: ['[CLS] check, french ]metrybi sophie french - low mean british italianing pointless ipod writer french driver - import pointless writer [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.411 (perp=11.021, rec=0.206, cos=0.001), tot_loss_proj:2.877 [t=0.26s]
prediction: ['[CLS] mean ) french ) fran so sophie french - low mean death importing pointless attempt writer french driver - import pointless age [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=2.160 (perp=9.872, rec=0.185, cos=0.001), tot_loss_proj:2.599 [t=0.26s]
prediction: ['[CLS] mean age french ) catherine this sophie french - low mean age importder pointlesswoman writer french writer - import pointless, [SEP]']
[ 300/ 500] tot_loss=2.090 (perp=9.611, rec=0.167, cos=0.000), tot_loss_proj:2.555 [t=0.26s]
prediction: ['[CLS] type age french ) catherine and sophie french - low mean age importder pointless import writer french writer - import pointless, [SEP]']
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=1.972 (perp=9.007, rec=0.169, cos=0.001), tot_loss_proj:2.470 [t=0.26s]
prediction: ['[CLS] this sophie french ) age and sophie french - low mean age importder in import writer french writer - import pointless this [SEP]']
Attempt swap
Moved token
[ 400/ 500] tot_loss=1.935 (perp=8.951, rec=0.144, cos=0.001), tot_loss_proj:2.444 [t=0.29s]
prediction: ['[CLS] this sophie french age ) and sophie french - low mean age importder in import writer french writer - import pointless this [SEP]']
[ 450/ 500] tot_loss=1.964 (perp=9.196, rec=0.125, cos=0.001), tot_loss_proj:2.561 [t=0.25s]
prediction: ['[CLS] this sophie french age ) and sophie - - for mean age importder in import from french writer - coming pointless this [SEP]']
Attempt swap
Swapped tokens
[ 500/ 500] tot_loss=1.959 (perp=9.245, rec=0.109, cos=0.000), tot_loss_proj:2.523 [t=0.27s]
prediction: ['[CLS] this sophie french age ) and sophie - - of mean age importder - this from franco writer - coming pointless import [SEP]']
Done with input #26 of 100.
reference: 
========================
[CLS] this ) meandering and pointless french coming - of - age import from writer - director anne - sophie birot [SEP]
========================
predicted: 
========================
[CLS] this sophie french age ) and sophie - - of mean age importder - this from franco writer - coming pointless import [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 72.222 | p: 68.421 | r: 76.471
rouge2     | fm: 5.882 | p: 5.556 | r: 6.250
rougeL     | fm: 44.444 | p: 42.105 | r: 47.059
rougeLsum  | fm: 44.444 | p: 42.105 | r: 47.059
r1fm+r2fm = 78.105

[Aggregate metrics]:
rouge1     | fm: 80.404 | p: 80.186 | r: 80.859
rouge2     | fm: 41.216 | p: 41.092 | r: 41.547
rougeL     | fm: 68.658 | p: 68.652 | r: 68.862
rougeLsum  | fm: 68.129 | p: 68.118 | r: 68.460
r1fm+r2fm = 121.620

input #26 time: 0:03:29 | total time: 1:34:22


Running input #27 of 100.
reference: 
========================
are so generic 
========================
*********************************
*********************************
average of cosine similarity 0.9993452360030666
highest_index [0]
highest [0.9993452360030666]
Debug: ids_shape = 5, pads = [5]
Debug: input ids = tensor([[  101,  2024,  2061, 12391,   102]], device='cuda:0')
Debug: ref = ['[CLS] are so generic [SEP]']
[Init] best rec loss: 1.9401098489761353 for ['[CLS] sidney ft roman [SEP]']
[Init] best rec loss: 1.9116452932357788 for ['[CLS] dun where occupied [SEP]']
[Init] best rec loss: 1.8886722326278687 for ['[CLS] estate kiss ale [SEP]']
[Init] best rec loss: 1.8630187511444092 for ['[CLS] heel clinical birth [SEP]']
[Init] best rec loss: 1.6387312412261963 for ['[CLS] and universal universe [SEP]']
[Init] best rec loss: 1.5562210083007812 for ['[CLS] [CLS] evidence darkness [SEP]']
[Init] best rec loss: 1.4637377262115479 for ['[CLS] part portion mid [SEP]']
[Init] best rec loss: 1.3929609060287476 for ['[CLS] fat mattream [SEP]']
[Init] best rec loss: 1.2099963426589966 for ['[CLS] givenwine transit [SEP]']
[Init] best perm rec loss: 1.209096074104309 for ['[CLS] transitwine given [SEP]']
[Init] best perm rec loss: 1.2082395553588867 for ['[CLS] transit givenwine [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=3.567 (perp=13.685, rec=0.812, cos=0.018), tot_loss_proj:4.559 [t=0.26s]
prediction: ['[CLS] katrina provided shakes [SEP]']
Attempt swap
Moved token
[ 100/ 500] tot_loss=3.283 (perp=12.671, rec=0.723, cos=0.026), tot_loss_proj:4.284 [t=0.26s]
prediction: ['[CLS] lawn anyway package [SEP]']
[ 150/ 500] tot_loss=3.426 (perp=13.912, rec=0.640, cos=0.004), tot_loss_proj:4.295 [t=0.26s]
prediction: ['[CLS] generic survivesable [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.984 (perp=11.509, rec=0.661, cos=0.021), tot_loss_proj:3.710 [t=0.26s]
prediction: ['[CLS]able survives generic [SEP]']
Attempt swap
[ 250/ 500] tot_loss=3.220 (perp=13.046, rec=0.602, cos=0.009), tot_loss_proj:3.291 [t=0.26s]
prediction: ['[CLS]ableselle generic [SEP]']
[ 300/ 500] tot_loss=2.480 (perp=9.509, rec=0.574, cos=0.004), tot_loss_proj:2.327 [t=0.26s]
prediction: ['[CLS] are generic generic [SEP]']
Attempt swap
[ 350/ 500] tot_loss=3.063 (perp=11.698, rec=0.663, cos=0.060), tot_loss_proj:2.877 [t=0.26s]
prediction: ['[CLS] areurity generic [SEP]']
Attempt swap
[ 400/ 500] tot_loss=2.486 (perp=9.509, rec=0.583, cos=0.001), tot_loss_proj:2.336 [t=0.26s]
prediction: ['[CLS] are generic generic [SEP]']
[ 450/ 500] tot_loss=2.490 (perp=9.509, rec=0.574, cos=0.015), tot_loss_proj:2.331 [t=0.26s]
prediction: ['[CLS] are generic generic [SEP]']
Attempt swap
[ 500/ 500] tot_loss=2.963 (perp=11.698, rec=0.593, cos=0.030), tot_loss_proj:2.881 [t=0.28s]
prediction: ['[CLS] areurity generic [SEP]']
Done with input #27 of 100.
reference: 
========================
[CLS] are so generic [SEP]
========================
predicted: 
========================
[CLS] are generic generic [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 80.000 | p: 80.000 | r: 80.000
rouge2     | fm: 50.000 | p: 50.000 | r: 50.000
rougeL     | fm: 80.000 | p: 80.000 | r: 80.000
rougeLsum  | fm: 80.000 | p: 80.000 | r: 80.000
r1fm+r2fm = 130.000

[Aggregate metrics]:
rouge1     | fm: 80.345 | p: 80.287 | r: 80.852
rouge2     | fm: 40.789 | p: 40.585 | r: 41.151
rougeL     | fm: 68.920 | p: 69.014 | r: 69.158
rougeLsum  | fm: 68.539 | p: 68.568 | r: 68.852
r1fm+r2fm = 121.134

input #27 time: 0:03:27 | total time: 1:37:50


Running input #28 of 100.
reference: 
========================
for only 71 minutes 
========================
*********************************
*********************************
average of cosine similarity 0.9993345319062628
highest_index [0]
highest [0.9993345319062628]
Debug: ids_shape = 6, pads = [6]
Debug: input ids = tensor([[ 101, 2005, 2069, 6390, 2781,  102]], device='cuda:0')
Debug: ref = ['[CLS] for only 71 minutes [SEP]']
[Init] best rec loss: 1.6155987977981567 for ['[CLS] wheel pc liz tube [SEP]']
[Init] best rec loss: 1.5106481313705444 for ['[CLS] boundaries towards lands delivery [SEP]']
[Init] best rec loss: 1.505471110343933 for ['[CLS] immortal coma thierry imperial [SEP]']
[Init] best rec loss: 1.4851624965667725 for ['[CLS] perhaps childrenogical beta [SEP]']
[Init] best rec loss: 1.4608865976333618 for ['[CLS] bro asher lit majority [SEP]']
[Init] best rec loss: 1.460343837738037 for ['[CLS] site georgia chambers nicholas [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=3.269 (perp=14.479, rec=0.362, cos=0.011), tot_loss_proj:4.315 [t=0.29s]
prediction: ['[CLS] itunes start ¹⁄₂ longest [SEP]']
Attempt swap
Moved sequence
[ 100/ 500] tot_loss=2.417 (perp=10.795, rec=0.254, cos=0.004), tot_loss_proj:3.202 [t=0.29s]
prediction: ['[CLS] longest least minute minutes [SEP]']
[ 150/ 500] tot_loss=1.653 (perp=7.699, rec=0.112, cos=0.002), tot_loss_proj:1.644 [t=0.30s]
prediction: ['[CLS] for only 71 minutes [SEP]']
Attempt swap
[ 200/ 500] tot_loss=1.617 (perp=7.699, rec=0.077, cos=0.001), tot_loss_proj:1.630 [t=0.29s]
prediction: ['[CLS] for only 71 minutes [SEP]']
Attempt swap
[ 250/ 500] tot_loss=1.618 (perp=7.699, rec=0.077, cos=0.001), tot_loss_proj:1.627 [t=0.31s]
prediction: ['[CLS] for only 71 minutes [SEP]']
[ 300/ 500] tot_loss=1.602 (perp=7.699, rec=0.062, cos=0.000), tot_loss_proj:1.637 [t=0.28s]
prediction: ['[CLS] for only 71 minutes [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.616 (perp=7.699, rec=0.076, cos=0.000), tot_loss_proj:1.632 [t=0.29s]
prediction: ['[CLS] for only 71 minutes [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.602 (perp=7.699, rec=0.062, cos=0.000), tot_loss_proj:1.629 [t=0.30s]
prediction: ['[CLS] for only 71 minutes [SEP]']
[ 450/ 500] tot_loss=1.591 (perp=7.699, rec=0.051, cos=0.000), tot_loss_proj:1.625 [t=0.30s]
prediction: ['[CLS] for only 71 minutes [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.588 (perp=7.699, rec=0.048, cos=0.000), tot_loss_proj:1.626 [t=0.29s]
prediction: ['[CLS] for only 71 minutes [SEP]']
Done with input #28 of 100.
reference: 
========================
[CLS] for only 71 minutes [SEP]
========================
predicted: 
========================
[CLS] for only 71 minutes [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 81.071 | p: 80.821 | r: 81.600
rouge2     | fm: 43.455 | p: 43.283 | r: 43.693
rougeL     | fm: 69.932 | p: 69.973 | r: 70.192
rougeLsum  | fm: 69.590 | p: 69.427 | r: 69.820
r1fm+r2fm = 124.526

input #28 time: 0:03:54 | total time: 1:41:45


Running input #29 of 100.
reference: 
========================
i also believe that resident evil is not it . 
========================
*********************************
*********************************
average of cosine similarity 0.9992943703711437
highest_index [0]
highest [0.9992943703711437]
Debug: ids_shape = 12, pads = [12]
Debug: input ids = tensor([[ 101, 1045, 2036, 2903, 2008, 6319, 4763, 2003, 2025, 2009, 1012,  102]],
       device='cuda:0')
Debug: ref = ['[CLS] i also believe that resident evil is not it. [SEP]']
[Init] best rec loss: 1.9422098398208618 for ['[CLS] enthusiast strings jose occupied natasha respectivelyst times tale lane [SEP]']
[Init] best rec loss: 1.725492238998413 for ['[CLS] fine bedside please blanco fight colonel so ■stick manitoba [SEP]']
[Init] best rec loss: 1.665594458580017 for ['[CLS] label which £ anyway shoes mediamont campbell her cullen [SEP]']
[Init] best rec loss: 1.6626322269439697 for ['[CLS] retrieved kicking quite misunderstanding race camp streaked shot larger fields [SEP]']
[Init] best rec loss: 1.544628381729126 for ['[CLS] upperœggio award metresnay centrally managing un suddenly [SEP]']
[Init] best rec loss: 1.4950032234191895 for ['[CLS] passes training too alongside flopst tel twicerangle resident [SEP]']
[Init] best rec loss: 1.4667460918426514 for ['[CLS] hectares overshadowed° angeles me festival panels dean eventually towards [SEP]']
[Init] best rec loss: 1.451490879058838 for ['[CLS] chart numbers jar union touch of terms extreme 0 mean [SEP]']
[Init] best rec loss: 1.3281519412994385 for ['[CLS] consuming after intern coach acres surf class speed tongues period [SEP]']
[Init] best rec loss: 1.2676228284835815 for ['[CLS] crap extra cape epic apartbeat fork historia fk joyah [SEP]']
[Init] best rec loss: 1.2215781211853027 for ['[CLS] f transmit mostly axle veto cells u bufounded meters [SEP]']
[Init] best rec loss: 1.2187750339508057 for ['[CLS] tv envelope engagement administration landed tasteuted this runs oil [SEP]']
[Init] best perm rec loss: 1.2168614864349365 for ['[CLS] tv oil envelope administration taste this engagementuted runs landed [SEP]']
[Init] best perm rec loss: 1.2131327390670776 for ['[CLS] taste landed administration this runs tv engagementuted oil envelope [SEP]']
[Init] best perm rec loss: 1.2063658237457275 for ['[CLS] oil this taste landed tv runs envelope engagement administrationuted [SEP]']
[Init] best perm rec loss: 1.2030150890350342 for ['[CLS] tv oil engagement landeduted this taste envelope runs administration [SEP]']
[Init] best perm rec loss: 1.200951337814331 for ['[CLS] taste runs landeduted engagement envelope this oil tv administration [SEP]']
[Init] best perm rec loss: 1.2006797790527344 for ['[CLS] landeduted taste engagement runs tv envelope this oil administration [SEP]']
[Init] best perm rec loss: 1.2004748582839966 for ['[CLS] this oil landed engagement tv envelopeuted taste runs administration [SEP]']
[Init] best perm rec loss: 1.1998974084854126 for ['[CLS]uted taste tv engagement oil runs envelope landed this administration [SEP]']
[Init] best perm rec loss: 1.197476863861084 for ['[CLS] taste runsuted tv landed engagement oil envelope this administration [SEP]']
[Init] best perm rec loss: 1.1972182989120483 for ['[CLS] taste oiluted engagement landed tv envelope administration this runs [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=3.048 (perp=13.122, rec=0.416, cos=0.007), tot_loss_proj:3.754 [t=0.25s]
prediction: ['[CLS] stupid disputesuted opposed allegedly workers label denied anything cult [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.639 (perp=11.545, rec=0.328, cos=0.003), tot_loss_proj:3.588 [t=0.27s]
prediction: ['[CLS] bounty britain actually opposed causing workers. denied it monster [SEP]']
[ 150/ 500] tot_loss=1.866 (perp=8.109, rec=0.243, cos=0.002), tot_loss_proj:2.588 [t=0.26s]
prediction: ['[CLS] evilceptive actually believes saudi owner is not it. [SEP]']
Attempt swap
[ 200/ 500] tot_loss=1.985 (perp=8.927, rec=0.198, cos=0.001), tot_loss_proj:2.778 [t=0.26s]
prediction: ['[CLS] eviloxide actually believe saudi resident is not it. [SEP]']
Attempt swap
[ 250/ 500] tot_loss=1.798 (perp=8.082, rec=0.181, cos=0.001), tot_loss_proj:2.530 [t=0.25s]
prediction: ['[CLS] evil exists also believe saudi resident is not it. [SEP]']
[ 300/ 500] tot_loss=1.711 (perp=7.777, rec=0.155, cos=0.001), tot_loss_proj:2.537 [t=0.24s]
prediction: ['[CLS] resident also also believe saudi resident is not it. [SEP]']
Attempt swap
Moved token
[ 350/ 500] tot_loss=1.775 (perp=8.038, rec=0.167, cos=0.001), tot_loss_proj:2.894 [t=0.26s]
prediction: ['[CLS] actually resident also believe saudi resident is not it. [SEP]']
Attempt swap
Swapped tokens
[ 400/ 500] tot_loss=1.696 (perp=7.629, rec=0.168, cos=0.002), tot_loss_proj:2.880 [t=0.26s]
prediction: ['[CLS] also resident also saudi believe resident is not it. [SEP]']
[ 450/ 500] tot_loss=1.799 (perp=8.241, rec=0.150, cos=0.000), tot_loss_proj:2.703 [t=0.25s]
prediction: ['[CLS] also resident also medication believe resident is not it. [SEP]']
Attempt swap
Swapped tokens
[ 500/ 500] tot_loss=1.711 (perp=7.872, rec=0.136, cos=0.001), tot_loss_proj:2.613 [t=0.25s]
prediction: ['[CLS] also resident also resident believeflict is not it. [SEP]']
Done with input #29 of 100.
reference: 
========================
[CLS] i also believe that resident evil is not it. [SEP]
========================
predicted: 
========================
[CLS] also resident also resident believeflict is not it. [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 66.667 | p: 70.000 | r: 63.636
rouge2     | fm: 31.579 | p: 33.333 | r: 30.000
rougeL     | fm: 66.667 | p: 70.000 | r: 63.636
rougeLsum  | fm: 66.667 | p: 70.000 | r: 63.636
r1fm+r2fm = 98.246

[Aggregate metrics]:
rouge1     | fm: 80.520 | p: 80.512 | r: 80.975
rouge2     | fm: 43.047 | p: 42.883 | r: 43.240
rougeL     | fm: 69.918 | p: 70.064 | r: 69.879
rougeLsum  | fm: 69.496 | p: 69.612 | r: 69.624
r1fm+r2fm = 123.566

input #29 time: 0:03:26 | total time: 1:45:11


Running input #30 of 100.
reference: 
========================
fizzability 
========================
*********************************
*********************************
average of cosine similarity 0.9992720994590749
highest_index [0]
highest [0.9992720994590749]
Debug: ids_shape = 5, pads = [5]
Debug: input ids = tensor([[  101, 10882, 20715,  8553,   102]], device='cuda:0')
Debug: ref = ['[CLS] fizzability [SEP]']
[Init] best rec loss: 1.8995320796966553 for ['[CLS]aver tone darling [SEP]']
[Init] best rec loss: 1.870033860206604 for ['[CLS] count four shone [SEP]']
[Init] best rec loss: 1.8638838529586792 for ['[CLS] superior vary lynne [SEP]']
[Init] best rec loss: 1.4753696918487549 for ['[CLS]kon everyone jennings [SEP]']
[Init] best rec loss: 1.3648078441619873 for ['[CLS] shell albeittai [SEP]']
[Init] best rec loss: 1.221232533454895 for ['[CLS] middle lighthouse case [SEP]']
[Init] best rec loss: 1.1736953258514404 for ['[CLS] acceleration council lizard [SEP]']
[Init] best rec loss: 1.155712604522705 for ['[CLS] spent who mom [SEP]']
[Init] best perm rec loss: 1.1542490720748901 for ['[CLS] spent mom who [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.621 (perp=11.471, rec=0.320, cos=0.007), tot_loss_proj:3.103 [t=0.27s]
prediction: ['[CLS] badzzinessbility [SEP]']
Attempt swap
[ 100/ 500] tot_loss=2.364 (perp=10.667, rec=0.227, cos=0.004), tot_loss_proj:3.923 [t=0.25s]
prediction: ['[CLS] summerbilitybility [SEP]']
[ 150/ 500] tot_loss=2.072 (perp=9.540, rec=0.161, cos=0.003), tot_loss_proj:1.993 [t=0.25s]
prediction: ['[CLS] fizzability [SEP]']
Attempt swap
[ 200/ 500] tot_loss=2.015 (perp=9.540, rec=0.106, cos=0.002), tot_loss_proj:1.990 [t=0.26s]
prediction: ['[CLS] fizzability [SEP]']
Attempt swap
[ 250/ 500] tot_loss=2.005 (perp=9.540, rec=0.095, cos=0.003), tot_loss_proj:1.980 [t=0.26s]
prediction: ['[CLS] fizzability [SEP]']
[ 300/ 500] tot_loss=1.994 (perp=9.540, rec=0.085, cos=0.002), tot_loss_proj:1.982 [t=0.25s]
prediction: ['[CLS] fizzability [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.981 (perp=9.540, rec=0.072, cos=0.001), tot_loss_proj:1.992 [t=0.25s]
prediction: ['[CLS] fizzability [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.978 (perp=9.540, rec=0.069, cos=0.001), tot_loss_proj:1.994 [t=0.26s]
prediction: ['[CLS] fizzability [SEP]']
[ 450/ 500] tot_loss=1.971 (perp=9.540, rec=0.061, cos=0.001), tot_loss_proj:1.982 [t=0.25s]
prediction: ['[CLS] fizzability [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.981 (perp=9.540, rec=0.072, cos=0.001), tot_loss_proj:1.992 [t=0.25s]
prediction: ['[CLS] fizzability [SEP]']
Done with input #30 of 100.
reference: 
========================
[CLS] fizzability [SEP]
========================
predicted: 
========================
[CLS] fizzability [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 81.150 | p: 81.118 | r: 81.435
rouge2     | fm: 44.708 | p: 44.583 | r: 44.865
rougeL     | fm: 70.915 | p: 71.033 | r: 71.002
rougeLsum  | fm: 70.542 | p: 70.562 | r: 70.800
r1fm+r2fm = 125.858

input #30 time: 0:03:27 | total time: 1:48:38


Running input #31 of 100.
reference: 
========================
a better vehicle 
========================
*********************************
*********************************
average of cosine similarity 0.999339325216174
highest_index [0]
highest [0.999339325216174]
Debug: ids_shape = 5, pads = [5]
Debug: input ids = tensor([[ 101, 1037, 2488, 4316,  102]], device='cuda:0')
Debug: ref = ['[CLS] a better vehicle [SEP]']
[Init] best rec loss: 1.914638876914978 for ['[CLS] stock dorsal generations [SEP]']
[Init] best rec loss: 1.7327791452407837 for ['[CLS] afctlement generation [SEP]']
[Init] best rec loss: 1.6876567602157593 for ['[CLS] che episode band [SEP]']
[Init] best rec loss: 1.3702372312545776 for ['[CLS] running artwork robin [SEP]']
[Init] best perm rec loss: 1.3683922290802002 for ['[CLS] robin artwork running [SEP]']
[Init] best perm rec loss: 1.3629165887832642 for ['[CLS] artwork robin running [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.423 (perp=10.371, rec=0.336, cos=0.012), tot_loss_proj:3.209 [t=0.28s]
prediction: ['[CLS] better creditneuve [SEP]']
Attempt swap
[ 100/ 500] tot_loss=2.132 (perp=9.658, rec=0.196, cos=0.005), tot_loss_proj:2.401 [t=0.25s]
prediction: ['[CLS] better vehicle vehicle [SEP]']
[ 150/ 500] tot_loss=1.854 (perp=8.742, rec=0.104, cos=0.002), tot_loss_proj:3.285 [t=0.25s]
prediction: ['[CLS] better a vehicle [SEP]']
Attempt swap
Moved token
[ 200/ 500] tot_loss=1.589 (perp=7.603, rec=0.067, cos=0.001), tot_loss_proj:1.682 [t=0.25s]
prediction: ['[CLS] a better vehicle [SEP]']
Attempt swap
[ 250/ 500] tot_loss=1.593 (perp=7.603, rec=0.072, cos=0.001), tot_loss_proj:1.675 [t=0.25s]
prediction: ['[CLS] a better vehicle [SEP]']
[ 300/ 500] tot_loss=1.581 (perp=7.603, rec=0.060, cos=0.001), tot_loss_proj:1.685 [t=0.26s]
prediction: ['[CLS] a better vehicle [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.587 (perp=7.603, rec=0.065, cos=0.002), tot_loss_proj:1.689 [t=0.26s]
prediction: ['[CLS] a better vehicle [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.586 (perp=7.603, rec=0.065, cos=0.001), tot_loss_proj:1.684 [t=0.27s]
prediction: ['[CLS] a better vehicle [SEP]']
[ 450/ 500] tot_loss=1.583 (perp=7.603, rec=0.062, cos=0.001), tot_loss_proj:1.684 [t=0.25s]
prediction: ['[CLS] a better vehicle [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.577 (perp=7.603, rec=0.056, cos=0.001), tot_loss_proj:1.674 [t=0.26s]
prediction: ['[CLS] a better vehicle [SEP]']
Done with input #31 of 100.
reference: 
========================
[CLS] a better vehicle [SEP]
========================
predicted: 
========================
[CLS] a better vehicle [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 81.845 | p: 81.742 | r: 82.200
rouge2     | fm: 46.626 | p: 46.472 | r: 46.891
rougeL     | fm: 71.819 | p: 71.975 | r: 71.950
rougeLsum  | fm: 71.708 | p: 71.822 | r: 71.800
r1fm+r2fm = 128.471

input #31 time: 0:03:25 | total time: 1:52:04


Running input #32 of 100.
reference: 
========================
pull together easily accessible stories that resonate with profundity 
========================
*********************************
*********************************
average of cosine similarity 0.9992521630934186
highest_index [0]
highest [0.9992521630934186]
Debug: ids_shape = 14, pads = [14]
Debug: input ids = tensor([[  101,  4139,  2362,  4089,  7801,  3441,  2008, 24501, 21149,  2007,
         11268,  8630,  3012,   102]], device='cuda:0')
Debug: ref = ['[CLS] pull together easily accessible stories that resonate with profundity [SEP]']
[Init] best rec loss: 2.020369052886963 for ['[CLS] united serena cedarrooms mat especially lumpur riddle dye brien orthodox they [SEP]']
[Init] best rec loss: 1.688668131828308 for ['[CLS] gut chicago otherwise dharma import miracles hindu partnerships permitted gayogo poly [SEP]']
[Init] best rec loss: 1.6886156797409058 for ['[CLS] particular usual lana rid part awaitfication felt worked bolt algorithm tristan [SEP]']
[Init] best rec loss: 1.6725391149520874 for ['[CLS] carl formulacl overall network would ± < meyricktia archangel level [SEP]']
[Init] best rec loss: 1.655273675918579 for ['[CLS] unharmed spit health should llc relative front die threat skateer demolished [SEP]']
[Init] best perm rec loss: 1.6545727252960205 for ['[CLS] demolished spit health should die llc relative skate unharmed threater front [SEP]']
[Init] best perm rec loss: 1.6517530679702759 for ['[CLS] spit should relative threat demolished skate unharmed fronter llc die health [SEP]']
[Init] best perm rec loss: 1.6510119438171387 for ['[CLS] demolished unharmed should health spit relative llc die front threater skate [SEP]']
[Init] best perm rec loss: 1.6505436897277832 for ['[CLS] skateer health unharmed front die demolished should relative threat spit llc [SEP]']
[Init] best perm rec loss: 1.6478852033615112 for ['[CLS] unharmed demolished spit llcer skate die health should threat relative front [SEP]']
[Init] best perm rec loss: 1.646742582321167 for ['[CLS]er llc relative health die demolished threat unharmed spit should skate front [SEP]']
[Init] best perm rec loss: 1.6462005376815796 for ['[CLS] dieer health should llc skate threat spit unharmed demolished relative front [SEP]']
[Init] best perm rec loss: 1.6457756757736206 for ['[CLS] front llc skate should demolished healther threat spit unharmed die relative [SEP]']
[Init] best perm rec loss: 1.6436963081359863 for ['[CLS] unharmed die llc skate demolished healther front should relative threat spit [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=3.346 (perp=13.011, rec=0.732, cos=0.011), tot_loss_proj:4.118 [t=0.31s]
prediction: ['[CLS] gravel days less noο taste dullided privacy inventedquent climate [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=3.403 (perp=13.634, rec=0.671, cos=0.005), tot_loss_proj:4.230 [t=0.29s]
prediction: ['[CLS] gravel miserable thick disappear fat taste death lifetime privacyundatable climate [SEP]']
[ 150/ 500] tot_loss=3.346 (perp=13.799, rec=0.584, cos=0.002), tot_loss_proj:4.418 [t=0.31s]
prediction: ['[CLS] becomes effectively same adhere horse vitro death ambrose temptingundatable climate [SEP]']
Attempt swap
Moved token
[ 200/ 500] tot_loss=3.380 (perp=14.111, rec=0.556, cos=0.002), tot_loss_proj:4.370 [t=0.29s]
prediction: ['[CLS] becomes effectivelyructured adherefect horseguard death temptingundhope privacy [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=3.219 (perp=13.371, rec=0.543, cos=0.002), tot_loss_proj:4.487 [t=0.30s]
prediction: ['[CLS] becomes effectivelyructured disappear sense volume offering death susannahundgizing privacy [SEP]']
[ 300/ 500] tot_loss=3.187 (perp=13.336, rec=0.519, cos=0.001), tot_loss_proj:4.330 [t=0.29s]
prediction: ['[CLS] becomes effectivelyructured disappearonate resity deathhopeundturing privacy [SEP]']
Attempt swap
Moved token
[ 350/ 500] tot_loss=3.489 (perp=14.693, rec=0.529, cos=0.021), tot_loss_proj:4.697 [t=0.31s]
prediction: ['[CLS] kilometers effectivelyructured packet resonateund limit susannahundgizing imprisonment [SEP]']
Attempt swap
Swapped tokens
[ 400/ 500] tot_loss=3.243 (perp=13.630, rec=0.514, cos=0.002), tot_loss_proj:4.734 [t=0.31s]
prediction: ['[CLS] kilometershopeructured packet resonateund use effectivelyundgizing privacy [SEP]']
[ 450/ 500] tot_loss=3.203 (perp=13.393, rec=0.510, cos=0.014), tot_loss_proj:4.399 [t=0.30s]
prediction: ['[CLS] kilometers obtainructured packet resonateund use effectivelyund kaladin imprisonment [SEP]']
Attempt swap
Moved sequence
[ 500/ 500] tot_loss=3.283 (perp=13.848, rec=0.502, cos=0.011), tot_loss_proj:4.511 [t=0.30s]
prediction: ['[CLS] obtainructured kilometers packet resonateund use comfortableund kaladin imprisonment [SEP]']
Done with input #32 of 100.
reference: 
========================
[CLS] pull together easily accessible stories that resonate with profundity [SEP]
========================
predicted: 
========================
[CLS] obtainructured kilometers packet resonateund use comfortableund kaladin privacy [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 19.048 | p: 20.000 | r: 18.182
rouge2     | fm: 0.000 | p: 0.000 | r: 0.000
rougeL     | fm: 19.048 | p: 20.000 | r: 18.182
rougeLsum  | fm: 19.048 | p: 20.000 | r: 18.182
r1fm+r2fm = 19.048

[Aggregate metrics]:
rouge1     | fm: 79.815 | p: 79.773 | r: 80.110
rouge2     | fm: 44.981 | p: 44.898 | r: 45.208
rougeL     | fm: 70.303 | p: 70.388 | r: 70.481
rougeLsum  | fm: 70.048 | p: 70.166 | r: 70.305
r1fm+r2fm = 124.797

input #32 time: 0:03:55 | total time: 1:55:59


Running input #33 of 100.
reference: 
========================
higher 
========================
*********************************
*********************************
average of cosine similarity 0.9992763851579931
highest_index [0]
highest [0.9992763851579931]
Debug: ids_shape = 3, pads = [3]
Debug: input ids = tensor([[ 101, 3020,  102]], device='cuda:0')
Debug: ref = ['[CLS] higher [SEP]']
[Init] best rec loss: 1.8901952505111694 for ['[CLS] riots [SEP]']
[Init] best rec loss: 1.7840126752853394 for ['[CLS] lord [SEP]']
[Init] best rec loss: 1.6915725469589233 for ['[CLS] master [SEP]']
[Init] best rec loss: 1.5511021614074707 for ['[CLS] training [SEP]']
[Init] best rec loss: 1.4511288404464722 for ['[CLS] strip [SEP]']
[Init] best rec loss: 1.3865865468978882 for ['[CLS] less [SEP]']
[Init] best rec loss: 1.3548933267593384 for ['[CLS] higher [SEP]']
[Init] best rec loss: 1.121751308441162 for ['[CLS] positive [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.479 (perp=11.231, rec=0.222, cos=0.010), tot_loss_proj:2.956 [t=0.26s]
prediction: ['[CLS] higher [SEP]']
Attempt swap
[ 100/ 500] tot_loss=2.362 (perp=11.231, rec=0.114, cos=0.002), tot_loss_proj:2.522 [t=0.25s]
prediction: ['[CLS] higher [SEP]']
[ 150/ 500] tot_loss=2.319 (perp=11.231, rec=0.071, cos=0.001), tot_loss_proj:2.418 [t=0.25s]
prediction: ['[CLS] higher [SEP]']
Attempt swap
[ 200/ 500] tot_loss=2.338 (perp=11.231, rec=0.090, cos=0.001), tot_loss_proj:2.404 [t=0.25s]
prediction: ['[CLS] higher [SEP]']
Attempt swap
[ 250/ 500] tot_loss=2.327 (perp=11.231, rec=0.080, cos=0.001), tot_loss_proj:2.406 [t=0.25s]
prediction: ['[CLS] higher [SEP]']
[ 300/ 500] tot_loss=2.321 (perp=11.231, rec=0.074, cos=0.001), tot_loss_proj:2.397 [t=0.26s]
prediction: ['[CLS] higher [SEP]']
Attempt swap
[ 350/ 500] tot_loss=2.310 (perp=11.231, rec=0.062, cos=0.001), tot_loss_proj:2.401 [t=0.26s]
prediction: ['[CLS] higher [SEP]']
Attempt swap
[ 400/ 500] tot_loss=2.293 (perp=11.231, rec=0.046, cos=0.001), tot_loss_proj:2.393 [t=0.26s]
prediction: ['[CLS] higher [SEP]']
[ 450/ 500] tot_loss=2.320 (perp=11.231, rec=0.073, cos=0.001), tot_loss_proj:2.403 [t=0.26s]
prediction: ['[CLS] higher [SEP]']
Attempt swap
[ 500/ 500] tot_loss=2.313 (perp=11.231, rec=0.065, cos=0.001), tot_loss_proj:2.381 [t=0.27s]
prediction: ['[CLS] higher [SEP]']
Done with input #33 of 100.
reference: 
========================
[CLS] higher [SEP]
========================
predicted: 
========================
[CLS] higher [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 80.338 | p: 80.450 | r: 80.690
rouge2     | fm: 46.313 | p: 46.216 | r: 46.531
rougeL     | fm: 71.022 | p: 71.116 | r: 71.122
rougeLsum  | fm: 70.923 | p: 70.941 | r: 71.082
r1fm+r2fm = 126.651

input #33 time: 0:03:26 | total time: 1:59:25


Running input #34 of 100.
reference: 
========================
build in the mind of the viewer and take on extreme urgency . 
========================
*********************************
*********************************
average of cosine similarity 0.9992091879502123
highest_index [0]
highest [0.9992091879502123]
Debug: ids_shape = 15, pads = [15]
Debug: input ids = tensor([[  101,  3857,  1999,  1996,  2568,  1997,  1996, 13972,  1998,  2202,
          2006,  6034, 19353,  1012,   102]], device='cuda:0')
Debug: ref = ['[CLS] build in the mind of the viewer and take on extreme urgency. [SEP]']
[Init] best rec loss: 1.8906997442245483 for ['[CLS] poker counties toyston boyle packing softball reich,sack episodes andy mexico [SEP]']
[Init] best rec loss: 1.821115493774414 for ['[CLS] mistress quality security throughout trunkught warning age marketing experiments despite bug travel [SEP]']
[Init] best rec loss: 1.8150413036346436 for ['[CLS] bus japan coyote being far united away sal during cov : fair [SEP]']
[Init] best rec loss: 1.764905333518982 for ['[CLS] seriously sid representativeupt ang v missrize fran wildlife normally universe registration [SEP]']
[Init] best rec loss: 1.7503083944320679 for ['[CLS] competing rode until oxygenqua schools streets cha sole disguiser modernlore [SEP]']
[Init] best rec loss: 1.716280460357666 for ['[CLS] bird cursed led ao wearing one keys nearlysco tom constitutionnem љ [SEP]']
[Init] best rec loss: 1.469266414642334 for ['[CLS]ask founder statue okay whoibe worth along slight drivers ship field lissa [SEP]']
[Init] best perm rec loss: 1.4675160646438599 for ['[CLS] drivers slight okay field worth statue founder shipibeask who along lissa [SEP]']
[Init] best perm rec loss: 1.4614590406417847 for ['[CLS] field slight alongask whoibe statue worth lissa founder ship okay drivers [SEP]']
[Init] best perm rec loss: 1.4533991813659668 for ['[CLS] ship slight founder statue lissaask drivers okay worth fieldibe who along [SEP]']
[Init] best perm rec loss: 1.4512879848480225 for ['[CLS]ask along slight founder worth okay ship lissa field statue driversibe who [SEP]']
[Init] best perm rec loss: 1.4508488178253174 for ['[CLS]ibe along worth driversask slight statue field okay who lissa ship founder [SEP]']
[Init] best perm rec loss: 1.4482790231704712 for ['[CLS] alongibe drivers who ship statue okay lissaask worth slight founder field [SEP]']
[Init] best perm rec loss: 1.4482308626174927 for ['[CLS] lissa ship along drivers slight okayaskibe who statue founder field worth [SEP]']
[Init] best perm rec loss: 1.4454642534255981 for ['[CLS] along slight ship worth drivers founder okay whoask field lissa statueibe [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.850 (perp=12.277, rec=0.387, cos=0.008), tot_loss_proj:3.678 [t=0.26s]
prediction: ['[CLS] attraction slightly magnitude build desperate grasping gordon highnessneuve power current earnest and [SEP]']
Attempt swap
Moved sequence
[ 100/ 500] tot_loss=2.379 (perp=10.459, rec=0.282, cos=0.005), tot_loss_proj:3.310 [t=0.26s]
prediction: ['[CLS] take positive urgency and slightlyecure urgency extreme global. highness urgency power [SEP]']
[ 150/ 500] tot_loss=2.366 (perp=10.799, rec=0.204, cos=0.003), tot_loss_proj:3.971 [t=0.26s]
prediction: ['[CLS] take least urgency and viewer viewers urgency extremeber in region viewer. [SEP]']
Attempt swap
Moved sequence
[ 200/ 500] tot_loss=2.203 (perp=10.066, rec=0.186, cos=0.004), tot_loss_proj:3.755 [t=0.25s]
prediction: ['[CLS] take least urgency. urgency extreme ( in region viewer viewer viewer. [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=2.125 (perp=9.790, rec=0.163, cos=0.005), tot_loss_proj:3.748 [t=0.25s]
prediction: ['[CLS] take least region. urgency extreme ( in urgency viewer viewer viewer. [SEP]']
[ 300/ 500] tot_loss=2.080 (perp=9.658, rec=0.143, cos=0.005), tot_loss_proj:2.814 [t=0.25s]
prediction: ['[CLS] take mind of. urgency extreme in in urgency viewer viewer build. [SEP]']
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=1.766 (perp=8.138, rec=0.138, cos=0.001), tot_loss_proj:2.604 [t=0.26s]
prediction: ['[CLS] take mind of extreme urgency. and in urgency extreme viewer build. [SEP]']
Attempt swap
Swapped tokens
[ 400/ 500] tot_loss=1.688 (perp=7.813, rec=0.124, cos=0.001), tot_loss_proj:2.578 [t=0.26s]
prediction: ['[CLS] take mind of extreme urgency. and on extreme urgency viewer build. [SEP]']
[ 450/ 500] tot_loss=1.676 (perp=7.813, rec=0.112, cos=0.001), tot_loss_proj:2.573 [t=0.27s]
prediction: ['[CLS] take mind of extreme urgency. and on extreme urgency viewer build. [SEP]']
Attempt swap
Moved token
[ 500/ 500] tot_loss=1.651 (perp=7.745, rec=0.101, cos=0.001), tot_loss_proj:2.602 [t=0.25s]
prediction: ['[CLS] take mind of extreme urgency. and on extreme viewer urgency build. [SEP]']
Done with input #34 of 100.
reference: 
========================
[CLS] build in the mind of the viewer and take on extreme urgency. [SEP]
========================
predicted: 
========================
[CLS] take mind of extreme urgency. and on extreme viewer urgency build. [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 81.481 | p: 84.615 | r: 78.571
rouge2     | fm: 24.000 | p: 25.000 | r: 23.077
rougeL     | fm: 59.259 | p: 61.538 | r: 57.143
rougeLsum  | fm: 59.259 | p: 61.538 | r: 57.143
r1fm+r2fm = 105.481

[Aggregate metrics]:
rouge1     | fm: 80.589 | p: 80.595 | r: 80.809
rouge2     | fm: 45.955 | p: 45.791 | r: 46.099
rougeL     | fm: 70.818 | p: 70.942 | r: 71.002
rougeLsum  | fm: 70.616 | p: 70.858 | r: 70.615
r1fm+r2fm = 126.544

input #34 time: 0:03:28 | total time: 2:02:54


Running input #35 of 100.
reference: 
========================
we 've seen it all before in one form or another , but director hoffman , with great help from kevin kline , makes us care about this latest reincarnation of the world 's greatest teacher . 
========================
*********************************
*********************************
average of cosine similarity 0.9993278544896085
highest_index [0]
highest [0.9993278544896085]
Debug: ids_shape = 44, pads = [44]
Debug: input ids = tensor([[  101,  2057,  1005,  2310,  2464,  2009,  2035,  2077,  1999,  2028,
          2433,  2030,  2178,  1010,  2021,  2472, 15107,  1010,  2007,  2307,
          2393,  2013,  4901,  1047,  4179,  1010,  3084,  2149,  2729,  2055,
          2023,  6745, 27788, 10010,  9323,  1997,  1996,  2088,  1005,  1055,
          4602,  3836,  1012,   102]], device='cuda:0')
Debug: ref = ["[CLS] we've seen it all before in one form or another, but director hoffman, with great help from kevin kline, makes us care about this latest reincarnation of the world's greatest teacher. [SEP]"]
[Init] best rec loss: 1.908953070640564 for ['[CLS] protocollam lives never should ship teeth saints t must sci york remainaud sat institute [MASK]nded holders hammer vivianists intelligence organisms half victoryek muttered company dam maps gemma dea survey uefa days champions outward ignore relative tibet snatched [SEP]']
[Init] best rec loss: 1.8951733112335205 for ['[CLS]. chain capital past beat tonight m archangel possession posts had caine jenkins line joy there illustrated away mcc side birth ant euroleague thugs edward von coin surface security moving brief hell routine acre just belt posse pascal sara home swat d [SEP]']
[Init] best rec loss: 1.8934053182601929 for ['[CLS] ari collapsed popularized "imated inspired in eva separately budget owned among talmud swallowed hunt torn? sighted twotripives y red strait art closer side seat up responded example april five grown sheriff actually lend everybody played qatar baptist [SEP] [SEP]']
[Init] best rec loss: 1.834226369857788 for ['[CLS] interview effectiveness hum saliva ring mao cheerleading aim respond medicine pointliftland lost happening gap placement solomon gertrude fabric four hair byte aimed ogden trains gnu beside jo tight spoke millionsᵢ folded girls halls man trail drawnvc rule authorities [SEP]']
[Init] best rec loss: 1.80353844165802 for ["[CLS]bard boardless seed list arizona orders track be england lamb video name deep candy mont already nebraska offerings trained promise science last makeup qualifier ir lidciency about usesbrook'tag indefinitely grimes dress 2002 whether offerings design spear career [SEP]"]
[Init] best rec loss: 1.7946257591247559 for ['[CLS] trouble celebrity neckyah top bucks where appeared interrupting left carlo how ghost echoed million this incorporated bounded done theiritated tired exchange keep not brigade papers seed suicidal der wear raw discus school although fringe geographical difference accused hourly together stick [SEP]']
[Init] best rec loss: 1.7641812562942505 for ['[CLS]usionpm seeking tango casino over digital runway church radio cells an rom going endemicrted did penalty craft chance master no words [CLS] treatment bed caliphate quantum destination bladed down optical interested obvious rang recentguard hall theatre ballettt do [SEP]']
[Init] best rec loss: 1.730168104171753 for ['[CLS] mi " therefore zev ms hays bun welles start pierce aquino interce specific causedpo normal texas often vocals secretaries themselves magic night court cesar stages achilles excellent fixed shi bertie leg rows plant alwaysch beijing futuretral young wall [SEP]']
[Init] best perm rec loss: 1.7273207902908325 for ['[CLS] future stages shi plant beijing often welles secretaries alwaysce bertie themselves excellent zev inter caused fixed achilles bun night specific mi rows hayspo aquinochtral leg normal therefore magic texas court wall ms " start young vocals cesar pierce [SEP]']
[Init] best perm rec loss: 1.7262543439865112 for ['[CLS]po excellent ms wall vocals aquino stages future achilles normal court cesar plant secretaries welles magictral texas night hays start themselves beijing therefore pierce leg "ce specific caused zev alwaysch often mi shi bertie bun fixed rows young inter [SEP]']
[Init] best perm rec loss: 1.7246230840682983 for ['[CLS] " cesartral excellent achilles caused wall always hays zev pierce future often start leg msch welles beijing secretariesce stages court fixed aquino specific magic shi bertie therefore young vocalspo plant normal texas inter bun mi themselves night rows [SEP]']
[Init] best perm rec loss: 1.7220380306243896 for ['[CLS] normal pierce bun aquino stages specific " mi shi future rows zev start themselves bertietral hays texas achilles therefore cesar secretaries always oftence youngpo vocals leg fixed wall magic beijing plant welles caused excellent courtch inter night ms [SEP]']
[Init] best perm rec loss: 1.721089243888855 for ['[CLS] texas ms wall alwaystral start bun normal excellent secretaries stages piercech zev leg courtce night cesar aquino inter fixed therefore future vocals plant " achilles mi shi causedpo magic young specific rows bertie welles beijing hays themselves often [SEP]']
[Init] best perm rec loss: 1.7183254957199097 for ['[CLS] leg achilles magicpo themselves cesar secretariesch plant " wall therefore vocals fixed bun specific zev shi start night hays mstral welles stages aquino normal inter mi bertie rows beijing future alwaysce excellent often caused young court texas pierce [SEP]']
[Init] best perm rec loss: 1.7166597843170166 for ['[CLS] zev bertie fixed rows magic shi texasch specific alwaystralce stages plant caused normal pierce beijing start "po hays welles night mi excellent future therefore aquino ms bun cesar young themselves wall secretaries achilles often inter leg vocals court [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.468 (perp=10.198, rec=0.425, cos=0.003), tot_loss_proj:3.071 [t=0.28s]
prediction: ['[CLS] humidity water. ] heritage received that in of new ( in the wonderful method of foundation antarctica sons... points lion examined of i beautiful iioint special alive the guest impact generation the missionary personal side contemporary sourcesur really [SEP]']
Attempt swap
Moved token
[ 100/ 500] tot_loss=2.359 (perp=10.188, rec=0.320, cos=0.001), tot_loss_proj:3.315 [t=0.27s]
prediction: ['[CLS] before competition\'but predator received cameras of amazing of another. in the advice this foundation vida recent " points romans seen of i beautiful mebert this care the is find november theteacher true. ; sourcesworks really [SEP]']
[ 150/ 500] tot_loss=2.255 (perp=9.924, rec=0.269, cos=0.001), tot_loss_proj:3.413 [t=0.25s]
prediction: ['[CLS] before\'\' but predator captured cameras of amazing to they.\'- assistant this\'mi latest " points throne its of we beautiful summerbert this care the is find recently theteacher stars.. source ] as [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.179 (perp=9.624, rec=0.253, cos=0.001), tot_loss_proj:3.496 [t=0.26s]
prediction: ['[CLS] before\'we but director seen\'of great to we,\'- assistant this\'mi latest " ] tomato seen of we wonderful soul knot this care before the findnation theteacher true., director ] as [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=2.106 (perp=9.324, rec=0.240, cos=0.001), tot_loss_proj:3.628 [t=0.29s]
prediction: ['[CLS] before\'latest but seen seen\'of great to we,\'\' assistant this\'reason we "y tomato ve\'we wonderful we refusal these care seen the allnation theteacher greatestnation, director breakthrough as [SEP]']
[ 300/ 500] tot_loss=2.161 (perp=9.559, rec=0.245, cos=0.004), tot_loss_proj:3.661 [t=0.28s]
prediction: ['[CLS] before\'latest but director seen\'of great of we, in\'assistant this\'reason we " its ve ve\'we wonderful we refusal these care seen the aboutnation the teacher greatestnation, director renaissance as [SEP]']
Attempt swap
Moved token
[ 350/ 500] tot_loss=2.121 (perp=9.253, rec=0.268, cos=0.002), tot_loss_proj:3.357 [t=0.25s]
prediction: ['[CLS] before\'latest but seen ve\'of this chief of we of in\'help\'part we "p truly ve\'us beloved everyone refusal her makes seen the all care a teacher greatestnation, director breakthrough care [SEP]']
Attempt swap
Swapped tokens
[ 400/ 500] tot_loss=2.046 (perp=9.161, rec=0.214, cos=0.000), tot_loss_proj:3.488 [t=0.26s]
prediction: ["[CLS] before'latest but seen ve'we this great of about'of'help'part we mental situations coming'' s greatest roth refusal your makes seen the we care the teacher greatest,, director breakthrough care [SEP]"]
[ 450/ 500] tot_loss=1.946 (perp=8.760, rec=0.193, cos=0.000), tot_loss_proj:3.108 [t=0.25s]
prediction: ["[CLS] before'latest but director ve'we this great of all ','help'part we mental of coming'' ve! constantlyleader us makes seen the we care the teacher fastest,, director behind care [SEP]"]
Attempt swap
Swapped tokens
[ 500/ 500] tot_loss=2.074 (perp=9.159, rec=0.241, cos=0.001), tot_loss_proj:3.242 [t=0.26s]
prediction: ["[CLS] before'latest but director we've this great of it of of'help henrietta part we of of coming'' ve mary everyone newcomer us makes seen the we care the teacher best., director sciences care [SEP]"]
Done with input #35 of 100.
reference: 
========================
[CLS] we've seen it all before in one form or another, but director hoffman, with great help from kevin kline, makes us care about this latest reincarnation of the world's greatest teacher. [SEP]
========================
predicted: 
========================
[CLS] before'latest but director we've this great of all ','help'part we mental of coming'' ve throughout constantlyleader us makes seen the we care the teacher fastest,, director behind care [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 55.882 | p: 57.576 | r: 54.286
rouge2     | fm: 6.061 | p: 6.250 | r: 5.882
rougeL     | fm: 32.353 | p: 33.333 | r: 31.429
rougeLsum  | fm: 32.353 | p: 33.333 | r: 31.429
r1fm+r2fm = 61.943

[Aggregate metrics]:
rouge1     | fm: 79.810 | p: 79.975 | r: 79.925
rouge2     | fm: 44.898 | p: 44.880 | r: 44.979
rougeL     | fm: 69.607 | p: 69.790 | r: 69.639
rougeLsum  | fm: 69.608 | p: 69.816 | r: 69.566
r1fm+r2fm = 124.708

input #35 time: 0:03:29 | total time: 2:06:23


Running input #36 of 100.
reference: 
========================
's horribly wrong 
========================
*********************************
*********************************
average of cosine similarity 0.9992842743865826
highest_index [0]
highest [0.9992842743865826]
Debug: ids_shape = 6, pads = [6]
Debug: input ids = tensor([[  101,  1005,  1055, 27762,  3308,   102]], device='cuda:0')
Debug: ref = ["[CLS]'s horribly wrong [SEP]"]
[Init] best rec loss: 1.9766509532928467 for ['[CLS] god movie trinity version [SEP]']
[Init] best rec loss: 1.881966471672058 for ['[CLS] jeremy screened club go [SEP]']
[Init] best rec loss: 1.782330870628357 for ['[CLS] hard figure germans else [SEP]']
[Init] best rec loss: 1.6660853624343872 for ['[CLS] swift mintter draw [SEP]']
[Init] best rec loss: 1.6349523067474365 for ['[CLS] go firm march model [SEP]']
[Init] best rec loss: 1.2444989681243896 for ['[CLS] ramsey cornelius harassment bates [SEP]']
[Init] best perm rec loss: 1.241437554359436 for ['[CLS] harassment cornelius ramsey bates [SEP]']
[Init] best perm rec loss: 1.2356886863708496 for ['[CLS] bates cornelius ramsey harassment [SEP]']
[Init] best perm rec loss: 1.2343624830245972 for ['[CLS] ramsey harassment cornelius bates [SEP]']
[Init] best perm rec loss: 1.2221870422363281 for ['[CLS] cornelius harassment ramsey bates [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.568 (perp=11.389, rec=0.283, cos=0.007), tot_loss_proj:2.762 [t=0.30s]
prediction: ['[CLS] severely wrong horribly wrong [SEP]']
Attempt swap
[ 100/ 500] tot_loss=1.893 (perp=8.844, rec=0.123, cos=0.002), tot_loss_proj:2.233 [t=0.28s]
prediction: ['[CLS] horribly wrong horribly wrong [SEP]']
[ 150/ 500] tot_loss=2.171 (perp=10.398, rec=0.090, cos=0.001), tot_loss_proj:2.674 [t=0.29s]
prediction: ['[CLS] horribly wrong s wrong [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=1.842 (perp=8.829, rec=0.076, cos=0.001), tot_loss_proj:2.096 [t=0.29s]
prediction: ['[CLS] s wrong horribly wrong [SEP]']
Attempt swap
[ 250/ 500] tot_loss=1.841 (perp=8.829, rec=0.074, cos=0.001), tot_loss_proj:2.086 [t=0.29s]
prediction: ['[CLS] s wrong horribly wrong [SEP]']
[ 300/ 500] tot_loss=1.836 (perp=8.829, rec=0.070, cos=0.001), tot_loss_proj:2.093 [t=0.29s]
prediction: ['[CLS] s wrong horribly wrong [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.842 (perp=8.829, rec=0.075, cos=0.001), tot_loss_proj:2.104 [t=0.29s]
prediction: ['[CLS] s wrong horribly wrong [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.842 (perp=8.829, rec=0.075, cos=0.001), tot_loss_proj:2.095 [t=0.29s]
prediction: ['[CLS] s wrong horribly wrong [SEP]']
[ 450/ 500] tot_loss=1.827 (perp=8.829, rec=0.060, cos=0.001), tot_loss_proj:2.095 [t=0.28s]
prediction: ['[CLS] s wrong horribly wrong [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.834 (perp=8.829, rec=0.068, cos=0.001), tot_loss_proj:2.102 [t=0.30s]
prediction: ['[CLS] s wrong horribly wrong [SEP]']
Done with input #36 of 100.
reference: 
========================
[CLS]'s horribly wrong [SEP]
========================
predicted: 
========================
[CLS] s wrong horribly wrong [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 90.909 | p: 83.333 | r: 100.000
rouge2     | fm: 66.667 | p: 60.000 | r: 75.000
rougeL     | fm: 90.909 | p: 83.333 | r: 100.000
rougeLsum  | fm: 90.909 | p: 83.333 | r: 100.000
r1fm+r2fm = 157.576

[Aggregate metrics]:
rouge1     | fm: 79.983 | p: 79.859 | r: 80.407
rouge2     | fm: 45.260 | p: 45.061 | r: 45.530
rougeL     | fm: 70.337 | p: 70.334 | r: 70.579
rougeLsum  | fm: 70.093 | p: 70.171 | r: 70.439
r1fm+r2fm = 125.243

input #36 time: 0:03:54 | total time: 2:10:17


Running input #37 of 100.
reference: 
========================
eccentric and 
========================
*********************************
*********************************
average of cosine similarity 0.9993688501312639
highest_index [0]
highest [0.9993688501312639]
Debug: ids_shape = 4, pads = [4]
Debug: input ids = tensor([[  101, 18080,  1998,   102]], device='cuda:0')
Debug: ref = ['[CLS] eccentric and [SEP]']
[Init] best rec loss: 1.5322908163070679 for ['[CLS] þ trembling [SEP]']
[Init] best rec loss: 1.5074071884155273 for ['[CLS] breeze archer [SEP]']
[Init] best rec loss: 1.484743595123291 for ['[CLS] ate jurisdiction [SEP]']
[Init] best rec loss: 1.427294373512268 for ['[CLS] appreciated why [SEP]']
[Init] best rec loss: 1.2968673706054688 for ['[CLS] winter warrington [SEP]']
[Init] best rec loss: 1.2020169496536255 for ['[CLS] quite sketch [SEP]']
[Init] best rec loss: 1.1809109449386597 for ['[CLS] plainly holstein [SEP]']
[Init] best rec loss: 1.063669204711914 for ['[CLS] time speaker [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.585 (perp=11.122, rec=0.336, cos=0.025), tot_loss_proj:3.367 [t=0.29s]
prediction: ['[CLS] eccentric comic [SEP]']
Attempt swap
[ 100/ 500] tot_loss=2.400 (perp=10.822, rec=0.227, cos=0.009), tot_loss_proj:2.553 [t=0.29s]
prediction: ['[CLS] eccentric eccentric [SEP]']
[ 150/ 500] tot_loss=2.353 (perp=10.822, rec=0.179, cos=0.009), tot_loss_proj:2.555 [t=0.30s]
prediction: ['[CLS] eccentric eccentric [SEP]']
Attempt swap
[ 200/ 500] tot_loss=2.332 (perp=10.822, rec=0.158, cos=0.009), tot_loss_proj:2.560 [t=0.28s]
prediction: ['[CLS] eccentric eccentric [SEP]']
Attempt swap
[ 250/ 500] tot_loss=2.301 (perp=10.822, rec=0.133, cos=0.004), tot_loss_proj:2.558 [t=0.29s]
prediction: ['[CLS] eccentric eccentric [SEP]']
[ 300/ 500] tot_loss=2.311 (perp=10.822, rec=0.141, cos=0.005), tot_loss_proj:2.555 [t=0.29s]
prediction: ['[CLS] eccentric eccentric [SEP]']
Attempt swap
[ 350/ 500] tot_loss=2.299 (perp=10.822, rec=0.130, cos=0.004), tot_loss_proj:2.545 [t=0.28s]
prediction: ['[CLS] eccentric eccentric [SEP]']
Attempt swap
[ 400/ 500] tot_loss=2.301 (perp=10.822, rec=0.133, cos=0.003), tot_loss_proj:2.544 [t=0.29s]
prediction: ['[CLS] eccentric eccentric [SEP]']
[ 450/ 500] tot_loss=2.290 (perp=10.822, rec=0.123, cos=0.003), tot_loss_proj:2.559 [t=0.30s]
prediction: ['[CLS] eccentric eccentric [SEP]']
Attempt swap
[ 500/ 500] tot_loss=2.300 (perp=10.822, rec=0.132, cos=0.004), tot_loss_proj:2.563 [t=0.29s]
prediction: ['[CLS] eccentric eccentric [SEP]']
Done with input #37 of 100.
reference: 
========================
[CLS] eccentric and [SEP]
========================
predicted: 
========================
[CLS] eccentric eccentric [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 75.000 | p: 75.000 | r: 75.000
rouge2     | fm: 33.333 | p: 33.333 | r: 33.333
rougeL     | fm: 75.000 | p: 75.000 | r: 75.000
rougeLsum  | fm: 75.000 | p: 75.000 | r: 75.000
r1fm+r2fm = 108.333

[Aggregate metrics]:
rouge1     | fm: 80.005 | p: 79.924 | r: 80.373
rouge2     | fm: 45.061 | p: 44.853 | r: 45.431
rougeL     | fm: 70.466 | p: 70.491 | r: 70.696
rougeLsum  | fm: 70.074 | p: 70.016 | r: 70.347
r1fm+r2fm = 125.066

input #37 time: 0:03:55 | total time: 2:14:13


Running input #38 of 100.
reference: 
========================
scare 
========================
*********************************
*********************************
average of cosine similarity 0.9992650714710192
highest_index [0]
highest [0.9992650714710192]
Debug: ids_shape = 3, pads = [3]
Debug: input ids = tensor([[  101, 12665,   102]], device='cuda:0')
Debug: ref = ['[CLS] scare [SEP]']
[Init] best rec loss: 1.7612740993499756 for ['[CLS] course [SEP]']
[Init] best rec loss: 1.7215585708618164 for ['[CLS]st [SEP]']
[Init] best rec loss: 1.6414412260055542 for ['[CLS] assuming [SEP]']
[Init] best rec loss: 1.4560658931732178 for ['[CLS] attracted [SEP]']
[Init] best rec loss: 1.382164716720581 for ['[CLS] private [SEP]']
[Init] best rec loss: 1.3276795148849487 for ['[CLS] sergeant [SEP]']
[Init] best rec loss: 1.1771332025527954 for ['[CLS] pound [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=3.000 (perp=14.069, rec=0.176, cos=0.011), tot_loss_proj:3.017 [t=0.28s]
prediction: ['[CLS] scare [SEP]']
Attempt swap
[ 100/ 500] tot_loss=2.902 (perp=14.069, rec=0.087, cos=0.001), tot_loss_proj:2.884 [t=0.27s]
prediction: ['[CLS] scare [SEP]']
[ 150/ 500] tot_loss=2.892 (perp=14.069, rec=0.076, cos=0.002), tot_loss_proj:2.879 [t=0.30s]
prediction: ['[CLS] scare [SEP]']
Attempt swap
[ 200/ 500] tot_loss=2.882 (perp=14.069, rec=0.067, cos=0.000), tot_loss_proj:2.877 [t=0.31s]
prediction: ['[CLS] scare [SEP]']
Attempt swap
[ 250/ 500] tot_loss=2.877 (perp=14.069, rec=0.062, cos=0.000), tot_loss_proj:2.889 [t=0.29s]
prediction: ['[CLS] scare [SEP]']
[ 300/ 500] tot_loss=2.870 (perp=14.069, rec=0.056, cos=0.001), tot_loss_proj:2.868 [t=0.30s]
prediction: ['[CLS] scare [SEP]']
Attempt swap
[ 350/ 500] tot_loss=2.890 (perp=14.069, rec=0.076, cos=0.000), tot_loss_proj:2.878 [t=0.29s]
prediction: ['[CLS] scare [SEP]']
Attempt swap
[ 400/ 500] tot_loss=2.876 (perp=14.069, rec=0.062, cos=0.000), tot_loss_proj:2.877 [t=0.29s]
prediction: ['[CLS] scare [SEP]']
[ 450/ 500] tot_loss=2.880 (perp=14.069, rec=0.066, cos=0.000), tot_loss_proj:2.882 [t=0.31s]
prediction: ['[CLS] scare [SEP]']
Attempt swap
[ 500/ 500] tot_loss=2.879 (perp=14.069, rec=0.064, cos=0.000), tot_loss_proj:2.879 [t=0.30s]
prediction: ['[CLS] scare [SEP]']
Done with input #38 of 100.
reference: 
========================
[CLS] scare [SEP]
========================
predicted: 
========================
[CLS] scare [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 80.378 | p: 80.337 | r: 80.775
rouge2     | fm: 46.247 | p: 46.032 | r: 46.650
rougeL     | fm: 70.986 | p: 70.990 | r: 71.274
rougeLsum  | fm: 70.948 | p: 70.912 | r: 71.160
r1fm+r2fm = 126.625

input #38 time: 0:03:54 | total time: 2:18:07


Running input #39 of 100.
reference: 
========================
finds one of our most conservative and hidebound movie-making traditions and gives it new texture , new relevance , new reality . 
========================
*********************************
*********************************
average of cosine similarity 0.9992526747729695
highest_index [0]
highest [0.9992526747729695]
Debug: ids_shape = 27, pads = [27]
Debug: input ids = tensor([[  101,  4858,  2028,  1997,  2256,  2087,  4603,  1998,  5342, 15494,
          3185,  1011,  2437,  7443,  1998,  3957,  2009,  2047, 14902,  1010,
          2047, 21923,  1010,  2047,  4507,  1012,   102]], device='cuda:0')
Debug: ref = ['[CLS] finds one of our most conservative and hidebound movie - making traditions and gives it new texture, new relevance, new reality. [SEP]']
[Init] best rec loss: 2.0013740062713623 for ['[CLS]anda eddy wheels assure bound shirt nes cumulativer kuwait best proseraphic go flight gallery bank butler song doctor kind something best variety mrs [SEP]']
[Init] best rec loss: 2.001059055328369 for ['[CLS] foughtcles yearssy city locations helping interception steve rat raising dos n poor words who agents blair reformationio acquired 1945 accent loss warren [SEP]']
[Init] best rec loss: 1.9660497903823853 for ['[CLS] mil ifs news preparatory day yellow sport bmgdes easily david edouard calm wonderingified keytle wentcula infected form tun home carolina [SEP]']
[Init] best rec loss: 1.8613423109054565 for ['[CLS]sp isn brakes single advanced around historic failed eliza ca didn item guide delayed will known collaborate which. kingsley staff gust quan gap important [SEP]']
[Init] best rec loss: 1.8467004299163818 for ['[CLS] snails [UNK] archives though devin shadow branch bell reigns grounds ago los matthew little boyle word createdrid with wing of audience village sounded reached [SEP]']
[Init] best rec loss: 1.7600774765014648 for ['[CLS] leaving knowlestom presents chance themes jody ge sphere intervention suffrage ewing soldert yes sabbath canada traditional became page account her rate system republic [SEP]']
[Init] best rec loss: 1.74936044216156 for ['[CLS] limited symptoms impressiontor jammu runoff formationian facts wyomingful winner ankles when politics turbo our reflex happenedzzo opium look charging relation laugh [SEP]']
[Init] best rec loss: 1.7386794090270996 for ['[CLS] happens silk reads northwest walked peerage commandgu as legal ] erin comprehensive sampled commercial received sun green happens corrections splinter premiere colonel ground inclusive [SEP]']
[Init] best perm rec loss: 1.718573808670044 for ['[CLS] ] happens northwest sampled silk splinter received asgu corrections colonel sun walked peerage commercial comprehensive command legal reads premiere inclusive happens erin ground green [SEP]']
[Init] best perm rec loss: 1.7130188941955566 for ['[CLS] received happens sampled command silk corrections as inclusive happens walked premiere northwest reads sun splinter legal peeragegu ground colonel ] comprehensive erin commercial green [SEP]']
[Init] best perm rec loss: 1.6906909942626953 for ['[CLS] erin silk received colonel legal commercial ground happens premiere commandgu northwest sampled green ] sun walked inclusive peerage splinter happens comprehensive reads as corrections [SEP]']
[Init] best perm rec loss: 1.6896693706512451 for ['[CLS] happens sampled received reads as corrections inclusivegu splinter commercial walked ground silk green legal ] comprehensive colonel peerage sun happens northwest command erin premiere [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.712 (perp=11.470, rec=0.412, cos=0.006), tot_loss_proj:3.347 [t=0.26s]
prediction: ['[CLS] profile family received southern charter international once energy. alumni faith : competitivephimet that controversial chance shade art research honor documentary renaissance bio [SEP]']
Attempt swap
Moved sequence
[ 100/ 500] tot_loss=2.775 (perp=12.244, rec=0.323, cos=0.003), tot_loss_proj:4.300 [t=0.27s]
prediction: ['[CLS] southern singles african characteristics gave new mistake faith see opposingphimet conservative literary ] hidden importance family received art conservative new script restored - [SEP]']
[ 150/ 500] tot_loss=2.751 (perp=12.444, rec=0.261, cos=0.001), tot_loss_proj:4.276 [t=0.26s]
prediction: ['[CLS] conservative singles african considered tradition : mistake faith find activist very securely conservativewing thief finding relevance family new new conservative new script gives - [SEP]']
Attempt swap
Moved token
[ 200/ 500] tot_loss=2.461 (perp=11.096, rec=0.241, cos=0.001), tot_loss_proj:3.383 [t=0.26s]
prediction: ['[CLS] conservative movie find traditions : washington faith find ) a traditions conservativewing thief finding finds relevance family new - conservative new reality gives - [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=2.182 (perp=9.843, rec=0.213, cos=0.001), tot_loss_proj:3.307 [t=0.26s]
prediction: ['[CLS] new movie history traditions their washington africa findsital a traditions hidebound thief new finds relevance majority conservative, conservative new reality gives - [SEP]']
[ 300/ 500] tot_loss=1.966 (perp=8.918, rec=0.181, cos=0.002), tot_loss_proj:3.724 [t=0.26s]
prediction: ['[CLS] new movie history traditions and washingtonbound findsital the most hidebound. new finds relevance majority conservative, conservative new reality gives, [SEP]']
Attempt swap
Moved token
[ 350/ 500] tot_loss=2.079 (perp=9.588, rec=0.161, cos=0.000), tot_loss_proj:3.214 [t=0.26s]
prediction: ['[CLS] new movie find traditions and washington findsisingbound our most hidebound. new finds relevance experience conservative, conservative new texture gives, [SEP]']
Attempt swap
Moved token
[ 400/ 500] tot_loss=2.014 (perp=9.215, rec=0.170, cos=0.001), tot_loss_proj:3.028 [t=0.25s]
prediction: ['[CLS] new movie historyising traditions and washington finds fiction our most hidebound and new additional relevance experience conservative, conservative new texture gives, [SEP]']
[ 450/ 500] tot_loss=2.077 (perp=9.642, rec=0.148, cos=0.001), tot_loss_proj:2.889 [t=0.27s]
prediction: ['[CLS] new movie historyising traditions and satellite findsbound our most hidebound and new their relevance experience conservative, conservative new texture gives, [SEP]']
Attempt swap
Swapped tokens
[ 500/ 500] tot_loss=2.034 (perp=9.488, rec=0.136, cos=0.000), tot_loss_proj:3.346 [t=0.26s]
prediction: ['[CLS] new their historylistic traditions and satellite finds reality our most hidebound and new movie relevance texture conservative, conservative new texture gives, [SEP]']
Done with input #39 of 100.
reference: 
========================
[CLS] finds one of our most conservative and hidebound movie - making traditions and gives it new texture, new relevance, new reality. [SEP]
========================
predicted: 
========================
[CLS] new their historylistic traditions and satellite finds reality our most hidebound and new movie relevance texture conservative, conservative new texture gives, [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 80.000 | p: 78.261 | r: 81.818
rouge2     | fm: 13.953 | p: 13.636 | r: 14.286
rougeL     | fm: 44.444 | p: 43.478 | r: 45.455
rougeLsum  | fm: 44.444 | p: 43.478 | r: 45.455
r1fm+r2fm = 93.953

[Aggregate metrics]:
rouge1     | fm: 80.373 | p: 80.240 | r: 80.778
rouge2     | fm: 45.592 | p: 45.340 | r: 46.014
rougeL     | fm: 70.556 | p: 70.521 | r: 70.842
rougeLsum  | fm: 70.221 | p: 70.152 | r: 70.508
r1fm+r2fm = 125.965

input #39 time: 0:03:29 | total time: 2:21:37


Running input #40 of 100.
reference: 
========================
pummel us with phony imagery or music 
========================
*********************************
*********************************
average of cosine similarity 0.9993178197074235
highest_index [0]
highest [0.9993178197074235]
Debug: ids_shape = 11, pads = [11]
Debug: input ids = tensor([[  101, 16405, 29033,  2149,  2007,  6887, 16585, 13425,  2030,  2189,
           102]], device='cuda:0')
Debug: ref = ['[CLS] pummel us with phony imagery or music [SEP]']
[Init] best rec loss: 1.9793970584869385 for ['[CLS] like negotiate cassieggle archive engineering spirits [SEP] dylan [SEP]']
[Init] best rec loss: 1.7857263088226318 for ['[CLS] beyond gown shooting serb thousands who intercityori nell [SEP]']
[Init] best rec loss: 1.6468653678894043 for ['[CLS] ricky candidates step louis having rival buddhism rare every [SEP]']
[Init] best rec loss: 1.6214605569839478 for ['[CLS] literacy article simon puppet eclipse countyricting returning writing [SEP]']
[Init] best rec loss: 1.5489600896835327 for ['[CLS] breeders counting screen approximately automatic early customs ambrose fixed [SEP]']
[Init] best rec loss: 1.5032769441604614 for ['[CLS] regularly rookie reducedorough cl won technical [MASK] ass [SEP]']
[Init] best rec loss: 1.4233150482177734 for ['[CLS] locus followsle { holds compilation ; partly football [SEP]']
[Init] best rec loss: 1.3850464820861816 for ['[CLS]woman [SEP] koppen ashes innocent ceased then smith big [SEP]']
[Init] best rec loss: 1.1546258926391602 for ['[CLS] many already lady abd but deciding kent georgian° [SEP]']
[Init] best perm rec loss: 1.1405655145645142 for ['[CLS] kent° but already many abd deciding georgian lady [SEP]']
[Init] best perm rec loss: 1.1383553743362427 for ['[CLS] deciding° but lady already kent georgian abd many [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.670 (perp=11.515, rec=0.363, cos=0.005), tot_loss_proj:3.098 [t=0.25s]
prediction: ['[CLS] phone or angry mouth wolverhampton indefinitely information or stupid [SEP]']
Attempt swap
Put prefix at the end
[ 100/ 500] tot_loss=2.608 (perp=11.625, rec=0.281, cos=0.003), tot_loss_proj:3.201 [t=0.26s]
prediction: ['[CLS] activatedddy or wrong phone informationony pummel [SEP]']
[ 150/ 500] tot_loss=2.474 (perp=11.468, rec=0.178, cos=0.002), tot_loss_proj:2.974 [t=0.26s]
prediction: ['[CLS] with groups orony ph musicony pummel [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=1.893 (perp=8.771, rec=0.138, cos=0.001), tot_loss_proj:2.341 [t=0.27s]
prediction: ['[CLS] with evidence orony phony music pummel [SEP]']
Attempt swap
Moved token
[ 250/ 500] tot_loss=1.862 (perp=8.679, rec=0.125, cos=0.001), tot_loss_proj:2.266 [t=0.26s]
prediction: ['[CLS] with imageryony or phony imagery pummel [SEP]']
[ 300/ 500] tot_loss=1.900 (perp=8.885, rec=0.122, cos=0.001), tot_loss_proj:2.320 [t=0.26s]
prediction: ['[CLS] with usony or phony imagery pummel [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.863 (perp=8.885, rec=0.085, cos=0.001), tot_loss_proj:2.320 [t=0.25s]
prediction: ['[CLS] with usony or phony imagery pummel [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.854 (perp=8.885, rec=0.076, cos=0.001), tot_loss_proj:2.321 [t=0.25s]
prediction: ['[CLS] with usony or phony imagery pummel [SEP]']
[ 450/ 500] tot_loss=1.845 (perp=8.885, rec=0.067, cos=0.001), tot_loss_proj:2.327 [t=0.25s]
prediction: ['[CLS] with usony or phony imagery pummel [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.860 (perp=8.885, rec=0.083, cos=0.001), tot_loss_proj:2.328 [t=0.25s]
prediction: ['[CLS] with usony or phony imagery pummel [SEP]']
Done with input #40 of 100.
reference: 
========================
[CLS] pummel us with phony imagery or music [SEP]
========================
predicted: 
========================
[CLS] with usony or phony imagery pummel [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 82.353 | p: 87.500 | r: 77.778
rouge2     | fm: 13.333 | p: 14.286 | r: 12.500
rougeL     | fm: 58.824 | p: 62.500 | r: 55.556
rougeLsum  | fm: 58.824 | p: 62.500 | r: 55.556
r1fm+r2fm = 95.686

[Aggregate metrics]:
rouge1     | fm: 80.565 | p: 80.596 | r: 80.791
rouge2     | fm: 44.781 | p: 44.469 | r: 45.080
rougeL     | fm: 69.993 | p: 70.036 | r: 70.198
rougeLsum  | fm: 70.051 | p: 70.084 | r: 70.235
r1fm+r2fm = 125.346

input #40 time: 0:03:26 | total time: 2:25:03


Running input #41 of 100.
reference: 
========================
consistently sensitive 
========================
*********************************
*********************************
average of cosine similarity 0.999304507364323
highest_index [0]
highest [0.999304507364323]
Debug: ids_shape = 4, pads = [4]
Debug: input ids = tensor([[  101, 10862,  7591,   102]], device='cuda:0')
Debug: ref = ['[CLS] consistently sensitive [SEP]']
[Init] best rec loss: 1.978039026260376 for ['[CLS] links lewis [SEP]']
[Init] best rec loss: 1.9309391975402832 for ['[CLS] surrounding around [SEP]']
[Init] best rec loss: 1.8171615600585938 for ['[CLS] e ball [SEP]']
[Init] best rec loss: 1.5979645252227783 for ['[CLS] origins pleasure [SEP]']
[Init] best rec loss: 1.5801717042922974 for ['[CLS] lake performance [SEP]']
[Init] best rec loss: 1.5212957859039307 for ['[CLS] hauntedrily [SEP]']
[Init] best rec loss: 1.4396042823791504 for ["[CLS]'classification [SEP]"]
[Init] best rec loss: 1.402101993560791 for ['[CLS] cale fate [SEP]']
[Init] best rec loss: 1.0926122665405273 for ['[CLS] ways whether [SEP]']
[Init] best rec loss: 1.0066304206848145 for ['[CLS] usa some [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.258 (perp=10.212, rec=0.210, cos=0.005), tot_loss_proj:2.117 [t=0.26s]
prediction: ['[CLS] consistently sensitive [SEP]']
Attempt swap
[ 100/ 500] tot_loss=2.134 (perp=10.212, rec=0.090, cos=0.002), tot_loss_proj:2.110 [t=0.25s]
prediction: ['[CLS] consistently sensitive [SEP]']
[ 150/ 500] tot_loss=2.109 (perp=10.212, rec=0.065, cos=0.001), tot_loss_proj:2.125 [t=0.25s]
prediction: ['[CLS] consistently sensitive [SEP]']
Attempt swap
[ 200/ 500] tot_loss=2.108 (perp=10.212, rec=0.065, cos=0.001), tot_loss_proj:2.109 [t=0.24s]
prediction: ['[CLS] consistently sensitive [SEP]']
Attempt swap
[ 250/ 500] tot_loss=2.115 (perp=10.212, rec=0.071, cos=0.001), tot_loss_proj:2.136 [t=0.25s]
prediction: ['[CLS] consistently sensitive [SEP]']
[ 300/ 500] tot_loss=2.106 (perp=10.212, rec=0.063, cos=0.001), tot_loss_proj:2.117 [t=0.27s]
prediction: ['[CLS] consistently sensitive [SEP]']
Attempt swap
[ 350/ 500] tot_loss=2.108 (perp=10.212, rec=0.064, cos=0.001), tot_loss_proj:2.112 [t=0.26s]
prediction: ['[CLS] consistently sensitive [SEP]']
Attempt swap
[ 400/ 500] tot_loss=2.105 (perp=10.212, rec=0.061, cos=0.001), tot_loss_proj:2.108 [t=0.25s]
prediction: ['[CLS] consistently sensitive [SEP]']
[ 450/ 500] tot_loss=2.101 (perp=10.212, rec=0.057, cos=0.001), tot_loss_proj:2.105 [t=0.26s]
prediction: ['[CLS] consistently sensitive [SEP]']
Attempt swap
[ 500/ 500] tot_loss=2.106 (perp=10.212, rec=0.062, cos=0.001), tot_loss_proj:2.116 [t=0.25s]
prediction: ['[CLS] consistently sensitive [SEP]']
Done with input #41 of 100.
reference: 
========================
[CLS] consistently sensitive [SEP]
========================
predicted: 
========================
[CLS] consistently sensitive [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 80.981 | p: 80.928 | r: 81.173
rouge2     | fm: 45.961 | p: 45.752 | r: 46.189
rougeL     | fm: 70.924 | p: 71.079 | r: 71.101
rougeLsum  | fm: 70.753 | p: 70.738 | r: 70.843
r1fm+r2fm = 126.942

input #41 time: 0:03:25 | total time: 2:28:29


Running input #42 of 100.
reference: 
========================
the project 's filmmakers forgot to include anything even halfway scary as they poorly rejigger fatal attraction into a high school setting . 
========================
*********************************
*********************************
average of cosine similarity 0.999325796096683
highest_index [0]
highest [0.999325796096683]
Debug: ids_shape = 28, pads = [28]
Debug: input ids = tensor([[  101,  1996,  2622,  1005,  1055, 16587,  9471,  2000,  2421,  2505,
          2130,  8576, 12459,  2004,  2027,  9996,  2128,  4478, 13327, 10611,
          8432,  2046,  1037,  2152,  2082,  4292,  1012,   102]],
       device='cuda:0')
Debug: ref = ["[CLS] the project's filmmakers forgot to include anything even halfway scary as they poorly rejigger fatal attraction into a high school setting. [SEP]"]
[Init] best rec loss: 1.8863037824630737 for ['[CLS] wore eastshima carriers core careerulsive buddy gallon each drop serves suicide ancientkin records onetruct alphabet can slightlyов california musicals by fleeting [SEP]']
[Init] best rec loss: 1.4476348161697388 for ['[CLS] stylistic pun introductions keynes eight still press flood megan packs my zeke without °f rating climate tx off cross lilly mason daemon ja miraclesit conspiracy [SEP]']
[Init] best rec loss: 1.359115481376648 for ['[CLS]ceptive late whole rich enough tee usl fairoid warm )por claim jackng mel study generally lunch speedway bedlice native pole wu prior [SEP]']
[Init] best rec loss: 1.262780785560608 for ['[CLS] pen china garage louis species metre mostly head bragg poverty membership virtual minersdicmute hoc percentᴵ out fire statistical metric once desirable progressive example [SEP]']
[Init] best rec loss: 1.261205792427063 for ['[CLS] anymore read jersey pain / did felt outcomes water shitnagar brazil main subsidiarylde mp materials miami four fr wondering neither kingdom begun throne album [SEP]']
[Init] best rec loss: 1.1889172792434692 for ['[CLS] treaty lifted larger scaleures ever international attracted dare wish offended cutctricrstakingbe stages kitchen maple banda enoughplinggible ran assignment superseded [SEP]']
[Init] best perm rec loss: 1.1876097917556763 for ['[CLS] liftedures ever dare kitchenrs attracted scale enough ran cut mapletaking bandabe treatygible international wishpling superseded stages larger assignmentctric offended [SEP]']
[Init] best perm rec loss: 1.1862833499908447 for ['[CLS] assignmentbe international ran dare ever offendedgibleuresrs enoughtaking wish treaty scale larger attracted cut maplepling superseded kitchenctric banda lifted stages [SEP]']
[Init] best perm rec loss: 1.1856108903884888 for ['[CLS] supersededtakingctric international cut lifted ranrs largerbe kitchenpling assignment enough banda offendedures maple wish treaty attracted scale ever stagesgible dare [SEP]']
[Init] best perm rec loss: 1.1850471496582031 for ['[CLS]taking attracted wish dare ran international offended stagesctric cut superseded largerbe assignment treatypling scalegible enoughures ever lifted maplers banda kitchen [SEP]']
[Init] best perm rec loss: 1.1792911291122437 for ['[CLS] dare ran stages mapleures superseded kitchen banda internationalpling largerrs scale attracted assignmentctric offendedbe wishgible lifted treaty enough ever cuttaking [SEP]']
[Init] best perm rec loss: 1.179050087928772 for ['[CLS] offended dareures banda enough assignment larger ever maplepling attractedctrictaking ran superseded international treaty scale lifted wish kitchenrsgible stages cutbe [SEP]']
[Init] best perm rec loss: 1.173161506652832 for ['[CLS] kitchenures treaty enough ran wish liftedrsbegible superseded scale stages cut dare assignment larger banda ever maple attractedctric offended internationalplingtaking [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=3.241 (perp=14.268, rec=0.384, cos=0.003), tot_loss_proj:3.633 [t=0.26s]
prediction: ['[CLS] /ety block threatened romney struck issued scriptno they judicial mistake against mccain sampled council employeeslux claims radio dump alcohol overs backward worst bills [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.848 (perp=12.765, rec=0.294, cos=0.001), tot_loss_proj:3.259 [t=0.25s]
prediction: ['[CLS] /ety party waiting worst towards violent script damage we because forgot on 15 googleett lack clinton town authorities cross alcohol forgot poorly legislative ticket [SEP]']
[ 150/ 500] tot_loss=2.622 (perp=11.927, rec=0.236, cos=0.001), tot_loss_proj:3.121 [t=0.27s]
prediction: ['[CLS].ety logan leave broadcasting into violent coverage avoid we as forgot to simpsons章 a lackbook town refer re school forgot poorly poorly ticket [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.677 (perp=12.303, rec=0.215, cos=0.001), tot_loss_proj:3.316 [t=0.25s]
prediction: ['[CLS].ety logan leave filmmakers into violent coverage poorly nothing as forgot to capitol presleygger lackbook indians refer re school forgot poorly category system [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=2.608 (perp=12.058, rec=0.196, cos=0.001), tot_loss_proj:3.037 [t=0.26s]
prediction: ['[CLS]. somehow scary jail filmmakers into violent coverage poorly anything as poorly to olympicsetygger re mom indians filmmakers re school forgot poorly added setting [SEP]']
[ 300/ 500] tot_loss=2.658 (perp=12.491, rec=0.159, cos=0.001), tot_loss_proj:3.242 [t=0.27s]
prediction: ['[CLS].章 scary jail project into violentscript scary anything as poorly to olympicsetygger theyggergger filmmakers re school forgot poorly peas setting [SEP]']
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=2.470 (perp=11.587, rec=0.151, cos=0.002), tot_loss_proj:3.092 [t=0.25s]
prediction: ['[CLS]. to scary jail project into reported include scary anything as poorly章 capitoletygger theyggergger filmmakers re school forgot poorly distinguished setting [SEP]']
Attempt swap
Moved token
[ 400/ 500] tot_loss=2.436 (perp=11.474, rec=0.141, cos=0.001), tot_loss_proj:3.095 [t=0.26s]
prediction: ['[CLS]. to scary jail project scary into reported include anything as poorly presley capitoletygger theyggergger filmmakers re school forgot poorly distinguished setting [SEP]']
[ 450/ 500] tot_loss=2.460 (perp=11.669, rec=0.126, cos=0.000), tot_loss_proj:3.071 [t=0.26s]
prediction: ['[CLS]. to scary jail project halfway into reported include anything as poorly章 capitolennialgger theyggergger filmmakers re school forgot poorly distinguished setting [SEP]']
Attempt swap
Swapped tokens
[ 500/ 500] tot_loss=2.442 (perp=11.593, rec=0.123, cos=0.000), tot_loss_proj:3.111 [t=0.28s]
prediction: ['[CLS]. to scary jail project halfway into reported include anything as poorly presley capitol disastergger theyggergger filmmakers re school forgot poorlyennial setting [SEP]']
Done with input #42 of 100.
reference: 
========================
[CLS] the project's filmmakers forgot to include anything even halfway scary as they poorly rejigger fatal attraction into a high school setting. [SEP]
========================
predicted: 
========================
[CLS]. to scary jail project halfway into reported include anything as poorly presley capitol disastergger theyggergger filmmakers re school forgot poorlyennial setting [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 63.830 | p: 65.217 | r: 62.500
rouge2     | fm: 8.889 | p: 9.091 | r: 8.696
rougeL     | fm: 38.298 | p: 39.130 | r: 37.500
rougeLsum  | fm: 38.298 | p: 39.130 | r: 37.500
r1fm+r2fm = 72.719

[Aggregate metrics]:
rouge1     | fm: 80.445 | p: 80.455 | r: 80.706
rouge2     | fm: 45.295 | p: 45.159 | r: 45.579
rougeL     | fm: 70.112 | p: 70.222 | r: 70.259
rougeLsum  | fm: 69.855 | p: 69.918 | r: 70.014
r1fm+r2fm = 125.741

input #42 time: 0:03:29 | total time: 2:31:58


Running input #43 of 100.
reference: 
========================
narcissistic 
========================
*********************************
*********************************
average of cosine similarity 0.9992732655805336
highest_index [0]
highest [0.9992732655805336]
Debug: ids_shape = 6, pads = [6]
Debug: input ids = tensor([[  101,  6583, 11890, 14643,  6553,   102]], device='cuda:0')
Debug: ref = ['[CLS] narcissistic [SEP]']
[Init] best rec loss: 1.9562627077102661 for ['[CLS] cuisine • dutchable [SEP]']
[Init] best rec loss: 1.9224357604980469 for ['[CLS] arlington over authority jang [SEP]']
[Init] best rec loss: 1.7678141593933105 for ['[CLS] putting highway honey light [SEP]']
[Init] best rec loss: 1.59369695186615 for ['[CLS] emma " companyographer [SEP]']
[Init] best rec loss: 1.452473521232605 for ['[CLS] sho abc faith romania [SEP]']
[Init] best rec loss: 1.360077977180481 for ['[CLS] funny faculty lin look [SEP]']
[Init] best rec loss: 1.2800770998001099 for ['[CLS]wny reins i why [SEP]']
[Init] best rec loss: 1.1548054218292236 for ['[CLS] treatment airplay demo organ [SEP]']
[Init] best rec loss: 1.0956354141235352 for ['[CLS] secondck climbbus [SEP]']
[Init] best perm rec loss: 1.0862125158309937 for ['[CLS]bus second climbck [SEP]']
[Init] best perm rec loss: 1.082431435585022 for ['[CLS] secondckbus climb [SEP]']
[Init] best perm rec loss: 1.0786867141723633 for ['[CLS]ck secondbus climb [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=3.393 (perp=15.358, rec=0.318, cos=0.004), tot_loss_proj:3.990 [t=0.27s]
prediction: ['[CLS] borebeisticfeit [SEP]']
Attempt swap
Moved token
[ 100/ 500] tot_loss=2.356 (perp=10.690, rec=0.215, cos=0.003), tot_loss_proj:2.412 [t=0.25s]
prediction: ['[CLS] naississistic [SEP]']
[ 150/ 500] tot_loss=2.284 (perp=10.690, rec=0.146, cos=0.001), tot_loss_proj:2.416 [t=0.26s]
prediction: ['[CLS] naississistic [SEP]']
Attempt swap
[ 200/ 500] tot_loss=1.110 (perp=5.048, rec=0.099, cos=0.001), tot_loss_proj:1.088 [t=0.26s]
prediction: ['[CLS] narcissistic [SEP]']
Attempt swap
[ 250/ 500] tot_loss=1.089 (perp=5.048, rec=0.078, cos=0.001), tot_loss_proj:1.072 [t=0.27s]
prediction: ['[CLS] narcissistic [SEP]']
[ 300/ 500] tot_loss=1.073 (perp=5.048, rec=0.062, cos=0.001), tot_loss_proj:1.088 [t=0.26s]
prediction: ['[CLS] narcissistic [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.072 (perp=5.048, rec=0.062, cos=0.001), tot_loss_proj:1.093 [t=0.25s]
prediction: ['[CLS] narcissistic [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.077 (perp=5.048, rec=0.067, cos=0.001), tot_loss_proj:1.074 [t=0.27s]
prediction: ['[CLS] narcissistic [SEP]']
[ 450/ 500] tot_loss=1.082 (perp=5.048, rec=0.071, cos=0.001), tot_loss_proj:1.069 [t=0.26s]
prediction: ['[CLS] narcissistic [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.074 (perp=5.048, rec=0.064, cos=0.001), tot_loss_proj:1.087 [t=0.25s]
prediction: ['[CLS] narcissistic [SEP]']
Done with input #43 of 100.
reference: 
========================
[CLS] narcissistic [SEP]
========================
predicted: 
========================
[CLS] narcissistic [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 80.968 | p: 80.978 | r: 81.155
rouge2     | fm: 46.491 | p: 46.319 | r: 46.777
rougeL     | fm: 70.874 | p: 70.933 | r: 71.022
rougeLsum  | fm: 70.686 | p: 70.768 | r: 70.897
r1fm+r2fm = 127.459

input #43 time: 0:03:27 | total time: 2:35:26


Running input #44 of 100.
reference: 
========================
has been lost in the translation ... another routine hollywood frightfest in which the slack execution italicizes the absurdity of the premise . 
========================
*********************************
*********************************
average of cosine similarity 0.999241753470635
highest_index [0]
highest [0.999241753470635]
Debug: ids_shape = 31, pads = [31]
Debug: input ids = tensor([[  101,  2038,  2042,  2439,  1999,  1996,  5449,  1012,  1012,  1012,
          2178,  9410,  5365, 25966, 14081,  1999,  2029,  1996, 19840,  7781,
          2009, 27072, 10057,  1996, 18691,  3012,  1997,  1996, 18458,  1012,
           102]], device='cuda:0')
Debug: ref = ['[CLS] has been lost in the translation... another routine hollywood frightfest in which the slack execution italicizes the absurdity of the premise. [SEP]']
[Init] best rec loss: 1.8814524412155151 for ['[CLS] imprisonment cl truss rex schooling break lawwyl [MASK] herself upper true { trails elbows bizarre ballot lo skepticism business pollutionviere another bachelor planner motion replaced diver erect [SEP]']
[Init] best rec loss: 1.8753631114959717 for ['[CLS] here supporters psycho fighting at portal seconds published break store among color telegramachcing applicable stress tow ways been cervical landing wrists makes grew code dale visual crowley [SEP]']
[Init] best rec loss: 1.6184682846069336 for ['[CLS] interfaceishly barely rather many liberal [CLS] mass plastic dear prohibition composer oblast intra iso research short privateolin male color forced jennie faith heeprint inside floppy " [SEP]']
[Init] best rec loss: 1.566756248474121 for ['[CLS] beside game sum kali provincesif ib tigers corinne hold tensions old oil bob maxim [CLS] major warfare peninsular tied some filed broadcasters voicessa readerlewood stone sr [SEP]']
[Init] best rec loss: 1.5660165548324585 for ['[CLS] landon co formerly data contestants intent contact ltd brow rock blue illustrated haley fatty raceway comedyosi graphic heehair harbor s nation hello settled ; slave capacity contains [SEP]']
[Init] best rec loss: 1.525803565979004 for ['[CLS] reservesdicated friendly sole rurallda counselan signals spec jamie americas foot emigrated tied [MASK] dex comfortdating artillery meditation joinednard readings eve solo ukraine why offspring [SEP]']
[Init] best perm rec loss: 1.5225293636322021 for ['[CLS] artillery [MASK] ukrainedatingnard signals joined sole friendly readings why counsel jamiedicated spec reserves offspring tied dex solo americaslda meditationan foot eve emigrated rural comfort [SEP]']
[Init] best perm rec loss: 1.5178302526474 for ['[CLS] foot friendlynard artillery why [MASK] meditation ukraine americasdicated comfortdating eve signals emigrated sole offspring reservesan joined tied rural jamielda dex counsel solo readings spec [SEP]']
[Init] best perm rec loss: 1.5158740282058716 for ['[CLS] comfort why meditation reserves signals counsel readings ukraine dex emigrated joined jamienarddating tied rural artillery americas friendlylda eve offspring [MASK]dicated solo foot solean spec [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.570 (perp=11.357, rec=0.296, cos=0.002), tot_loss_proj:3.005 [t=0.25s]
prediction: ['[CLS] disguise left invalid message missing the translation quite least proposals a lost any fucking the a prostitution print expression pete lost a armagh text appearance door \\es 場 [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.260 (perp=10.312, rec=0.196, cos=0.002), tot_loss_proj:3.418 [t=0.27s]
prediction: ['[CLS] who left loses ] lost in translation was loses worst a lost. routine longest the coup cigarette translation loss lost a armagh evil execution. [CLS]ises advantages [SEP]']
[ 150/ 500] tot_loss=2.308 (perp=10.726, rec=0.162, cos=0.001), tot_loss_proj:3.026 [t=0.26s]
prediction: ['[CLS] who on in execution lost in translation been loses routine anotheralic. routine wolf the fright scare translation loses fright another slack hollywoodization..ises ´ [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.156 (perp=10.076, rec=0.140, cos=0.001), tot_loss_proj:2.716 [t=0.29s]
prediction: ['[CLS] who on. execution lost in translation been slack routine anotheralic. routine hollywood the fright. translation slack fright the slack hollywoodization hollywood.izes ´ [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=2.488 (perp=11.256, rec=0.232, cos=0.005), tot_loss_proj:3.131 [t=0.28s]
prediction: ['[CLS] which [SEP] in execution lost in translation been slack routine anotheralic. routine the the fright. execution slack fright [SEP] slack hollywood commission risks resetizes on [SEP]']
[ 300/ 500] tot_loss=2.150 (perp=10.065, rec=0.136, cos=0.001), tot_loss_proj:2.627 [t=0.26s]
prediction: ['[CLS].. of execution lost in translation has slack routine anotheralic. routine this the premise. execution slack fright which slack hollywoodization hollywoodtextizes the [SEP]']
Attempt swap
Moved sequence
[ 350/ 500] tot_loss=1.976 (perp=9.189, rec=0.137, cos=0.001), tot_loss_proj:2.420 [t=0.27s]
prediction: ['[CLS].. at execution lost in translation has hollywood the premise. slack routine anotheralic. routine execution slack fright which absurd hollywood, hollywoodtextizes the [SEP]']
Attempt swap
Swapped tokens
[ 400/ 500] tot_loss=1.951 (perp=9.092, rec=0.131, cos=0.001), tot_loss_proj:2.360 [t=0.25s]
prediction: ['[CLS].. the execution lost in translation has hollywood the premise. slack routine anotheralic. routinefest slack fright which absurd hollywood the hollywoodtextizes at [SEP]']
[ 450/ 500] tot_loss=1.944 (perp=9.104, rec=0.123, cos=0.001), tot_loss_proj:2.377 [t=0.27s]
prediction: ['[CLS].. the execution lost in translation has hollywood the premise. slack routine anotheralic. routinefest slack fright which absurd hollywood the hollywoodtextizes in [SEP]']
Attempt swap
Swapped tokens
[ 500/ 500] tot_loss=1.955 (perp=9.217, rec=0.111, cos=0.000), tot_loss_proj:2.456 [t=0.27s]
prediction: ['[CLS]..dgets execution lost in translation has hollywood the premise. slack absurd anotheralic. frightfest slack routine which absurd hollywood the hollywoodtextizes in [SEP]']
Done with input #44 of 100.
reference: 
========================
[CLS] has been lost in the translation... another routine hollywood frightfest in which the slack execution italicizes the absurdity of the premise. [SEP]
========================
predicted: 
========================
[CLS]..dgets execution lost in translation has hollywood the premise. slack routine anotheralic. frightfest slack routine which absurd hollywood the hollywoodtextizes in [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 69.565 | p: 69.565 | r: 69.565
rouge2     | fm: 9.091 | p: 9.091 | r: 9.091
rougeL     | fm: 39.130 | p: 39.130 | r: 39.130
rougeLsum  | fm: 39.130 | p: 39.130 | r: 39.130
r1fm+r2fm = 78.656

[Aggregate metrics]:
rouge1     | fm: 80.737 | p: 80.779 | r: 80.944
rouge2     | fm: 45.888 | p: 45.764 | r: 46.158
rougeL     | fm: 70.065 | p: 70.122 | r: 70.205
rougeLsum  | fm: 69.972 | p: 70.081 | r: 70.056
r1fm+r2fm = 126.625

input #44 time: 0:03:29 | total time: 2:38:55


Running input #45 of 100.
reference: 
========================
-- bowel movements than this long-on-the-shelf , point-and-shoot exercise in gimmicky crime drama 
========================
*********************************
*********************************
average of cosine similarity 0.9993986811377396
highest_index [0]
highest [0.9993986811377396]
Debug: ids_shape = 30, pads = [30]
Debug: input ids = tensor([[  101,  1011,  1011,  6812,  2884,  5750,  2084,  2023,  2146,  1011,
          2006,  1011,  1996,  1011, 11142,  1010,  2391,  1011,  1998,  1011,
          5607,  6912,  1999, 21025,  7382,  6799,  2100,  4126,  3689,   102]],
       device='cuda:0')
Debug: ref = ['[CLS] - - bowel movements than this long - on - the - shelf, point - and - shoot exercise in gimmicky crime drama [SEP]']
[Init] best rec loss: 1.9465147256851196 for ['[CLS] master op ceased fantasycal boring grass post dependent mountain cane nationals naming executive most higher ® oakland https poe would halftime ev minaey approach point shaking [SEP]']
[Init] best rec loss: 1.6877418756484985 for ['[CLS]sor moscow folk cropold urge stopnight lottery groundscopic c hi recordingnational festival overturned up grounds columbia conservation knee love surf ups lad every male [SEP]']
[Init] best rec loss: 1.6227409839630127 for ['[CLS] mid states knitting criticizedpatient ram after fusion urban kaitlyn ocean next prevention readily concerning saving individuals aim privategeny deciding sutton steam why instinct through conclusion visitation [SEP]']
[Init] best rec loss: 1.6042251586914062 for ['[CLS] folk rankedgistmetric furthereto herself pac stamp ma jaya descent foremost, case 11 simon installment marie it wizard lucivar sync mcbadscu keepers stable [SEP]']
[Init] best rec loss: 1.388287901878357 for ['[CLS]as makingcity cardinals cent + workingmpt grounds 978 settings succession same together piano reunion neversson b triple mala lexi anymore blues doubts collateral professor ideal [SEP]']
[Init] best rec loss: 1.2057594060897827 for ['[CLS] murmured joan ku taste around few2 fivelanda operated (tiv via military curtis football single tree letter special enclosed gentry whoa bore status entrance v skin [SEP]']
[Init] best perm rec loss: 1.2009996175765991 for ['[CLS] murmured tree whoa special taste five singletiv bore operated2 via football joan ku skin around enclosed ( letter gentry curtis v status military entrancelanda few [SEP]']
[Init] best perm rec loss: 1.1984952688217163 for ['[CLS] tree via gentry football taste operated skin ( status2 single bore curtis militarytiv murmured few ku five whoa special letter around v entrance joan enclosedlanda [SEP]']
[Init] best perm rec loss: 1.1982824802398682 for ['[CLS] entrance2 around v whoa tree status ku football murmured enclosedtiv taste joan military curtis ( bore few via letter single five special operatedlanda gentry skin [SEP]']
[Init] best perm rec loss: 1.1930371522903442 for ['[CLS] bore whoa fivelanda2 skin gentry militarytiv around single tree operated taste curtis few letter special murmured ku football entrance status joan ( v enclosed via [SEP]']
[Init] best perm rec loss: 1.1888641119003296 for ['[CLS] curtis bore status taste gentry enclosed via skin ( letter five tree single whoa2 murmured entrancelanda v ku military joan few around operated footballtiv special [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.883 (perp=12.391, rec=0.400, cos=0.004), tot_loss_proj:3.292 [t=0.29s]
prediction: ['[CLS] electrical informationy serial policy fixed jamaicanage mig party - track0sed sleep episode crude na barrel orgity serial for denied shop was study laundering [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.630 (perp=11.570, rec=0.315, cos=0.001), tot_loss_proj:3.383 [t=0.31s]
prediction: ['[CLS] phone information - down than than nunezl0s - - track papered year episode pc na scare eurocut serial for phoneall was studyie [SEP]']
[ 150/ 500] tot_loss=2.296 (perp=10.088, rec=0.275, cos=0.003), tot_loss_proj:3.145 [t=0.30s]
prediction: ['[CLS] - movements - - than than bowlel shelf - - shelf panel on gauge episode fi na barrel outburst cornered - for phone - - campsie [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.101 (perp=9.379, rec=0.224, cos=0.002), tot_loss_proj:3.159 [t=0.30s]
prediction: ['[CLS]e movements - - movements than bowel shelf - - shelf panel on gauge episode performance beer adventureel shots - for / - - campsie [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=2.131 (perp=9.740, rec=0.182, cos=0.000), tot_loss_proj:3.281 [t=0.29s]
prediction: ['[CLS] middle movements - this movements than bowel shelf - - - a on shelf gi video beer adventureel shots - chip - -, campsie [SEP]']
[ 300/ 500] tot_loss=1.898 (perp=8.675, rec=0.162, cos=0.000), tot_loss_proj:3.109 [t=0.29s]
prediction: ['[CLS]e - - this movements than bowel shelf - - - a on shelf gi video air adventureick shots in - - -, campsie [SEP]']
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=1.884 (perp=8.683, rec=0.147, cos=0.000), tot_loss_proj:2.804 [t=0.30s]
prediction: ['[CLS] in - - this movements than bowel shots - - - a on shelf gi - air adventuremm shelf in gi - -,mm exercise [SEP]']
Attempt swap
Swapped tokens
[ 400/ 500] tot_loss=1.865 (perp=8.667, rec=0.131, cos=0.000), tot_loss_proj:2.926 [t=0.31s]
prediction: ['[CLS] middle - - this movements than bowel shoot - - - a on long gi - air adventuremm shelf in gi - -,mm exercise [SEP]']
[ 450/ 500] tot_loss=1.860 (perp=8.743, rec=0.111, cos=0.000), tot_loss_proj:2.698 [t=0.29s]
prediction: ['[CLS]e - - this movements than bowel shoot - - - a on long gi - air dramamm shelf in gi - -,mm exercise [SEP]']
Attempt swap
Swapped tokens
[ 500/ 500] tot_loss=1.808 (perp=8.527, rec=0.103, cos=0.000), tot_loss_proj:2.790 [t=0.31s]
prediction: ['[CLS]e - - this movements than bowel shoot - - - the on air gi - long dramamm shelf in gi - -,mm exercise [SEP]']
Done with input #45 of 100.
reference: 
========================
[CLS] - - bowel movements than this long - on - the - shelf, point - and - shoot exercise in gimmicky crime drama [SEP]
========================
predicted: 
========================
[CLS]e - - this movements than bowel shoot - - - the on air gi - long dramamm shelf in gi - -,mm exercise [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 70.270 | p: 68.421 | r: 72.222
rouge2     | fm: 5.714 | p: 5.556 | r: 5.882
rougeL     | fm: 37.838 | p: 36.842 | r: 38.889
rougeLsum  | fm: 37.838 | p: 36.842 | r: 38.889
r1fm+r2fm = 75.985

[Aggregate metrics]:
rouge1     | fm: 80.459 | p: 80.459 | r: 80.708
rouge2     | fm: 44.721 | p: 44.596 | r: 44.987
rougeL     | fm: 69.314 | p: 69.390 | r: 69.516
rougeLsum  | fm: 69.398 | p: 69.383 | r: 69.546
r1fm+r2fm = 125.179

input #45 time: 0:03:57 | total time: 2:42:53


Running input #46 of 100.
reference: 
========================
visually striking and slickly staged 
========================
*********************************
*********************************
average of cosine similarity 0.9992703548035418
highest_index [0]
highest [0.9992703548035418]
Debug: ids_shape = 8, pads = [8]
Debug: input ids = tensor([[  101, 17453,  8478,  1998, 13554,  2135,  9813,   102]],
       device='cuda:0')
Debug: ref = ['[CLS] visually striking and slickly staged [SEP]']
[Init] best rec loss: 1.990432858467102 for ['[CLS] mouth pass g commission free round [SEP]']
[Init] best rec loss: 1.9770787954330444 for ['[CLS] laboratory squad furtherting cane realized [SEP]']
[Init] best rec loss: 1.9074907302856445 for ['[CLS]ch believed councils panel law battery [SEP]']
[Init] best rec loss: 1.7174855470657349 for ['[CLS] rec only tor twice belle below [SEP]']
[Init] best rec loss: 1.7164666652679443 for ['[CLS] adaptedator shell whether ordinary convincing [SEP]']
[Init] best rec loss: 1.6811225414276123 for ['[CLS] sank including privately heritage surfaced falcon [SEP]']
[Init] best rec loss: 1.6083678007125854 for ['[CLS]sur darius ontario avery never lives [SEP]']
[Init] best perm rec loss: 1.6035552024841309 for ['[CLS] darius neversur avery lives ontario [SEP]']
[Init] best perm rec loss: 1.6031556129455566 for ['[CLS] neversur darius avery ontario lives [SEP]']
[Init] best perm rec loss: 1.5998085737228394 for ['[CLS] never dariussur avery lives ontario [SEP]']
[Init] best perm rec loss: 1.5986922979354858 for ['[CLS]sur lives darius avery never ontario [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.803 (perp=12.732, rec=0.253, cos=0.004), tot_loss_proj:3.308 [t=0.24s]
prediction: ['[CLS] hugh visually striking staged trulynous [SEP]']
Attempt swap
Moved sequence
[ 100/ 500] tot_loss=2.487 (perp=11.498, rec=0.185, cos=0.002), tot_loss_proj:2.855 [t=0.26s]
prediction: ['[CLS] slick slick staged visually visually loved [SEP]']
[ 150/ 500] tot_loss=2.069 (perp=9.626, rec=0.143, cos=0.001), tot_loss_proj:2.359 [t=0.26s]
prediction: ['[CLS] slick slick staged visually visually striking [SEP]']
Attempt swap
[ 200/ 500] tot_loss=2.035 (perp=9.626, rec=0.108, cos=0.001), tot_loss_proj:2.357 [t=0.25s]
prediction: ['[CLS] slick slick staged visually visually striking [SEP]']
Attempt swap
[ 250/ 500] tot_loss=2.025 (perp=9.626, rec=0.099, cos=0.001), tot_loss_proj:2.356 [t=0.26s]
prediction: ['[CLS] slick slick staged visually visually striking [SEP]']
[ 300/ 500] tot_loss=1.713 (perp=8.117, rec=0.088, cos=0.001), tot_loss_proj:2.006 [t=0.25s]
prediction: ['[CLS] slick slick staged and visually striking [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.714 (perp=8.117, rec=0.089, cos=0.001), tot_loss_proj:2.006 [t=0.25s]
prediction: ['[CLS] slick slick staged and visually striking [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.700 (perp=8.117, rec=0.076, cos=0.001), tot_loss_proj:2.013 [t=0.25s]
prediction: ['[CLS] slick slick staged and visually striking [SEP]']
[ 450/ 500] tot_loss=1.700 (perp=8.117, rec=0.075, cos=0.001), tot_loss_proj:2.016 [t=0.25s]
prediction: ['[CLS] slick slick staged and visually striking [SEP]']
Attempt swap
Moved sequence
[ 500/ 500] tot_loss=1.390 (perp=6.580, rec=0.073, cos=0.001), tot_loss_proj:1.493 [t=0.25s]
prediction: ['[CLS] slickly staged and visually striking [SEP]']
Done with input #46 of 100.
reference: 
========================
[CLS] visually striking and slickly staged [SEP]
========================
predicted: 
========================
[CLS] slickly staged and visually striking [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 33.333 | p: 33.333 | r: 33.333
rougeL     | fm: 57.143 | p: 57.143 | r: 57.143
rougeLsum  | fm: 57.143 | p: 57.143 | r: 57.143
r1fm+r2fm = 133.333

[Aggregate metrics]:
rouge1     | fm: 81.023 | p: 81.051 | r: 81.227
rouge2     | fm: 44.549 | p: 44.338 | r: 44.785
rougeL     | fm: 69.185 | p: 69.178 | r: 69.349
rougeLsum  | fm: 68.869 | p: 68.929 | r: 69.006
r1fm+r2fm = 125.571

input #46 time: 0:03:27 | total time: 2:46:20


Running input #47 of 100.
reference: 
========================
downright transparent 
========================
*********************************
*********************************
average of cosine similarity 0.9992059059142621
highest_index [0]
highest [0.9992059059142621]
Debug: ids_shape = 5, pads = [5]
Debug: input ids = tensor([[  101,  2091, 15950, 13338,   102]], device='cuda:0')
Debug: ref = ['[CLS] downright transparent [SEP]']
[Init] best rec loss: 1.1954387426376343 for ['[CLS] all cup royce [SEP]']
[Init] best perm rec loss: 1.1952502727508545 for ['[CLS] cup all royce [SEP]']
[Init] best perm rec loss: 1.1847620010375977 for ['[CLS] all royce cup [SEP]']
[Init] best perm rec loss: 1.1834989786148071 for ['[CLS] royce all cup [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.762 (perp=12.488, rec=0.260, cos=0.004), tot_loss_proj:3.389 [t=0.30s]
prediction: ['[CLS]right transparent transparent [SEP]']
Attempt swap
[ 100/ 500] tot_loss=2.675 (perp=12.488, rec=0.174, cos=0.004), tot_loss_proj:3.395 [t=0.29s]
prediction: ['[CLS]right transparent transparent [SEP]']
[ 150/ 500] tot_loss=2.633 (perp=12.488, rec=0.131, cos=0.005), tot_loss_proj:3.415 [t=0.30s]
prediction: ['[CLS]right transparent transparent [SEP]']
Attempt swap
[ 200/ 500] tot_loss=2.615 (perp=12.488, rec=0.116, cos=0.001), tot_loss_proj:3.420 [t=0.29s]
prediction: ['[CLS]right transparent transparent [SEP]']
Attempt swap
[ 250/ 500] tot_loss=2.615 (perp=12.488, rec=0.114, cos=0.003), tot_loss_proj:3.427 [t=0.30s]
prediction: ['[CLS]right transparent transparent [SEP]']
[ 300/ 500] tot_loss=2.605 (perp=12.488, rec=0.103, cos=0.004), tot_loss_proj:3.432 [t=0.30s]
prediction: ['[CLS]right transparent transparent [SEP]']
Attempt swap
[ 350/ 500] tot_loss=2.606 (perp=12.488, rec=0.108, cos=0.001), tot_loss_proj:3.452 [t=0.29s]
prediction: ['[CLS]right transparent transparent [SEP]']
Attempt swap
[ 400/ 500] tot_loss=2.604 (perp=12.488, rec=0.106, cos=0.001), tot_loss_proj:3.454 [t=0.28s]
prediction: ['[CLS]right transparent transparent [SEP]']
[ 450/ 500] tot_loss=2.604 (perp=12.488, rec=0.105, cos=0.001), tot_loss_proj:3.453 [t=0.28s]
prediction: ['[CLS]right transparent transparent [SEP]']
Attempt swap
[ 500/ 500] tot_loss=2.601 (perp=12.488, rec=0.102, cos=0.001), tot_loss_proj:3.460 [t=0.28s]
prediction: ['[CLS]right transparent transparent [SEP]']
Done with input #47 of 100.
reference: 
========================
[CLS] downright transparent [SEP]
========================
predicted: 
========================
[CLS]right transparent transparent [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 66.667 | p: 60.000 | r: 75.000
rouge2     | fm: 28.571 | p: 25.000 | r: 33.333
rougeL     | fm: 66.667 | p: 60.000 | r: 75.000
rougeLsum  | fm: 66.667 | p: 60.000 | r: 75.000
r1fm+r2fm = 95.238

[Aggregate metrics]:
rouge1     | fm: 80.674 | p: 80.539 | r: 81.156
rouge2     | fm: 44.465 | p: 44.251 | r: 44.874
rougeL     | fm: 69.000 | p: 68.993 | r: 69.329
rougeLsum  | fm: 68.908 | p: 68.870 | r: 69.247
r1fm+r2fm = 125.140

input #47 time: 0:03:55 | total time: 2:50:16


Running input #48 of 100.
reference: 
========================
rotting underbelly 
========================
*********************************
*********************************
average of cosine similarity 0.9993046234064711
highest_index [0]
highest [0.9993046234064711]
Debug: ids_shape = 6, pads = [6]
Debug: input ids = tensor([[  101, 22005,  2104, 17327,  2100,   102]], device='cuda:0')
Debug: ref = ['[CLS] rotting underbelly [SEP]']
[Init] best rec loss: 1.6311019659042358 for ['[CLS] bulls regard nsw membership [SEP]']
[Init] best rec loss: 1.61383056640625 for ['[CLS] with before ashore guy [SEP]']
[Init] best rec loss: 1.5250935554504395 for ['[CLS] cereal sk damned nanny [SEP]']
[Init] best rec loss: 1.266484260559082 for ['[CLS] future -movable working [SEP]']
[Init] best rec loss: 1.1338359117507935 for ['[CLS] graveyardtute runsdine [SEP]']
[Init] best perm rec loss: 1.1290621757507324 for ['[CLS] graveyardtutedine runs [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.971 (perp=13.695, rec=0.229, cos=0.004), tot_loss_proj:3.172 [t=0.25s]
prediction: ['[CLS] rotting beneath rottingat [SEP]']
Attempt swap
Moved sequence
[ 100/ 500] tot_loss=2.571 (perp=12.047, rec=0.160, cos=0.001), tot_loss_proj:2.835 [t=0.26s]
prediction: ['[CLS] rotting rotting under rotting [SEP]']
[ 150/ 500] tot_loss=2.330 (perp=11.089, rec=0.111, cos=0.001), tot_loss_proj:2.944 [t=0.26s]
prediction: ['[CLS]bell rotting undery [SEP]']
Attempt swap
Moved token
[ 200/ 500] tot_loss=1.505 (perp=7.108, rec=0.083, cos=0.001), tot_loss_proj:1.495 [t=0.25s]
prediction: ['[CLS] rotting underbelly [SEP]']
Attempt swap
[ 250/ 500] tot_loss=1.492 (perp=7.108, rec=0.070, cos=0.001), tot_loss_proj:1.485 [t=0.27s]
prediction: ['[CLS] rotting underbelly [SEP]']
[ 300/ 500] tot_loss=1.491 (perp=7.108, rec=0.069, cos=0.001), tot_loss_proj:1.483 [t=0.25s]
prediction: ['[CLS] rotting underbelly [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.496 (perp=7.108, rec=0.073, cos=0.001), tot_loss_proj:1.495 [t=0.29s]
prediction: ['[CLS] rotting underbelly [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.482 (perp=7.108, rec=0.060, cos=0.001), tot_loss_proj:1.490 [t=0.26s]
prediction: ['[CLS] rotting underbelly [SEP]']
[ 450/ 500] tot_loss=1.488 (perp=7.108, rec=0.066, cos=0.001), tot_loss_proj:1.480 [t=0.25s]
prediction: ['[CLS] rotting underbelly [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.490 (perp=7.108, rec=0.068, cos=0.001), tot_loss_proj:1.497 [t=0.25s]
prediction: ['[CLS] rotting underbelly [SEP]']
Done with input #48 of 100.
reference: 
========================
[CLS] rotting underbelly [SEP]
========================
predicted: 
========================
[CLS] rotting underbelly [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 81.048 | p: 80.897 | r: 81.455
rouge2     | fm: 45.268 | p: 45.048 | r: 45.650
rougeL     | fm: 69.633 | p: 69.465 | r: 69.942
rougeLsum  | fm: 69.630 | p: 69.509 | r: 69.924
r1fm+r2fm = 126.316

input #48 time: 0:03:27 | total time: 2:53:43


Running input #49 of 100.
reference: 
========================
could possibly be more contemptuous of the single female population . 
========================
*********************************
*********************************
average of cosine similarity 0.9992285770800434
highest_index [0]
highest [0.9992285770800434]
Debug: ids_shape = 14, pads = [14]
Debug: input ids = tensor([[  101,  2071,  4298,  2022,  2062, 17152,  8918,  1997,  1996,  2309,
          2931,  2313,  1012,   102]], device='cuda:0')
Debug: ref = ['[CLS] could possibly be more contemptuous of the single female population. [SEP]']
[Init] best rec loss: 1.508844017982483 for ['[CLS] torture correctly situation tracks license romano records jones full dylanos study [SEP]']
[Init] best rec loss: 1.4430917501449585 for ['[CLS] man players te wan time ever personnel killer extra raja race roll [SEP]']
[Init] best rec loss: 1.4197598695755005 for ['[CLS] chargesbered harsh today dion craftsuvenssen tool break backup guest [SEP]']
[Init] best rec loss: 1.3730542659759521 for ['[CLS] evolved [SEP] armed desert silence rugby peering officers down did society quarter [SEP]']
[Init] best rec loss: 1.3694813251495361 for ['[CLS] in before car lend only surprise securities radiation following montagu turkishpers [SEP]']
[Init] best rec loss: 1.316090703010559 for ['[CLS] chair assured dick fine chance household every expect boat couple freestyleerly [SEP]']
[Init] best rec loss: 1.3142051696777344 for ['[CLS] pyrenees answered bowling riding chloe minus bo language attentionocating nataya [SEP]']
[Init] best rec loss: 1.313950538635254 for ['[CLS] votesify dawn longingo destructive stop fortxious branchperationħ [SEP]']
[Init] best rec loss: 1.296747088432312 for ['[CLS] awareness sort domestic roles challenge @rifiedound optical family pass numbers [SEP]']
[Init] best rec loss: 1.296575665473938 for ['[CLS] londonrst victoria traded host dear free rided letting technicallytagram [SEP]']
[Init] best perm rec loss: 1.2936689853668213 for ['[CLS] ride technically dear free londonrsttagram victoria lettingd traded host [SEP]']
[Init] best perm rec loss: 1.292286992073059 for ['[CLS] letting victoria hosttagramrst dear ride free london tradedd technically [SEP]']
[Init] best perm rec loss: 1.2903485298156738 for ['[CLS] ride free technically hosttagramrst london victoria dear lettingd traded [SEP]']
[Init] best perm rec loss: 1.2879154682159424 for ['[CLS] technicallyrst dear host victoria traded london lettingtagram free rided [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.923 (perp=12.698, rec=0.381, cos=0.003), tot_loss_proj:3.674 [t=0.25s]
prediction: ['[CLS] activist should devil αfles inequality human afterwards matches, community double [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.784 (perp=12.657, rec=0.245, cos=0.007), tot_loss_proj:3.790 [t=0.24s]
prediction: ['[CLS] afterwards couldtious☉ sheila could female visual worse. population contempt [SEP]']
[ 150/ 500] tot_loss=2.397 (perp=10.985, rec=0.192, cos=0.007), tot_loss_proj:3.356 [t=0.26s]
prediction: ['[CLS] twins coulduous caused. possibly ofuous more. population contempt [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.137 (perp=9.856, rec=0.166, cos=0.000), tot_loss_proj:2.964 [t=0.26s]
prediction: ['[CLS] more coulduous person. possibly ofuous derived. population contempt [SEP]']
Attempt swap
Moved token
[ 250/ 500] tot_loss=2.144 (perp=9.948, rec=0.153, cos=0.001), tot_loss_proj:2.911 [t=0.27s]
prediction: ['[CLS] more coulduous single. possibly ofuous derived population contempt the [SEP]']
[ 300/ 500] tot_loss=2.121 (perp=9.948, rec=0.131, cos=0.000), tot_loss_proj:2.912 [t=0.25s]
prediction: ['[CLS] more coulduous single. possibly ofuous derived population contempt the [SEP]']
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=1.865 (perp=8.663, rec=0.132, cos=0.000), tot_loss_proj:2.596 [t=0.35s]
prediction: ['[CLS] more femaleuous single. possibly of the derived population contemptuous [SEP]']
Attempt swap
Swapped tokens
[ 400/ 500] tot_loss=1.816 (perp=8.400, rec=0.135, cos=0.002), tot_loss_proj:2.596 [t=0.26s]
prediction: ['[CLS]uous female more single. possibly of the derived population contemptuous [SEP]']
[ 450/ 500] tot_loss=1.807 (perp=8.400, rec=0.127, cos=0.000), tot_loss_proj:2.595 [t=0.25s]
prediction: ['[CLS]uous female more single. possibly of the derived population contemptuous [SEP]']
Attempt swap
Moved sequence
[ 500/ 500] tot_loss=2.121 (perp=10.039, rec=0.112, cos=0.000), tot_loss_proj:3.044 [t=0.25s]
prediction: ['[CLS]uous female more single possibly of the of populationriated contemptuous [SEP]']
Done with input #49 of 100.
reference: 
========================
[CLS] could possibly be more contemptuous of the single female population. [SEP]
========================
predicted: 
========================
[CLS]uous female more single possibly of the of populationriated contemptuous [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 75.000 | p: 75.000 | r: 75.000
rouge2     | fm: 9.091 | p: 9.091 | r: 9.091
rougeL     | fm: 41.667 | p: 41.667 | r: 41.667
rougeLsum  | fm: 41.667 | p: 41.667 | r: 41.667
r1fm+r2fm = 84.091

[Aggregate metrics]:
rouge1     | fm: 80.904 | p: 80.786 | r: 81.325
rouge2     | fm: 44.678 | p: 44.363 | r: 45.092
rougeL     | fm: 69.208 | p: 69.095 | r: 69.468
rougeLsum  | fm: 69.058 | p: 68.924 | r: 69.437
r1fm+r2fm = 125.581

input #49 time: 0:03:28 | total time: 2:57:11


Running input #50 of 100.
reference: 
========================
what the english call ` too clever by half 
========================
*********************************
*********************************
average of cosine similarity 0.9992840964591766
highest_index [0]
highest [0.9992840964591766]
Debug: ids_shape = 11, pads = [11]
Debug: input ids = tensor([[  101,  2054,  1996,  2394,  2655,  1036,  2205, 12266,  2011,  2431,
           102]], device='cuda:0')
Debug: ref = ['[CLS] what the english call ` too clever by half [SEP]']
[Init] best rec loss: 1.7681097984313965 for ['[CLS] de malaya warlord different hay li quick anymore governing [SEP]']
[Init] best rec loss: 1.5493779182434082 for ['[CLS] young division operator earliest kabul maud trail kay bra [SEP]']
[Init] best rec loss: 1.5217300653457642 for ['[CLS] garbage well delegationaround slow songs j spoke into [SEP]']
[Init] best rec loss: 1.4632673263549805 for ['[CLS] redieving by crisislent nightham bridget accordance [SEP]']
[Init] best rec loss: 1.4029443264007568 for ['[CLS] rama fueled sq napkinok unit trust associated gall [SEP]']
[Init] best perm rec loss: 1.401523470878601 for ['[CLS] associated fueled trustok rama sq napkin gall unit [SEP]']
[Init] best perm rec loss: 1.40097177028656 for ['[CLS]ok gall napkin associated fueled sq trust unit rama [SEP]']
[Init] best perm rec loss: 1.3933467864990234 for ['[CLS] gall sq napkin trust fueled unit ramaok associated [SEP]']
[Init] best perm rec loss: 1.3920629024505615 for ['[CLS] napkinok trust unit gall fueled rama sq associated [SEP]']
[Init] best perm rec loss: 1.3910012245178223 for ['[CLS] unit gall associated sq fueled napkinok rama trust [SEP]']
[Init] best perm rec loss: 1.3904179334640503 for ['[CLS] trust sq unit gallok associated napkin rama fueled [SEP]']
[Init] best perm rec loss: 1.390091061592102 for ['[CLS] gall napkin unit fueled associated sqok trust rama [SEP]']
[Init] best perm rec loss: 1.3895784616470337 for ['[CLS] unit gall napkin associated sq trustok fueled rama [SEP]']
[Init] best perm rec loss: 1.3889405727386475 for ['[CLS] unit gall trust fueledok sq associated napkin rama [SEP]']
[Init] best perm rec loss: 1.3880995512008667 for ['[CLS] fueled gall napkin unit sq trustok associated rama [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.676 (perp=10.948, rec=0.480, cos=0.006), tot_loss_proj:3.804 [t=0.29s]
prediction: ["[CLS] fee'serious award catch paying asking award desperate [SEP]"]
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.853 (perp=12.268, rec=0.398, cos=0.001), tot_loss_proj:3.925 [t=0.28s]
prediction: ['[CLS] something [ serious award regarded decay backwards reforms catch [SEP]']
[ 150/ 500] tot_loss=3.179 (perp=14.111, rec=0.356, cos=0.001), tot_loss_proj:3.814 [t=0.28s]
prediction: ['[CLS] what ` serious award clever susannah half reforms jade [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.739 (perp=12.083, rec=0.321, cos=0.001), tot_loss_proj:3.383 [t=0.30s]
prediction: ['[CLS] what ` serious half clever susannah english half jade [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=3.248 (perp=14.053, rec=0.431, cos=0.007), tot_loss_proj:3.989 [t=0.30s]
prediction: ['[CLS] what hates amusing national clever ` english half halfway [SEP]']
[ 300/ 500] tot_loss=2.724 (perp=11.912, rec=0.341, cos=0.001), tot_loss_proj:3.492 [t=0.30s]
prediction: ['[CLS] what compared clever american clever ` english half halfway [SEP]']
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=2.482 (perp=10.803, rec=0.321, cos=0.001), tot_loss_proj:3.122 [t=0.29s]
prediction: ['[CLS] what half clever american clever ` english compared halfway [SEP]']
Attempt swap
Moved sequence
[ 400/ 500] tot_loss=2.359 (perp=10.308, rec=0.296, cos=0.001), tot_loss_proj:2.946 [t=0.29s]
prediction: ['[CLS] what half clever english clever compared ` english halfway [SEP]']
[ 450/ 500] tot_loss=2.333 (perp=10.308, rec=0.271, cos=0.000), tot_loss_proj:2.946 [t=0.29s]
prediction: ['[CLS] what half clever english clever compared ` english halfway [SEP]']
Attempt swap
[ 500/ 500] tot_loss=2.459 (perp=11.074, rec=0.244, cos=0.000), tot_loss_proj:3.126 [t=0.29s]
prediction: ['[CLS] what half clever english clever too ` english halfway [SEP]']
Done with input #50 of 100.
reference: 
========================
[CLS] what the english call ` too clever by half [SEP]
========================
predicted: 
========================
[CLS] what half clever english clever too ` english halfway [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 70.000 | p: 70.000 | r: 70.000
rouge2     | fm: 11.111 | p: 11.111 | r: 11.111
rougeL     | fm: 50.000 | p: 50.000 | r: 50.000
rougeLsum  | fm: 50.000 | p: 50.000 | r: 50.000
r1fm+r2fm = 81.111

[Aggregate metrics]:
rouge1     | fm: 80.636 | p: 80.565 | r: 81.060
rouge2     | fm: 44.246 | p: 43.908 | r: 44.600
rougeL     | fm: 68.715 | p: 68.664 | r: 68.959
rougeLsum  | fm: 68.715 | p: 68.549 | r: 69.003
r1fm+r2fm = 124.882

input #50 time: 0:03:54 | total time: 3:01:06


Running input #51 of 100.
reference: 
========================
sucks , but has a funny moment or two . 
========================
*********************************
*********************************
average of cosine similarity 0.9992548315433465
highest_index [0]
highest [0.9992548315433465]
Debug: ids_shape = 12, pads = [12]
Debug: input ids = tensor([[  101, 19237,  1010,  2021,  2038,  1037,  6057,  2617,  2030,  2048,
          1012,   102]], device='cuda:0')
Debug: ref = ['[CLS] sucks, but has a funny moment or two. [SEP]']
[Init] best rec loss: 1.5581773519515991 for ['[CLS] professor teenager a rooney often ass travel [MASK] tideid [SEP]']
[Init] best rec loss: 1.3123705387115479 for ['[CLS] wickets welcomed unto arnold centuries sirens conductor english face eve [SEP]']
[Init] best rec loss: 1.2994669675827026 for ['[CLS] link bullshitw couldn reid bbc took e frustration in [SEP]']
[Init] best rec loss: 1.2463165521621704 for ['[CLS] nationals offense - vaguely world justine domesticished majorage [SEP]']
[Init] best rec loss: 1.2434269189834595 for ["[CLS] rested judo 'wig admitted matt knowledgeni heard gee [SEP]"]
[Init] best rec loss: 1.1884088516235352 for ['[CLS] flight sync breathework - faintlyase wild ownershipki [SEP]']
[Init] best rec loss: 1.1509753465652466 for ['[CLS] include plants hole abuse especially multiple fingers & since accepting [SEP]']
[Init] best perm rec loss: 1.1506011486053467 for ['[CLS] accepting hole multiple since abuse especially plants & include fingers [SEP]']
[Init] best perm rec loss: 1.1483800411224365 for ['[CLS] multiple accepting hole especially include since & fingers abuse plants [SEP]']
[Init] best perm rec loss: 1.1480402946472168 for ['[CLS] accepting plants include especially since multiple & fingers hole abuse [SEP]']
[Init] best perm rec loss: 1.146215558052063 for ['[CLS] multiple especially accepting since abuse & hole fingers plants include [SEP]']
[Init] best perm rec loss: 1.1452608108520508 for ['[CLS] multiple especially accepting since & plants hole fingers include abuse [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=3.076 (perp=13.177, rec=0.431, cos=0.009), tot_loss_proj:4.176 [t=0.30s]
prediction: ['[CLS] slow fu weak suddenly sucks tail creators correct sucked failed [SEP]']
Attempt swap
Moved sequence
[ 100/ 500] tot_loss=2.702 (perp=11.693, rec=0.361, cos=0.002), tot_loss_proj:3.290 [t=0.30s]
prediction: ['[CLS] surprise weakulsion naturally sucks tail history funny sucked. [SEP]']
[ 150/ 500] tot_loss=2.568 (perp=11.321, rec=0.302, cos=0.002), tot_loss_proj:3.947 [t=0.27s]
prediction: ['[CLS] romance unfortunately because occurs sucks without sets funny sucks. [SEP]']
Attempt swap
Moved token
[ 200/ 500] tot_loss=2.485 (perp=11.134, rec=0.258, cos=0.001), tot_loss_proj:3.268 [t=0.26s]
prediction: ['[CLS] sucks because funny occurs sucks without sets closely sucks. [SEP]']
Attempt swap
Moved token
[ 250/ 500] tot_loss=2.600 (perp=11.477, rec=0.299, cos=0.006), tot_loss_proj:3.405 [t=0.26s]
prediction: ['[CLS] sucks although funny funny occurs. has photo sucks but [SEP]']
[ 300/ 500] tot_loss=2.203 (perp=10.004, rec=0.201, cos=0.001), tot_loss_proj:3.030 [t=0.26s]
prediction: ['[CLS] sucks. funny funny or moment has starts sucks but [SEP]']
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=1.819 (perp=8.256, rec=0.168, cos=0.001), tot_loss_proj:2.777 [t=0.25s]
prediction: ['[CLS] sucks. funny or funny moment has a sucks but [SEP]']
Attempt swap
Swapped tokens
[ 400/ 500] tot_loss=1.648 (perp=7.547, rec=0.137, cos=0.001), tot_loss_proj:2.458 [t=0.27s]
prediction: ['[CLS] sucks. funny or funny sucks has a moment but [SEP]']
[ 450/ 500] tot_loss=1.635 (perp=7.547, rec=0.125, cos=0.001), tot_loss_proj:2.462 [t=0.25s]
prediction: ['[CLS] sucks. funny or funny sucks has a moment but [SEP]']
Attempt swap
Moved sequence
[ 500/ 500] tot_loss=1.605 (perp=7.433, rec=0.118, cos=0.000), tot_loss_proj:2.333 [t=0.25s]
prediction: ['[CLS] sucks. funny or sucks has a funny moment but [SEP]']
Done with input #51 of 100.
reference: 
========================
[CLS] sucks, but has a funny moment or two. [SEP]
========================
predicted: 
========================
[CLS] sucks. funny or sucks has a funny moment but [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 85.714 | p: 81.818 | r: 90.000
rouge2     | fm: 42.105 | p: 40.000 | r: 44.444
rougeL     | fm: 66.667 | p: 63.636 | r: 70.000
rougeLsum  | fm: 66.667 | p: 63.636 | r: 70.000
r1fm+r2fm = 127.820

[Aggregate metrics]:
rouge1     | fm: 80.887 | p: 80.648 | r: 81.344
rouge2     | fm: 44.035 | p: 43.701 | r: 44.385
rougeL     | fm: 68.597 | p: 68.416 | r: 68.947
rougeLsum  | fm: 68.551 | p: 68.346 | r: 68.965
r1fm+r2fm = 124.921

input #51 time: 0:03:40 | total time: 3:04:46


Running input #52 of 100.
reference: 
========================
trailer-trash 
========================
*********************************
*********************************
average of cosine similarity 0.9992769471396808
highest_index [0]
highest [0.9992769471396808]
Debug: ids_shape = 5, pads = [5]
Debug: input ids = tensor([[  101,  9117,  1011, 11669,   102]], device='cuda:0')
Debug: ref = ['[CLS] trailer - trash [SEP]']
[Init] best rec loss: 1.9447109699249268 for ['[CLS] onto cells country [SEP]']
[Init] best rec loss: 1.8204642534255981 for ['[CLS]nae part turk [SEP]']
[Init] best rec loss: 1.6631529331207275 for ['[CLS] federally by these [SEP]']
[Init] best rec loss: 1.606566071510315 for ['[CLS] field darkedge [SEP]']
[Init] best rec loss: 1.5411300659179688 for ['[CLS] treated death straw [SEP]']
[Init] best rec loss: 1.0370672941207886 for ['[CLS] token ghetto tree [SEP]']
[Init] best rec loss: 0.9874905347824097 for ['[CLS] vocabulary football expected [SEP]']
[Init] best perm rec loss: 0.9856306314468384 for ['[CLS] expected football vocabulary [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.391 (perp=10.789, rec=0.228, cos=0.005), tot_loss_proj:2.555 [t=0.28s]
prediction: ['[CLS] trash trailer trash [SEP]']
Attempt swap
Moved token
[ 100/ 500] tot_loss=2.521 (perp=11.737, rec=0.171, cos=0.003), tot_loss_proj:2.675 [t=0.29s]
prediction: ['[CLS] trailer trash trash [SEP]']
[ 150/ 500] tot_loss=2.471 (perp=11.737, rec=0.123, cos=0.001), tot_loss_proj:2.677 [t=0.28s]
prediction: ['[CLS] trailer trash trash [SEP]']
Attempt swap
Moved token
[ 200/ 500] tot_loss=2.043 (perp=9.660, rec=0.110, cos=0.001), tot_loss_proj:2.373 [t=0.29s]
prediction: ['[CLS] trash trash trailer [SEP]']
Attempt swap
[ 250/ 500] tot_loss=2.069 (perp=9.955, rec=0.078, cos=0.001), tot_loss_proj:2.345 [t=0.28s]
prediction: ['[CLS] trash - trailer [SEP]']
[ 300/ 500] tot_loss=2.055 (perp=9.955, rec=0.063, cos=0.001), tot_loss_proj:2.349 [t=0.30s]
prediction: ['[CLS] trash - trailer [SEP]']
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=1.758 (perp=8.482, rec=0.061, cos=0.001), tot_loss_proj:2.114 [t=0.29s]
prediction: ['[CLS] trash trailer - [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.770 (perp=8.482, rec=0.073, cos=0.001), tot_loss_proj:2.115 [t=0.29s]
prediction: ['[CLS] trash trailer - [SEP]']
[ 450/ 500] tot_loss=1.764 (perp=8.482, rec=0.067, cos=0.001), tot_loss_proj:2.120 [t=0.29s]
prediction: ['[CLS] trash trailer - [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.767 (perp=8.482, rec=0.070, cos=0.001), tot_loss_proj:2.121 [t=0.30s]
prediction: ['[CLS] trash trailer - [SEP]']
Done with input #52 of 100.
reference: 
========================
[CLS] trailer - trash [SEP]
========================
predicted: 
========================
[CLS] trash trailer - [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 0.000 | p: 0.000 | r: 0.000
rougeL     | fm: 75.000 | p: 75.000 | r: 75.000
rougeLsum  | fm: 75.000 | p: 75.000 | r: 75.000
r1fm+r2fm = 100.000

[Aggregate metrics]:
rouge1     | fm: 81.219 | p: 81.004 | r: 81.632
rouge2     | fm: 43.142 | p: 42.899 | r: 43.500
rougeL     | fm: 68.857 | p: 68.674 | r: 69.204
rougeLsum  | fm: 68.581 | p: 68.499 | r: 68.974
r1fm+r2fm = 124.361

input #52 time: 0:03:54 | total time: 3:08:41


Running input #53 of 100.
reference: 
========================
flinching 
========================
*********************************
*********************************
average of cosine similarity 0.9993462296030828
highest_index [0]
highest [0.9993462296030828]
Debug: ids_shape = 4, pads = [4]
Debug: input ids = tensor([[  101, 23224,  2075,   102]], device='cuda:0')
Debug: ref = ['[CLS] flinching [SEP]']
[Init] best rec loss: 1.7769790887832642 for ['[CLS] fixture trust [SEP]']
[Init] best rec loss: 1.7505303621292114 for ['[CLS] pledge se [SEP]']
[Init] best rec loss: 1.7399649620056152 for ['[CLS] government cf [SEP]']
[Init] best rec loss: 1.6215876340866089 for ['[CLS]gens maybe [SEP]']
[Init] best rec loss: 1.6025766134262085 for ['[CLS] manga rise [SEP]']
[Init] best rec loss: 1.2801052331924438 for ['[CLS] zone top [SEP]']
[Init] best rec loss: 1.2687050104141235 for ['[CLS]real hot [SEP]']
[Init] best rec loss: 1.1997308731079102 for ['[CLS] ralph not [SEP]']
[Init] best rec loss: 1.1960270404815674 for ['[CLS] university rock [SEP]']
[Init] best perm rec loss: 1.192598581314087 for ['[CLS] rock university [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=3.141 (perp=13.720, rec=0.380, cos=0.016), tot_loss_proj:4.194 [t=0.24s]
prediction: ['[CLS] asidelash [SEP]']
Attempt swap
[ 100/ 500] tot_loss=2.717 (perp=12.492, rec=0.214, cos=0.004), tot_loss_proj:3.339 [t=0.26s]
prediction: ['[CLS] flinch flinch [SEP]']
[ 150/ 500] tot_loss=2.668 (perp=12.492, rec=0.167, cos=0.002), tot_loss_proj:3.340 [t=0.25s]
prediction: ['[CLS] flinch flinch [SEP]']
Attempt swap
[ 200/ 500] tot_loss=2.647 (perp=12.492, rec=0.147, cos=0.002), tot_loss_proj:3.341 [t=0.26s]
prediction: ['[CLS] flinch flinch [SEP]']
Attempt swap
[ 250/ 500] tot_loss=2.642 (perp=12.492, rec=0.142, cos=0.002), tot_loss_proj:3.334 [t=0.26s]
prediction: ['[CLS] flinch flinch [SEP]']
[ 300/ 500] tot_loss=2.667 (perp=12.492, rec=0.161, cos=0.007), tot_loss_proj:3.339 [t=0.25s]
prediction: ['[CLS] flinch flinch [SEP]']
Attempt swap
[ 350/ 500] tot_loss=2.624 (perp=12.492, rec=0.124, cos=0.002), tot_loss_proj:3.359 [t=0.26s]
prediction: ['[CLS] flinch flinch [SEP]']
Attempt swap
[ 400/ 500] tot_loss=2.623 (perp=12.492, rec=0.123, cos=0.002), tot_loss_proj:3.347 [t=0.25s]
prediction: ['[CLS] flinch flinch [SEP]']
[ 450/ 500] tot_loss=2.613 (perp=12.492, rec=0.113, cos=0.002), tot_loss_proj:3.346 [t=0.25s]
prediction: ['[CLS] flinch flinch [SEP]']
Attempt swap
[ 500/ 500] tot_loss=2.575 (perp=12.413, rec=0.092, cos=0.001), tot_loss_proj:3.310 [t=0.25s]
prediction: ['[CLS]ing flinch [SEP]']
Done with input #53 of 100.
reference: 
========================
[CLS] flinching [SEP]
========================
predicted: 
========================
[CLS]ing flinch [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 57.143 | p: 50.000 | r: 66.667
rouge2     | fm: 0.000 | p: 0.000 | r: 0.000
rougeL     | fm: 57.143 | p: 50.000 | r: 66.667
rougeLsum  | fm: 57.143 | p: 50.000 | r: 66.667
r1fm+r2fm = 57.143

[Aggregate metrics]:
rouge1     | fm: 80.769 | p: 80.405 | r: 81.361
rouge2     | fm: 42.234 | p: 41.961 | r: 42.616
rougeL     | fm: 68.611 | p: 68.317 | r: 69.041
rougeLsum  | fm: 68.420 | p: 68.094 | r: 68.974
r1fm+r2fm = 123.003

input #53 time: 0:03:27 | total time: 3:12:08


Running input #54 of 100.
reference: 
========================
hot topics 
========================
*********************************
*********************************
average of cosine similarity 0.9991885466091153
highest_index [0]
highest [0.9991885466091153]
Debug: ids_shape = 4, pads = [4]
Debug: input ids = tensor([[ 101, 2980, 7832,  102]], device='cuda:0')
Debug: ref = ['[CLS] hot topics [SEP]']
[Init] best rec loss: 1.7071905136108398 for ['[CLS] bill background [SEP]']
[Init] best rec loss: 1.552504062652588 for ['[CLS] living devices [SEP]']
[Init] best rec loss: 1.5421050786972046 for ['[CLS] trinity passed [SEP]']
[Init] best rec loss: 1.5026865005493164 for ['[CLS] solutions on [SEP]']
[Init] best rec loss: 1.2422250509262085 for ['[CLS] soon anxious [SEP]']
[Init] best rec loss: 1.139829158782959 for ['[CLS] delivery content [SEP]']
[Init] best rec loss: 1.0659000873565674 for ['[CLS] deployment bro [SEP]']
[Init] best rec loss: 0.9896990060806274 for ['[CLS] wild exercised [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=1.989 (perp=8.198, rec=0.332, cos=0.018), tot_loss_proj:1.761 [t=0.26s]
prediction: ['[CLS] hot topics [SEP]']
Attempt swap
[ 100/ 500] tot_loss=1.802 (perp=8.198, rec=0.152, cos=0.011), tot_loss_proj:1.747 [t=0.28s]
prediction: ['[CLS] hot topics [SEP]']
[ 150/ 500] tot_loss=1.739 (perp=8.198, rec=0.098, cos=0.002), tot_loss_proj:1.734 [t=0.25s]
prediction: ['[CLS] hot topics [SEP]']
Attempt swap
[ 200/ 500] tot_loss=1.718 (perp=8.198, rec=0.078, cos=0.001), tot_loss_proj:1.739 [t=0.25s]
prediction: ['[CLS] hot topics [SEP]']
Attempt swap
[ 250/ 500] tot_loss=1.707 (perp=8.198, rec=0.066, cos=0.001), tot_loss_proj:1.730 [t=0.34s]
prediction: ['[CLS] hot topics [SEP]']
[ 300/ 500] tot_loss=1.714 (perp=8.198, rec=0.073, cos=0.002), tot_loss_proj:1.731 [t=0.27s]
prediction: ['[CLS] hot topics [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.705 (perp=8.198, rec=0.065, cos=0.001), tot_loss_proj:1.727 [t=0.27s]
prediction: ['[CLS] hot topics [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.699 (perp=8.198, rec=0.058, cos=0.002), tot_loss_proj:1.728 [t=0.25s]
prediction: ['[CLS] hot topics [SEP]']
[ 450/ 500] tot_loss=1.694 (perp=8.198, rec=0.053, cos=0.001), tot_loss_proj:1.736 [t=0.27s]
prediction: ['[CLS] hot topics [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.702 (perp=8.198, rec=0.061, cos=0.001), tot_loss_proj:1.721 [t=0.26s]
prediction: ['[CLS] hot topics [SEP]']
Done with input #54 of 100.
reference: 
========================
[CLS] hot topics [SEP]
========================
predicted: 
========================
[CLS] hot topics [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 81.034 | p: 80.663 | r: 81.682
rouge2     | fm: 43.453 | p: 43.174 | r: 43.807
rougeL     | fm: 69.194 | p: 68.982 | r: 69.663
rougeLsum  | fm: 69.031 | p: 68.784 | r: 69.562
r1fm+r2fm = 124.487

input #54 time: 0:03:28 | total time: 3:15:36


Running input #55 of 100.
reference: 
========================
settles too easily 
========================
*********************************
*********************************
average of cosine similarity 0.9991205863745831
highest_index [0]
highest [0.9991205863745831]
Debug: ids_shape = 5, pads = [5]
Debug: input ids = tensor([[  101, 27221,  2205,  4089,   102]], device='cuda:0')
Debug: ref = ['[CLS] settles too easily [SEP]']
[Init] best rec loss: 1.8857332468032837 for ['[CLS] records althoughhony [SEP]']
[Init] best rec loss: 1.5969682931900024 for ['[CLS] due tired letters [SEP]']
[Init] best rec loss: 1.32254958152771 for ['[CLS] are martha erin [SEP]']
[Init] best rec loss: 1.3047670125961304 for ['[CLS]ies finished haired [SEP]']
[Init] best rec loss: 1.264054298400879 for ['[CLS] stride holly post [SEP]']
[Init] best rec loss: 1.2127386331558228 for ['[CLS] precipitation written mounted [SEP]']
[Init] best perm rec loss: 1.2090678215026855 for ['[CLS] written precipitation mounted [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.881 (perp=11.227, rec=0.584, cos=0.052), tot_loss_proj:3.489 [t=0.29s]
prediction: ['[CLS] closed dry easy [SEP]']
Attempt swap
Moved token
[ 100/ 500] tot_loss=2.955 (perp=12.522, rec=0.439, cos=0.012), tot_loss_proj:3.880 [t=0.25s]
prediction: ['[CLS] dry relegated easy [SEP]']
[ 150/ 500] tot_loss=2.798 (perp=12.239, rec=0.344, cos=0.006), tot_loss_proj:4.320 [t=0.25s]
prediction: ['[CLS] evenolved easily [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.091 (perp=8.809, rec=0.323, cos=0.005), tot_loss_proj:3.213 [t=0.26s]
prediction: ['[CLS] too easily goes [SEP]']
Attempt swap
Put prefix at the end
[ 250/ 500] tot_loss=1.907 (perp=8.083, rec=0.285, cos=0.006), tot_loss_proj:2.864 [t=0.25s]
prediction: ['[CLS] goes too easily [SEP]']
[ 300/ 500] tot_loss=2.001 (perp=8.924, rec=0.211, cos=0.005), tot_loss_proj:2.117 [t=0.25s]
prediction: ['[CLS] settled too easily [SEP]']
Attempt swap
Put prefix at the end
[ 350/ 500] tot_loss=1.692 (perp=7.752, rec=0.140, cos=0.002), tot_loss_proj:2.476 [t=0.26s]
prediction: ['[CLS] too easily settled [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.845 (perp=8.687, rec=0.107, cos=0.001), tot_loss_proj:2.261 [t=0.26s]
prediction: ['[CLS] too easily settles [SEP]']
[ 450/ 500] tot_loss=1.850 (perp=8.687, rec=0.112, cos=0.001), tot_loss_proj:2.250 [t=0.26s]
prediction: ['[CLS] too easily settles [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.829 (perp=8.687, rec=0.091, cos=0.000), tot_loss_proj:2.252 [t=0.26s]
prediction: ['[CLS] too easily settles [SEP]']
Done with input #55 of 100.
reference: 
========================
[CLS] settles too easily [SEP]
========================
predicted: 
========================
[CLS] too easily settles [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 25.000 | p: 25.000 | r: 25.000
rougeL     | fm: 80.000 | p: 80.000 | r: 80.000
rougeLsum  | fm: 80.000 | p: 80.000 | r: 80.000
r1fm+r2fm = 125.000

[Aggregate metrics]:
rouge1     | fm: 81.390 | p: 81.116 | r: 81.983
rouge2     | fm: 43.255 | p: 43.031 | r: 43.612
rougeL     | fm: 69.306 | p: 69.187 | r: 69.854
rougeLsum  | fm: 69.260 | p: 68.999 | r: 69.719
r1fm+r2fm = 124.645

input #55 time: 0:03:29 | total time: 3:19:06


Running input #56 of 100.
reference: 
========================
films which will cause loads of irreparable damage that years and years of costly analysis could never fix 
========================
*********************************
*********************************
average of cosine similarity 0.9992743912224644
highest_index [0]
highest [0.9992743912224644]
Debug: ids_shape = 23, pads = [23]
Debug: input ids = tensor([[  101,  3152,  2029,  2097,  3426, 15665,  1997, 20868,  2890, 28689,
          3468,  4053,  2008,  2086,  1998,  2086,  1997, 17047,  4106,  2071,
          2196,  8081,   102]], device='cuda:0')
Debug: ref = ['[CLS] films which will cause loads of irreparable damage that years and years of costly analysis could never fix [SEP]']
[Init] best rec loss: 1.7055118083953857 for ['[CLS] rotting pleading victim hug signed lose tongue rebuilt biocky may listen voltage stein burnt districtde despite course shown [SEP]']
[Init] best rec loss: 1.6200506687164307 for ['[CLS] component sequence authority at language bit bottom now cross dominant offer. % setrated familyride ex into told hedge [SEP]']
[Init] best rec loss: 1.5344394445419312 for ['[CLS] press passhunorescence ellenot eveviritan conditioning sale past fabric lines plenty parentsstick? family need us [SEP]']
[Init] best rec loss: 1.4744815826416016 for ['[CLS] code laid sense strike determined iron depression charter bear technique avidured blame ; en unfortunately backed sympathy tis reflection k [SEP]']
[Init] best perm rec loss: 1.4654475450515747 for ['[CLS] sense en depression avid laid ironured bear k strike blame reflection technique determined backed unfortunately ; sympathy charter code tis [SEP]']
[Init] best perm rec loss: 1.456282377243042 for ['[CLS] strike charter unfortunately en backed reflection code laid tis blame k determined depression technique sense bear sympathy ironured ; avid [SEP]']
[Init] best perm rec loss: 1.4561269283294678 for ['[CLS] technique charter backed ; iron avid code sympathy sense bear determined laid en reflection depression tis blame k unfortunately strikeured [SEP]']
[Init] best perm rec loss: 1.4547836780548096 for ['[CLS] technique charter en tis kured code ; determined blame sympathy reflection sense unfortunately depression bear iron laid backed avid strike [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.728 (perp=11.824, rec=0.361, cos=0.003), tot_loss_proj:3.142 [t=0.26s]
prediction: ['[CLS] victim damage ; when arabia alleged assault settlers suchg causedx any very miss waste stupid waste inc journalism lawsuit [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.728 (perp=12.164, rec=0.292, cos=0.002), tot_loss_proj:3.840 [t=0.28s]
prediction: ['[CLS] films damage necessarily when could alleged assault ے incgizing miss no incurred very caused waste damage plenty gathered films issues [SEP]']
[ 150/ 500] tot_loss=2.599 (perp=11.879, rec=0.222, cos=0.001), tot_loss_proj:3.331 [t=0.27s]
prediction: ['[CLS] films damage necessarily that films which fix fix toopara costlyge any load caused costly costly plenty gathered films issues [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.361 (perp=10.882, rec=0.184, cos=0.001), tot_loss_proj:2.908 [t=0.25s]
prediction: ['[CLS] films damage will that films which fix fix toopara loadsco years load caused costly costly costly developed films issues [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=2.263 (perp=10.551, rec=0.152, cos=0.001), tot_loss_proj:2.866 [t=0.27s]
prediction: ['[CLS] films damage will that films which could fix neverpara loadsco years very caused costly costly costly caused films issues [SEP]']
[ 300/ 500] tot_loss=1.978 (perp=9.283, rec=0.121, cos=0.000), tot_loss_proj:2.708 [t=0.28s]
prediction: ['[CLS] films damage will that films which could fix neverpara loads could years of damage costly costly analysis cause films analysis [SEP]']
Attempt swap
Moved token
[ 350/ 500] tot_loss=2.210 (perp=10.477, rec=0.114, cos=0.001), tot_loss_proj:2.843 [t=0.26s]
prediction: ['[CLS] films damage will that films which could never fixpara loads could years years years costly costly analysis cause films analysis [SEP]']
Attempt swap
Swapped tokens
[ 400/ 500] tot_loss=2.133 (perp=10.156, rec=0.101, cos=0.001), tot_loss_proj:3.151 [t=0.28s]
prediction: ['[CLS] films films will that films which could never fixpara loads could years years years costly costly analysis cause damage analysis [SEP]']
[ 450/ 500] tot_loss=2.089 (perp=9.926, rec=0.103, cos=0.000), tot_loss_proj:2.934 [t=0.34s]
prediction: ['[CLS] of films will that films which could never fixpara loads could years yearsble costly costly analysis cause damage analysis [SEP]']
Attempt swap
Swapped tokens
[ 500/ 500] tot_loss=1.914 (perp=9.098, rec=0.094, cos=0.000), tot_loss_proj:3.169 [t=0.29s]
prediction: ['[CLS]ble films will that films which could never fixpara loads could years years of costly costly analysis cause damage analysis [SEP]']
Done with input #56 of 100.
reference: 
========================
[CLS] films which will cause loads of irreparable damage that years and years of costly analysis could never fix [SEP]
========================
predicted: 
========================
[CLS]ble films will that films which could never fixpara loads could years years of costly costly analysis cause damage analysis [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 76.190 | p: 72.727 | r: 80.000
rouge2     | fm: 25.000 | p: 23.810 | r: 26.316
rougeL     | fm: 47.619 | p: 45.455 | r: 50.000
rougeLsum  | fm: 47.619 | p: 45.455 | r: 50.000
r1fm+r2fm = 101.190

[Aggregate metrics]:
rouge1     | fm: 81.396 | p: 80.971 | r: 82.030
rouge2     | fm: 42.709 | p: 42.394 | r: 43.110
rougeL     | fm: 69.091 | p: 68.735 | r: 69.618
rougeLsum  | fm: 68.898 | p: 68.578 | r: 69.386
r1fm+r2fm = 124.105

input #56 time: 0:03:29 | total time: 3:22:35


Running input #57 of 100.
reference: 
========================
wears 
========================
*********************************
*********************************
average of cosine similarity 0.9993815950585869
highest_index [0]
highest [0.9993815950585869]
Debug: ids_shape = 3, pads = [3]
Debug: input ids = tensor([[  101, 11651,   102]], device='cuda:0')
Debug: ref = ['[CLS] wears [SEP]']
[Init] best rec loss: 1.7954795360565186 for ['[CLS]ne [SEP]']
[Init] best rec loss: 1.633437156677246 for ['[CLS] software [SEP]']
[Init] best rec loss: 1.5058315992355347 for ['[CLS] passed [SEP]']
[Init] best rec loss: 1.4965696334838867 for ['[CLS]cta [SEP]']
[Init] best rec loss: 1.4871668815612793 for ['[CLS] modern [SEP]']
[Init] best rec loss: 1.4436358213424683 for ['[CLS] bundesliga [SEP]']
[Init] best rec loss: 1.4208097457885742 for ['[CLS] thanks [SEP]']
[Init] best rec loss: 1.4164838790893555 for ['[CLS] decision [SEP]']
[Init] best rec loss: 1.390770435333252 for ['[CLS]ering [SEP]']
[Init] best rec loss: 1.3783762454986572 for ['[CLS] dorm [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.434 (perp=8.573, rec=0.538, cos=0.181), tot_loss_proj:3.485 [t=0.25s]
prediction: ['[CLS] graduate [SEP]']
Attempt swap
[ 100/ 500] tot_loss=2.673 (perp=12.282, rec=0.206, cos=0.011), tot_loss_proj:2.522 [t=0.24s]
prediction: ['[CLS] wears [SEP]']
[ 150/ 500] tot_loss=2.537 (perp=12.282, rec=0.078, cos=0.003), tot_loss_proj:2.523 [t=0.26s]
prediction: ['[CLS] wears [SEP]']
Attempt swap
[ 200/ 500] tot_loss=2.525 (perp=12.282, rec=0.065, cos=0.004), tot_loss_proj:2.522 [t=0.25s]
prediction: ['[CLS] wears [SEP]']
Attempt swap
[ 250/ 500] tot_loss=2.522 (perp=12.282, rec=0.065, cos=0.001), tot_loss_proj:2.532 [t=0.25s]
prediction: ['[CLS] wears [SEP]']
[ 300/ 500] tot_loss=2.523 (perp=12.282, rec=0.066, cos=0.000), tot_loss_proj:2.514 [t=0.27s]
prediction: ['[CLS] wears [SEP]']
Attempt swap
[ 350/ 500] tot_loss=2.511 (perp=12.282, rec=0.055, cos=0.001), tot_loss_proj:2.538 [t=0.25s]
prediction: ['[CLS] wears [SEP]']
Attempt swap
[ 400/ 500] tot_loss=2.512 (perp=12.282, rec=0.055, cos=0.001), tot_loss_proj:2.525 [t=0.26s]
prediction: ['[CLS] wears [SEP]']
[ 450/ 500] tot_loss=2.514 (perp=12.282, rec=0.057, cos=0.000), tot_loss_proj:2.513 [t=0.26s]
prediction: ['[CLS] wears [SEP]']
Attempt swap
[ 500/ 500] tot_loss=2.518 (perp=12.282, rec=0.061, cos=0.000), tot_loss_proj:2.505 [t=0.26s]
prediction: ['[CLS] wears [SEP]']
Done with input #57 of 100.
reference: 
========================
[CLS] wears [SEP]
========================
predicted: 
========================
[CLS] wears [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 81.593 | p: 81.214 | r: 82.267
rouge2     | fm: 43.571 | p: 43.264 | r: 43.943
rougeL     | fm: 69.563 | p: 69.312 | r: 70.085
rougeLsum  | fm: 69.342 | p: 69.062 | r: 69.856
r1fm+r2fm = 125.164

input #57 time: 0:03:27 | total time: 3:26:02


Running input #58 of 100.
reference: 
========================
is an inspirational love story , capturing the innocence and idealism of that first encounter 
========================
*********************************
*********************************
average of cosine similarity 0.9992685151622412
highest_index [0]
highest [0.9992685151622412]
Debug: ids_shape = 18, pads = [18]
Debug: input ids = tensor([[  101,  2003,  2019, 28676,  2293,  2466,  1010, 11847,  1996, 12660,
          1998,  7812,  2964,  1997,  2008,  2034,  8087,   102]],
       device='cuda:0')
Debug: ref = ['[CLS] is an inspirational love story, capturing the innocence and idealism of that first encounter [SEP]']
[Init] best rec loss: 1.8984190225601196 for ['[CLS] necessary cheated congregation nippleanian hold meetback seeking than claws bones position ever jobs time [SEP]']
[Init] best rec loss: 1.8598558902740479 for ['[CLS] thinking humming human speakers generalyal ended lowlands per willuity music hearts timing jazz toys [SEP]']
[Init] best rec loss: 1.6641579866409302 for ['[CLS] ) after rca adventures / yong beat bad http stoodagendingple ]erson trailing [SEP]']
[Init] best rec loss: 1.6362143754959106 for ['[CLS] insidethed subject layton devoted approachhead physicians keys vice tarzan mile norman warlord choosebaldi [SEP]']
[Init] best rec loss: 1.633800745010376 for ['[CLS] hissedoop whispered h delayed!lwyn [SEP] could prove atrocities backsie square shu tam [SEP]']
[Init] best rec loss: 1.5590686798095703 for ['[CLS]lusion wake case species applicationnding fluent passing bakeralo couple win used texas installed tucker [SEP]']
[Init] best perm rec loss: 1.5537745952606201 for ['[CLS] passingalo species used baker installednding tucker texas win wakelusion couple application case fluent [SEP]']
[Init] best perm rec loss: 1.5531631708145142 for ['[CLS] tucker installed used texas case bakerlusionnding wake application fluentalo couple passing species win [SEP]']
[Init] best perm rec loss: 1.5502347946166992 for ['[CLS] texas fluent used tucker bakeralolusion application case installednding passing win couple wake species [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=3.145 (perp=11.775, rec=0.781, cos=0.009), tot_loss_proj:3.852 [t=0.27s]
prediction: ['[CLS] probably stupid israeli got point been bmw our elevator ticket crater. - pile any yet [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=3.252 (perp=12.898, rec=0.669, cos=0.003), tot_loss_proj:4.099 [t=0.26s]
prediction: ['[CLS] hated stupid sometimes made point video anyple escort ticket crater committee -plebbling no [SEP]']
[ 150/ 500] tot_loss=3.085 (perp=12.246, rec=0.630, cos=0.005), tot_loss_proj:3.937 [t=0.27s]
prediction: ['[CLS] hated documentary remnants made passion in any volunteer escort remaining crater campaign morningple manner no [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=3.463 (perp=14.393, rec=0.582, cos=0.002), tot_loss_proj:4.670 [t=0.25s]
prediction: ['[CLS] goddamn conduct remnants get remaining alive any fake escort passion impose kingdom morningple assetsha [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=3.151 (perp=12.592, rec=0.606, cos=0.027), tot_loss_proj:4.218 [t=0.27s]
prediction: ['[CLS] inspirational documentary remnants manner remaining alive intensity fake escort story racial kingdom wasple get uncomfortable [SEP]']
[ 300/ 500] tot_loss=3.350 (perp=14.151, rec=0.510, cos=0.011), tot_loss_proj:4.496 [t=0.26s]
prediction: ['[CLS] inspirational documentary remnants row remaining of intensity fakesity story racial humanist an pile invasion uncomfortable [SEP]']
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=3.012 (perp=12.586, rec=0.484, cos=0.011), tot_loss_proj:4.394 [t=0.25s]
prediction: ['[CLS] inspirational documentary violin row remaining about narrative fakesity story an humanist racial pile invasion uncomfortable [SEP]']
Attempt swap
Moved token
[ 400/ 500] tot_loss=2.820 (perp=11.716, rec=0.473, cos=0.004), tot_loss_proj:4.323 [t=0.26s]
prediction: ['[CLS] inspirational documentary inspirational depicts remaining narrative fakesity story of an humanist masovian pile dictionary uncomfortable [SEP]']
[ 450/ 500] tot_loss=2.835 (perp=11.886, rec=0.452, cos=0.005), tot_loss_proj:4.183 [t=0.28s]
prediction: ['[CLS] inspirational sketch inspirational an remaining narrative prizessity story capturing an ideal accommodation remains dictionary uncomfortable [SEP]']
Attempt swap
Swapped tokens
[ 500/ 500] tot_loss=2.472 (perp=10.188, rec=0.433, cos=0.001), tot_loss_proj:4.006 [t=0.28s]
prediction: ['[CLS] remaining sketch inspirational an inspirational narrative fake religious story capturing an ideal accommodation represents encounterarable [SEP]']
Done with input #58 of 100.
reference: 
========================
[CLS] is an inspirational love story, capturing the innocence and idealism of that first encounter [SEP]
========================
predicted: 
========================
[CLS] remaining sketch inspirational an inspirational narrative fake religious story capturing an ideal accommodation represents encounterarable [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 36.364 | p: 35.294 | r: 37.500
rouge2     | fm: 12.903 | p: 12.500 | r: 13.333
rougeL     | fm: 36.364 | p: 35.294 | r: 37.500
rougeLsum  | fm: 36.364 | p: 35.294 | r: 37.500
r1fm+r2fm = 49.267

[Aggregate metrics]:
rouge1     | fm: 80.914 | p: 80.491 | r: 81.556
rouge2     | fm: 43.139 | p: 42.868 | r: 43.518
rougeL     | fm: 68.979 | p: 68.649 | r: 69.402
rougeLsum  | fm: 68.963 | p: 68.675 | r: 69.484
r1fm+r2fm = 124.053

input #58 time: 0:03:30 | total time: 3:29:33


Running input #59 of 100.
reference: 
========================
has the charisma of a young woman who knows how to hold the screen 
========================
*********************************
*********************************
average of cosine similarity 0.9992224669689527
highest_index [0]
highest [0.9992224669689527]
Debug: ids_shape = 18, pads = [18]
Debug: input ids = tensor([[  101,  2038,  1996, 25869,  2964,  2050,  1997,  1037,  2402,  2450,
          2040,  4282,  2129,  2000,  2907,  1996,  3898,   102]],
       device='cuda:0')
Debug: ref = ['[CLS] has the charisma of a young woman who knows how to hold the screen [SEP]']
[Init] best rec loss: 1.9130570888519287 for ['[CLS] zero treasurer kennedy vet when le bronze plans curiosity frigate greenbis wa ant chamber disks [SEP]']
[Init] best rec loss: 1.893306851387024 for ['[CLS]ization ul csi delegate its sex million glasses ek investigated through steep valkyrie prime wondering jordan [SEP]']
[Init] best rec loss: 1.8817111253738403 for ['[CLS] shares josephine settled commerce refrain bulletsgi moved plot awaitanies roger console fergusotidew [SEP]']
[Init] best rec loss: 1.7740832567214966 for ['[CLS] meetings primarily afar vietnamille sound explaining bun ii powerped able speaking [SEP] brow illumination [SEP]']
[Init] best rec loss: 1.75552499294281 for ['[CLS] trollsity underoh othersrion vault sorry days premiereend wivesjit reachedhold motorway [SEP]']
[Init] best rec loss: 1.6795601844787598 for ['[CLS] approaches dumb accept households frame relation sport replymis logan surrounding dutch dragon different com discipline [SEP]']
[Init] best rec loss: 1.6730873584747314 for ['[CLS] thus races noah pump awhile 1993 information bolognaby stuff temperament fleetak bobo was tunnel [SEP]']
[Init] best rec loss: 1.4757320880889893 for ['[CLS] organic passengers heroic wall duty change surgery drag kay statesflower hadn retirement cross will money [SEP]']
[Init] best perm rec loss: 1.473044991493225 for ['[CLS]flower states passengers cross drag organic money change wall hadn retirement kay heroic surgery will duty [SEP]']
[Init] best perm rec loss: 1.4710206985473633 for ['[CLS] wall passengers will states duty kay drag cross change money surgery heroicflower hadn organic retirement [SEP]']
[Init] best perm rec loss: 1.4710203409194946 for ['[CLS] hadn passengers retirement wall states organic drag change duty surgery kayflower money heroic cross will [SEP]']
[Init] best perm rec loss: 1.4642279148101807 for ['[CLS] wall organic passengers drag change surgeryflower will states retirement duty money kay cross heroic hadn [SEP]']
[Init] best perm rec loss: 1.4622763395309448 for ['[CLS] states passengers cross change organic surgeryflower kay money drag retirement wall will heroic duty hadn [SEP]']
[Init] best perm rec loss: 1.4614100456237793 for ['[CLS] hadn organic passengersflower surgery heroic cross kay will wall retirement change money states drag duty [SEP]']
[Init] best perm rec loss: 1.4608839750289917 for ['[CLS] organicflower retirement will money heroic duty hadn wall drag surgery cross passengers states change kay [SEP]']
[Init] best perm rec loss: 1.4605615139007568 for ['[CLS] organic drag passengers heroicflower cross kay surgery retirement wall will duty states money change hadn [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=3.208 (perp=14.125, rec=0.378, cos=0.004), tot_loss_proj:3.628 [t=0.25s]
prediction: ['[CLS] spiritual wonderful partner brandy regina monte portrait holder bscwife introduced kat mother spark charlie of [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.581 (perp=11.503, rec=0.273, cos=0.007), tot_loss_proj:3.932 [t=0.27s]
prediction: ['[CLS] patient young victoriawife ( actor a his screen char has once woman lee who her [SEP]']
[ 150/ 500] tot_loss=2.709 (perp=12.555, rec=0.197, cos=0.001), tot_loss_proj:4.172 [t=0.26s]
prediction: ['[CLS] spiritual screen ofwife ofka a the screenism has who woman char who her [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.130 (perp=9.841, rec=0.161, cos=0.001), tot_loss_proj:3.273 [t=0.26s]
prediction: ['[CLS] spiritual screen of screen of screen young the screenism has a woman char who her [SEP]']
Attempt swap
Moved sequence
[ 250/ 500] tot_loss=2.189 (perp=9.543, rec=0.276, cos=0.004), tot_loss_proj:3.410 [t=0.26s]
prediction: ['[CLS] spiritual screen of rhetoric char a - a the screenism has a woman who the [SEP]']
[ 300/ 500] tot_loss=2.304 (perp=10.319, rec=0.238, cos=0.002), tot_loss_proj:2.878 [t=0.26s]
prediction: ['[CLS] spiritual percussion of humour char a screen young the screenism has an woman who how [SEP]']
Attempt swap
Moved token
[ 350/ 500] tot_loss=2.305 (perp=10.497, rec=0.204, cos=0.001), tot_loss_proj:3.747 [t=0.27s]
prediction: ['[CLS]ized of tongue char a screen young the screen charism has screen woman who the [SEP]']
Attempt swap
Swapped tokens
[ 400/ 500] tot_loss=2.178 (perp=9.778, rec=0.218, cos=0.004), tot_loss_proj:3.439 [t=0.26s]
prediction: ['[CLS]ized of tongueism a screen young the screen charism the screen woman knows has [SEP]']
[ 450/ 500] tot_loss=2.287 (perp=10.493, rec=0.188, cos=0.001), tot_loss_proj:3.639 [t=0.25s]
prediction: ['[CLS]ized of tongueism a screen young the hold charism the screen woman knows has [SEP]']
Attempt swap
Moved token
[ 500/ 500] tot_loss=2.200 (perp=10.197, rec=0.160, cos=0.001), tot_loss_proj:3.212 [t=0.26s]
prediction: ['[CLS]izedism of who a screen young the hold charism the screen woman knows has [SEP]']
Done with input #59 of 100.
reference: 
========================
[CLS] has the charisma of a young woman who knows how to hold the screen [SEP]
========================
predicted: 
========================
[CLS] spiritual screen of screena screen young the screenism has a woman char who her [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 62.500 | p: 62.500 | r: 62.500
rouge2     | fm: 0.000 | p: 0.000 | r: 0.000
rougeL     | fm: 37.500 | p: 37.500 | r: 37.500
rougeLsum  | fm: 37.500 | p: 37.500 | r: 37.500
r1fm+r2fm = 62.500

[Aggregate metrics]:
rouge1     | fm: 80.594 | p: 80.182 | r: 81.274
rouge2     | fm: 42.533 | p: 42.350 | r: 42.885
rougeL     | fm: 68.426 | p: 68.161 | r: 69.006
rougeLsum  | fm: 68.271 | p: 67.963 | r: 68.792
r1fm+r2fm = 123.127

input #59 time: 0:03:28 | total time: 3:33:02


Running input #60 of 100.
reference: 
========================
circuit is the awkwardly paced soap opera-ish story . 
========================
*********************************
*********************************
average of cosine similarity 0.999371813039142
highest_index [0]
highest [0.999371813039142]
Debug: ids_shape = 14, pads = [14]
Debug: input ids = tensor([[  101,  4984,  2003,  1996, 18822, 13823,  7815,  3850,  1011,  2003,
          2232,  2466,  1012,   102]], device='cuda:0')
Debug: ref = ['[CLS] circuit is the awkwardly paced soap opera - ish story. [SEP]']
[Init] best rec loss: 1.8264330625534058 for ['[CLS] records instruments few stain lea conference searching " minor jp program abroad [SEP]']
[Init] best rec loss: 1.785794734954834 for ['[CLS] sub size practice duty sank topology pilgrims pyramid defense sc di dun [SEP]']
[Init] best rec loss: 1.5257476568222046 for ['[CLS] breath merely reception context find fake sale black duet side fitzgerald benny [SEP]']
[Init] best rec loss: 1.4993537664413452 for ['[CLS] analytics spring era jameson mortimer maddie ground! regiment how de deep [SEP]']
[Init] best rec loss: 1.494861125946045 for ['[CLS] internal begins by anything overrs mauriceorraation classroom yes josie [SEP]']
[Init] best rec loss: 1.4207831621170044 for ['[CLS] strungitude plenty majority cover through constitution /ouring upsetlbyshaw [SEP]']
[Init] best perm rec loss: 1.4200644493103027 for ['[CLS] constitutionitude upsetshaw coverlby strungouring majority plenty / through [SEP]']
[Init] best perm rec loss: 1.413394570350647 for ['[CLS] cover majority /lby constitution upset strung throughshawouringitude plenty [SEP]']
[Init] best perm rec loss: 1.4109059572219849 for ['[CLS] majority upset strungitude /ouringshawlby cover through plenty constitution [SEP]']
[Init] best perm rec loss: 1.4078900814056396 for ['[CLS] / majority strungouringshawlby through constitutionitude plenty cover upset [SEP]']
[Init] best perm rec loss: 1.4074980020523071 for ['[CLS] majority strung throughouringlbyitudeshaw / constitution upset plenty cover [SEP]']
[Init] best perm rec loss: 1.4071258306503296 for ['[CLS] plentyitude /lby strung cover upsetshaw constitution through majorityouring [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.560 (perp=11.207, rec=0.315, cos=0.004), tot_loss_proj:2.858 [t=0.25s]
prediction: ['[CLS] late awkwardly unclear snail campaign - month january hallway seat becameless [SEP]']
Attempt swap
Moved token
[ 100/ 500] tot_loss=2.499 (perp=11.281, rec=0.241, cos=0.002), tot_loss_proj:2.794 [t=0.25s]
prediction: ['[CLS] is awkwardly awkwardly opera ) / season is oven lecture hostage circuit [SEP]']
[ 150/ 500] tot_loss=2.050 (perp=9.277, rec=0.193, cos=0.002), tot_loss_proj:2.332 [t=0.26s]
prediction: ['[CLS] is awkwardly awkwardly opera paced the opera. soap opera circuit circuit [SEP]']
Attempt swap
Moved token
[ 200/ 500] tot_loss=2.109 (perp=9.699, rec=0.169, cos=0.001), tot_loss_proj:2.458 [t=0.25s]
prediction: ['[CLS] is awkwardly awkwardly soap circuit paced the story. opera circuit circuit [SEP]']
Attempt swap
Moved sequence
[ 250/ 500] tot_loss=2.036 (perp=9.347, rec=0.165, cos=0.002), tot_loss_proj:2.408 [t=0.26s]
prediction: ['[CLS] is awkwardly awkwardly soap story circuit circuit circuit paced the story. [SEP]']
[ 300/ 500] tot_loss=1.995 (perp=9.347, rec=0.125, cos=0.001), tot_loss_proj:2.411 [t=0.26s]
prediction: ['[CLS] is awkwardly awkwardly soap story circuit circuit circuit paced the story. [SEP]']
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=1.845 (perp=8.686, rec=0.107, cos=0.001), tot_loss_proj:2.302 [t=0.25s]
prediction: ['[CLS] is awkwardly awkwardly soap circuit circuit opera circuit paced the story. [SEP]']
Attempt swap
Moved token
[ 400/ 500] tot_loss=1.765 (perp=8.242, rec=0.115, cos=0.001), tot_loss_proj:2.220 [t=0.25s]
prediction: ['[CLS] is awkwardly awkwardly soap opera circuit circuit circuit paced the story. [SEP]']
[ 450/ 500] tot_loss=1.754 (perp=8.315, rec=0.091, cos=0.000), tot_loss_proj:2.182 [t=0.27s]
prediction: ['[CLS] is awkwardly awkwardly soap operah circuit circuit paced the story. [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.759 (perp=8.315, rec=0.096, cos=0.000), tot_loss_proj:2.186 [t=0.26s]
prediction: ['[CLS] is awkwardly awkwardly soap operah circuit circuit paced the story. [SEP]']
Done with input #60 of 100.
reference: 
========================
[CLS] circuit is the awkwardly paced soap opera - ish story. [SEP]
========================
predicted: 
========================
[CLS] is awkwardly awkwardly soap operah circuit circuit paced the story. [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 78.261 | p: 75.000 | r: 81.818
rouge2     | fm: 9.524 | p: 9.091 | r: 10.000
rougeL     | fm: 52.174 | p: 50.000 | r: 54.545
rougeLsum  | fm: 52.174 | p: 50.000 | r: 54.545
r1fm+r2fm = 87.785

[Aggregate metrics]:
rouge1     | fm: 80.482 | p: 80.077 | r: 81.247
rouge2     | fm: 41.844 | p: 41.585 | r: 42.178
rougeL     | fm: 68.175 | p: 67.844 | r: 68.786
rougeLsum  | fm: 68.035 | p: 67.760 | r: 68.665
r1fm+r2fm = 122.326

input #60 time: 0:03:26 | total time: 3:36:28


Running input #61 of 100.
reference: 
========================
, beautiful scene 
========================
*********************************
*********************************
average of cosine similarity 0.999284746941759
highest_index [0]
highest [0.999284746941759]
Debug: ids_shape = 5, pads = [5]
Debug: input ids = tensor([[ 101, 1010, 3376, 3496,  102]], device='cuda:0')
Debug: ref = ['[CLS], beautiful scene [SEP]']
[Init] best rec loss: 1.8300962448120117 for ['[CLS] deploytore glanced [SEP]']
[Init] best rec loss: 1.8116177320480347 for ['[CLS]zeeali donated [SEP]']
[Init] best rec loss: 1.6035362482070923 for ['[CLS] age bad link [SEP]']
[Init] best rec loss: 1.6008113622665405 for ['[CLS] maximus broken initiative [SEP]']
[Init] best rec loss: 1.175456166267395 for ['[CLS] request lets mini [SEP]']
[Init] best perm rec loss: 1.1720333099365234 for ['[CLS] lets mini request [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.042 (perp=8.940, rec=0.246, cos=0.008), tot_loss_proj:2.142 [t=0.26s]
prediction: ['[CLS] beautiful scene beautiful [SEP]']
Attempt swap
Moved token
[ 100/ 500] tot_loss=1.715 (perp=7.752, rec=0.159, cos=0.005), tot_loss_proj:1.823 [t=0.26s]
prediction: ['[CLS] beautiful beautiful scene [SEP]']
[ 150/ 500] tot_loss=1.697 (perp=7.752, rec=0.142, cos=0.004), tot_loss_proj:1.809 [t=0.26s]
prediction: ['[CLS] beautiful beautiful scene [SEP]']
Attempt swap
[ 200/ 500] tot_loss=1.693 (perp=7.752, rec=0.139, cos=0.004), tot_loss_proj:1.813 [t=0.27s]
prediction: ['[CLS] beautiful beautiful scene [SEP]']
Attempt swap
[ 250/ 500] tot_loss=1.684 (perp=7.752, rec=0.131, cos=0.003), tot_loss_proj:1.802 [t=0.27s]
prediction: ['[CLS] beautiful beautiful scene [SEP]']
[ 300/ 500] tot_loss=1.680 (perp=7.752, rec=0.126, cos=0.003), tot_loss_proj:1.818 [t=0.25s]
prediction: ['[CLS] beautiful beautiful scene [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.672 (perp=7.752, rec=0.119, cos=0.002), tot_loss_proj:1.820 [t=0.26s]
prediction: ['[CLS] beautiful beautiful scene [SEP]']
Attempt swap
Swapped tokens
[ 400/ 500] tot_loss=1.505 (perp=7.101, rec=0.083, cos=0.001), tot_loss_proj:1.619 [t=0.25s]
prediction: ['[CLS] beautiful scene, [SEP]']
[ 450/ 500] tot_loss=1.484 (perp=7.101, rec=0.062, cos=0.001), tot_loss_proj:1.622 [t=0.25s]
prediction: ['[CLS] beautiful scene, [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.496 (perp=7.101, rec=0.074, cos=0.001), tot_loss_proj:1.624 [t=0.26s]
prediction: ['[CLS] beautiful scene, [SEP]']
Done with input #61 of 100.
reference: 
========================
[CLS], beautiful scene [SEP]
========================
predicted: 
========================
[CLS] beautiful scene, [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 80.868 | p: 80.415 | r: 81.623
rouge2     | fm: 42.868 | p: 42.614 | r: 43.305
rougeL     | fm: 68.703 | p: 68.446 | r: 69.198
rougeLsum  | fm: 68.478 | p: 68.237 | r: 69.054
r1fm+r2fm = 123.735

input #61 time: 0:03:26 | total time: 3:39:54


Running input #62 of 100.
reference: 
========================
grace to call for prevention rather than to place blame , making it one of the best war movies ever made 
========================
*********************************
*********************************
average of cosine similarity 0.999255650346929
highest_index [0]
highest [0.999255650346929]
Debug: ids_shape = 23, pads = [23]
Debug: input ids = tensor([[ 101, 4519, 2000, 2655, 2005, 9740, 2738, 2084, 2000, 2173, 7499, 1010,
         2437, 2009, 2028, 1997, 1996, 2190, 2162, 5691, 2412, 2081,  102]],
       device='cuda:0')
Debug: ref = ['[CLS] grace to call for prevention rather than to place blame, making it one of the best war movies ever made [SEP]']
[Init] best rec loss: 1.9530375003814697 for ['[CLS] marries ship sonny research bug jersey society bench share states cope change plants talesisation henry settledgly curb ryan bare [SEP]']
[Init] best rec loss: 1.880612850189209 for ['[CLS] sometimes break last convention ian species thousandsthe draft outside over season train telecom ray gayivating migration masovian sure stuff [SEP]']
[Init] best rec loss: 1.7968920469284058 for ['[CLS] help rarely extensionbreaker local sea team mom beacon tear wax chairmanphstatic mum new osman intervention [CLS] attentionius [SEP]']
[Init] best rec loss: 1.767058253288269 for ['[CLS]tled traffic conduct atoms sets commercial : robertsrgeonkney hard sherman [SEP] bus forbc charlie dragons medal same gravity [SEP]']
[Init] best rec loss: 1.749898910522461 for ['[CLS] wasn homosexual carey sometimes wave october, vampire bbc bloody pie hudson contemporary crowd turning error helped pepper thus reaches recently [SEP]']
[Init] best rec loss: 1.741378664970398 for ['[CLS] such duo demand appeared being pv status stereotypes superlary eight song signage thing conform take pup i planetary feed free [SEP]']
[Init] best rec loss: 1.7179762125015259 for ['[CLS] ammunition nowheregut opinion deemed romansdong was mattered al body mono turkish abet main alone stations bag lead facebook [SEP]']
[Init] best rec loss: 1.6971365213394165 for ['[CLS] adam skin lissa love dannberg western wrote food serious apart departureml gameposedmat few resides because track believed [SEP]']
[Init] best perm rec loss: 1.6958225965499878 for ['[CLS]mat departure game wrote skin lissa resides adam western fewml foodnberg track apart because believed seriousposed dan love [SEP]']
[Init] best perm rec loss: 1.6951234340667725 for ['[CLS] adam few departure danmatmlposednberg food because game serious believed resides wrote lissa western apart track love skin [SEP]']
[Init] best perm rec loss: 1.6936944723129272 for ['[CLS] lissa apartml western believed love gamenberg fewposed trackmat because resides dan skin adam serious wrote food departure [SEP]']
[Init] best perm rec loss: 1.6933423280715942 for ['[CLS] track serious adamnberg gameposed apart because few wrote resides dan food skin believed westernml lissa lovemat departure [SEP]']
[Init] best perm rec loss: 1.6924365758895874 for ['[CLS] track resides departure apart few love becausenberg believed wrote lissa game adam dan skinml western seriousposedmat food [SEP]']
[Init] best perm rec loss: 1.6920912265777588 for ['[CLS]mat apart love few western skin because game believednbergml serious adam wrote track departure lissaposed food resides dan [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=3.209 (perp=12.265, rec=0.743, cos=0.013), tot_loss_proj:3.969 [t=0.25s]
prediction: ['[CLS] defense nfl opposition lacking week grip enacted proceedings for no rateund stupid fake numb foreign party cartoon partynesia etc [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.894 (perp=11.258, rec=0.633, cos=0.009), tot_loss_proj:3.834 [t=0.25s]
prediction: ['[CLS] damage jake opposition lacking catholic call were not for no rate put stupid officials ⟩ war stuffles party wrong court [SEP]']
[ 150/ 500] tot_loss=2.961 (perp=11.737, rec=0.610, cos=0.004), tot_loss_proj:3.811 [t=0.25s]
prediction: ["[CLS] invalid jake opposition aviv heavily call was fake forrri or give common war ⟩ war movies pp party'court [SEP]"]
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.849 (perp=11.397, rec=0.564, cos=0.006), tot_loss_proj:3.799 [t=0.26s]
prediction: ["[CLS] invalidmanship opposition hitter myth call him italian forrri or give worst war ⟩ war movies fraud party'avoided [SEP]"]
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=2.869 (perp=11.631, rec=0.541, cos=0.002), tot_loss_proj:3.849 [t=0.25s]
prediction: ["[CLS] fraudmanship opposition hitter violent call him french his sticking or give worst war ⟩ war movies damaged party'avoided [SEP]"]
[ 300/ 500] tot_loss=2.703 (perp=10.893, rec=0.523, cos=0.002), tot_loss_proj:3.740 [t=0.26s]
prediction: ['[CLS] fraudmanship opposition hitter accusations call him to his selena to making worst war ⟩ war movies damaged offenses to prevention [SEP]']
Attempt swap
Moved sequence
[ 350/ 500] tot_loss=2.664 (perp=10.737, rec=0.515, cos=0.002), tot_loss_proj:3.762 [t=0.27s]
prediction: ['[CLS] presentmanship hitter accusations call him opposition state to selena to make worst war grace war movies damaged offenses to prevention [SEP]']
Attempt swap
Moved token
[ 400/ 500] tot_loss=2.652 (perp=10.747, rec=0.501, cos=0.002), tot_loss_proj:3.743 [t=0.27s]
prediction: ['[CLS] presentmanship hitter accusations call him opposition existing to selena to worst war making grace war movies breakdown offenses to prevention [SEP]']
[ 450/ 500] tot_loss=2.700 (perp=10.947, rec=0.493, cos=0.017), tot_loss_proj:3.761 [t=0.26s]
prediction: ['[CLS] consideredmanship made maintenance call him opposition existing to selena to worst war making grace war movies breakdown offenses to prevention [SEP]']
Attempt swap
Moved sequence
[ 500/ 500] tot_loss=2.733 (perp=11.277, rec=0.474, cos=0.004), tot_loss_proj:3.889 [t=0.26s]
prediction: ['[CLS] attemptmanship made interdisciplinary call existing him savings to selena to worst war making grace war movies breakdown offenses to prevention [SEP]']
Done with input #62 of 100.
reference: 
========================
[CLS] grace to call for prevention rather than to place blame, making it one of the best war movies ever made [SEP]
========================
predicted: 
========================
[CLS] attemptmanship made interdisciplinary call to him savings to selena to worst war making grace war movies breakdown offenses to prevention [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 50.000 | p: 50.000 | r: 50.000
rouge2     | fm: 4.762 | p: 4.762 | r: 4.762
rougeL     | fm: 31.818 | p: 31.818 | r: 31.818
rougeLsum  | fm: 31.818 | p: 31.818 | r: 31.818
r1fm+r2fm = 54.762

[Aggregate metrics]:
rouge1     | fm: 80.280 | p: 79.846 | r: 80.999
rouge2     | fm: 42.256 | p: 42.009 | r: 42.614
rougeL     | fm: 68.183 | p: 67.849 | r: 68.757
rougeLsum  | fm: 67.873 | p: 67.606 | r: 68.448
r1fm+r2fm = 122.536

input #62 time: 0:03:28 | total time: 3:43:22


Running input #63 of 100.
reference: 
========================
looking for a return ticket 
========================
*********************************
*********************************
average of cosine similarity 0.9992646432214791
highest_index [0]
highest [0.9992646432214791]
Debug: ids_shape = 7, pads = [7]
Debug: input ids = tensor([[ 101, 2559, 2005, 1037, 2709, 7281,  102]], device='cuda:0')
Debug: ref = ['[CLS] looking for a return ticket [SEP]']
[Init] best rec loss: 1.875612497329712 for ['[CLS] mind newcastle avtees prussia [SEP]']
[Init] best rec loss: 1.2830250263214111 for ['[CLS] where box leftedope [SEP]']
[Init] best rec loss: 1.216496467590332 for ['[CLS] written spend brighter workingism [SEP]']
[Init] best rec loss: 1.2096959352493286 for ['[CLS] prison glided relations musician category [SEP]']
[Init] best rec loss: 1.208850383758545 for ['[CLS] established named tiffany club violent [SEP]']
[Init] best rec loss: 1.1488268375396729 for ['[CLS] ice coe poor t approaching [SEP]']
[Init] best perm rec loss: 1.1447150707244873 for ['[CLS] ice t approaching poor coe [SEP]']
[Init] best perm rec loss: 1.1429733037948608 for ['[CLS] coe approaching t poor ice [SEP]']
[Init] best perm rec loss: 1.142871618270874 for ['[CLS] approaching coe poor t ice [SEP]']
[Init] best perm rec loss: 1.1421325206756592 for ['[CLS] t ice approaching poor coe [SEP]']
[Init] best perm rec loss: 1.1394460201263428 for ['[CLS] coe poor approaching t ice [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.604 (perp=11.261, rec=0.345, cos=0.007), tot_loss_proj:3.088 [t=0.25s]
prediction: ['[CLS] finding slow approaching ticket return [SEP]']
Attempt swap
Moved sequence
[ 100/ 500] tot_loss=2.189 (perp=9.996, rec=0.185, cos=0.004), tot_loss_proj:3.060 [t=0.25s]
prediction: ['[CLS] finding looking return for ticket [SEP]']
[ 150/ 500] tot_loss=2.145 (perp=10.219, rec=0.100, cos=0.001), tot_loss_proj:2.881 [t=0.25s]
prediction: ['[CLS] looking looking return for ticket [SEP]']
Attempt swap
Moved token
[ 200/ 500] tot_loss=1.976 (perp=9.470, rec=0.081, cos=0.001), tot_loss_proj:2.217 [t=0.25s]
prediction: ['[CLS] looking looking for return ticket [SEP]']
Attempt swap
[ 250/ 500] tot_loss=1.875 (perp=8.974, rec=0.080, cos=0.001), tot_loss_proj:2.471 [t=0.25s]
prediction: ['[CLS] finding looking for return ticket [SEP]']
[ 300/ 500] tot_loss=1.754 (perp=8.384, rec=0.077, cos=0.000), tot_loss_proj:2.229 [t=0.25s]
prediction: ['[CLS] for looking for return ticket [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.757 (perp=8.384, rec=0.080, cos=0.000), tot_loss_proj:2.229 [t=0.26s]
prediction: ['[CLS] for looking for return ticket [SEP]']
Attempt swap
Swapped tokens
[ 400/ 500] tot_loss=1.289 (perp=6.111, rec=0.066, cos=0.001), tot_loss_proj:1.310 [t=0.28s]
prediction: ['[CLS] looking for a return ticket [SEP]']
[ 450/ 500] tot_loss=1.293 (perp=6.111, rec=0.070, cos=0.000), tot_loss_proj:1.296 [t=0.26s]
prediction: ['[CLS] looking for a return ticket [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.287 (perp=6.111, rec=0.064, cos=0.000), tot_loss_proj:1.310 [t=0.24s]
prediction: ['[CLS] looking for a return ticket [SEP]']
Done with input #63 of 100.
reference: 
========================
[CLS] looking for a return ticket [SEP]
========================
predicted: 
========================
[CLS] looking for a return ticket [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 80.727 | p: 80.264 | r: 81.407
rouge2     | fm: 43.227 | p: 43.000 | r: 43.562
rougeL     | fm: 68.715 | p: 68.389 | r: 69.240
rougeLsum  | fm: 68.537 | p: 68.201 | r: 69.052
r1fm+r2fm = 123.954

input #63 time: 0:03:28 | total time: 3:46:51


Running input #64 of 100.
reference: 
========================
the strange horror 
========================
*********************************
*********************************
average of cosine similarity 0.999160846219472
highest_index [0]
highest [0.999160846219472]
Debug: ids_shape = 5, pads = [5]
Debug: input ids = tensor([[ 101, 1996, 4326, 5469,  102]], device='cuda:0')
Debug: ref = ['[CLS] the strange horror [SEP]']
[Init] best rec loss: 1.8793610334396362 for ['[CLS] step maddie sorry [SEP]']
[Init] best rec loss: 1.8335012197494507 for ['[CLS]bled independence clearing [SEP]']
[Init] best rec loss: 1.8333404064178467 for ['[CLS]ounded keydale [SEP]']
[Init] best rec loss: 1.414121150970459 for ['[CLS] understood shelter cl [SEP]']
[Init] best rec loss: 1.4084160327911377 for ['[CLS] spends adrian mating [SEP]']
[Init] best rec loss: 1.1607979536056519 for ['[CLS]onale water visions [SEP]']
[Init] best perm rec loss: 1.154465675354004 for ['[CLS] water visionsonale [SEP]']
[Init] best perm rec loss: 1.1528769731521606 for ['[CLS] visions wateronale [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=1.974 (perp=8.653, rec=0.230, cos=0.013), tot_loss_proj:2.060 [t=0.25s]
prediction: ['[CLS] strange horror horror [SEP]']
Attempt swap
[ 100/ 500] tot_loss=1.886 (perp=8.653, rec=0.152, cos=0.003), tot_loss_proj:2.068 [t=0.25s]
prediction: ['[CLS] strange horror horror [SEP]']
[ 150/ 500] tot_loss=1.884 (perp=8.653, rec=0.151, cos=0.003), tot_loss_proj:2.062 [t=0.29s]
prediction: ['[CLS] strange horror horror [SEP]']
Attempt swap
[ 200/ 500] tot_loss=2.065 (perp=9.634, rec=0.133, cos=0.005), tot_loss_proj:2.548 [t=0.26s]
prediction: ['[CLS] strange horror strange [SEP]']
Attempt swap
Moved sequence
[ 250/ 500] tot_loss=1.947 (perp=9.190, rec=0.106, cos=0.003), tot_loss_proj:2.221 [t=0.25s]
prediction: ['[CLS] strange strange horror [SEP]']
[ 300/ 500] tot_loss=1.701 (perp=8.065, rec=0.087, cos=0.001), tot_loss_proj:1.716 [t=0.25s]
prediction: ['[CLS] the strange horror [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.679 (perp=8.065, rec=0.066, cos=0.001), tot_loss_proj:1.717 [t=0.26s]
prediction: ['[CLS] the strange horror [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.681 (perp=8.065, rec=0.067, cos=0.001), tot_loss_proj:1.700 [t=0.25s]
prediction: ['[CLS] the strange horror [SEP]']
[ 450/ 500] tot_loss=1.680 (perp=8.065, rec=0.067, cos=0.000), tot_loss_proj:1.702 [t=0.26s]
prediction: ['[CLS] the strange horror [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.682 (perp=8.065, rec=0.068, cos=0.000), tot_loss_proj:1.716 [t=0.25s]
prediction: ['[CLS] the strange horror [SEP]']
Done with input #64 of 100.
reference: 
========================
[CLS] the strange horror [SEP]
========================
predicted: 
========================
[CLS] the strange horror [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 81.007 | p: 80.567 | r: 81.665
rouge2     | fm: 44.484 | p: 44.252 | r: 44.936
rougeL     | fm: 69.068 | p: 68.800 | r: 69.553
rougeLsum  | fm: 69.178 | p: 68.783 | r: 69.611
r1fm+r2fm = 125.491

input #64 time: 0:03:26 | total time: 3:50:17


Running input #65 of 100.
reference: 
========================
, joyous romp of a film . 
========================
*********************************
*********************************
average of cosine similarity 0.9992260466615117
highest_index [0]
highest [0.9992260466615117]
Debug: ids_shape = 11, pads = [11]
Debug: input ids = tensor([[  101,  1010,  6569,  3560, 17083,  2361,  1997,  1037,  2143,  1012,
           102]], device='cuda:0')
Debug: ref = ['[CLS], joyous romp of a film. [SEP]']
[Init] best rec loss: 1.9480024576187134 for ['[CLS] propertylus total welcomed reichlet every based? [SEP]']
[Init] best rec loss: 1.935711145401001 for ['[CLS] question away greek ordained speaker archbishop afterward flip caine [SEP]']
[Init] best rec loss: 1.9066228866577148 for ['[CLS] stable bourne writers sp here plays spell keeping another [SEP]']
[Init] best rec loss: 1.7932060956954956 for ['[CLS] boss sucks goodbye poorlyions palestinianquest languagecliff [SEP]']
[Init] best rec loss: 1.7819745540618896 for ['[CLS] butter memorandumece happy cry laurence cum york accept [SEP]']
[Init] best rec loss: 1.7478106021881104 for ['[CLS] forth drag roger choice rival familiar howellacingbies [SEP]']
[Init] best rec loss: 1.7460284233093262 for ['[CLS] tobago resist clinched industryementga team rim cancer [SEP]']
[Init] best rec loss: 1.6370292901992798 for ['[CLS] puhoff overs fun someday general evenmament news [SEP]']
[Init] best rec loss: 1.5800213813781738 for ['[CLS] dreamed common evbell infantry soon duel paradise birds [SEP]']
[Init] best perm rec loss: 1.579031229019165 for ['[CLS] birds duel paradise dreamed ev infantry soonbell common [SEP]']
[Init] best perm rec loss: 1.5777679681777954 for ['[CLS] duel birds dreamed infantrybell soon ev paradise common [SEP]']
[Init] best perm rec loss: 1.57088303565979 for ['[CLS] birds commonbell infantry duel dreamed ev paradise soon [SEP]']
[Init] best perm rec loss: 1.5680181980133057 for ['[CLS] dreamed paradise duel common evbell infantry soon birds [SEP]']
[Init] best perm rec loss: 1.5678985118865967 for ['[CLS]bell common paradise dreamed birds duel infantry ev soon [SEP]']
[Init] best perm rec loss: 1.5646346807479858 for ['[CLS] birds duel dreamed infantry evbell common soon paradise [SEP]']
[Init] best perm rec loss: 1.563547968864441 for ['[CLS] dreamed duel paradise common birds evbell infantry soon [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.337 (perp=9.928, rec=0.346, cos=0.005), tot_loss_proj:2.637 [t=0.29s]
prediction: ['[CLS] joy beautiful joy joy inhabitants red joy joy and [SEP]']
Attempt swap
Moved token
[ 100/ 500] tot_loss=2.291 (perp=10.278, rec=0.232, cos=0.003), tot_loss_proj:2.691 [t=0.28s]
prediction: ['[CLS] joy joyed. brilliant ; rom joy joy [SEP]']
[ 150/ 500] tot_loss=2.080 (perp=9.630, rec=0.152, cos=0.002), tot_loss_proj:2.813 [t=0.29s]
prediction: ['[CLS] film joyous, rom ; rom rom joy [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=1.773 (perp=8.241, rec=0.123, cos=0.002), tot_loss_proj:2.466 [t=0.29s]
prediction: ['[CLS] rom joyous, rom ; rom film joy [SEP]']
Attempt swap
Moved token
[ 250/ 500] tot_loss=1.460 (perp=6.834, rec=0.092, cos=0.001), tot_loss_proj:2.143 [t=0.29s]
prediction: ['[CLS] rom joyous, of a filmp. [SEP]']
[ 300/ 500] tot_loss=1.454 (perp=6.834, rec=0.086, cos=0.001), tot_loss_proj:2.140 [t=0.30s]
prediction: ['[CLS] rom joyous, of a filmp. [SEP]']
Attempt swap
Moved token
[ 350/ 500] tot_loss=1.336 (perp=6.288, rec=0.077, cos=0.001), tot_loss_proj:1.763 [t=0.29s]
prediction: ['[CLS] romp joyous, of a film. [SEP]']
Attempt swap
Moved sequence
[ 400/ 500] tot_loss=1.121 (perp=5.096, rec=0.100, cos=0.002), tot_loss_proj:1.305 [t=0.29s]
prediction: ['[CLS] joyous romp, of a film. [SEP]']
[ 450/ 500] tot_loss=1.093 (perp=5.096, rec=0.073, cos=0.001), tot_loss_proj:1.313 [t=0.30s]
prediction: ['[CLS] joyous romp, of a film. [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.089 (perp=5.096, rec=0.069, cos=0.001), tot_loss_proj:1.299 [t=0.29s]
prediction: ['[CLS] joyous romp, of a film. [SEP]']
Done with input #65 of 100.
reference: 
========================
[CLS], joyous romp of a film. [SEP]
========================
predicted: 
========================
[CLS] joyous romp, of a film. [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 81.309 | p: 80.895 | r: 81.999
rouge2     | fm: 45.088 | p: 44.831 | r: 45.375
rougeL     | fm: 69.561 | p: 69.305 | r: 70.091
rougeLsum  | fm: 69.563 | p: 69.292 | r: 70.019
r1fm+r2fm = 126.397

input #65 time: 0:03:55 | total time: 3:54:13


Running input #66 of 100.
reference: 
========================
a longtime tolkien fan 
========================
*********************************
*********************************
average of cosine similarity 0.9992330092949469
highest_index [0]
highest [0.9992330092949469]
Debug: ids_shape = 6, pads = [6]
Debug: input ids = tensor([[  101,  1037, 11155, 23602,  5470,   102]], device='cuda:0')
Debug: ref = ['[CLS] a longtime tolkien fan [SEP]']
[Init] best rec loss: 1.9587653875350952 for ['[CLS] compiler devlin world spell [SEP]']
[Init] best rec loss: 1.838042974472046 for ['[CLS] same heads deep independents [SEP]']
[Init] best rec loss: 1.8294860124588013 for ['[CLS] adding guilty electricion [SEP]']
[Init] best rec loss: 1.8049302101135254 for ['[CLS] school divisional labor liberals [SEP]']
[Init] best rec loss: 1.6972644329071045 for ['[CLS] edo examplewell allmusic [SEP]']
[Init] best rec loss: 1.5281866788864136 for ['[CLS] background leader screen [CLS] [SEP]']
[Init] best rec loss: 1.4521524906158447 for ['[CLS] game scout juliet shoulders [SEP]']
[Init] best rec loss: 1.2160919904708862 for ['[CLS] finish eachensis clark [SEP]']
[Init] best perm rec loss: 1.2056833505630493 for ['[CLS] each clarkensis finish [SEP]']
[Init] best perm rec loss: 1.2052438259124756 for ['[CLS]ensis each clark finish [SEP]']
[Init] best perm rec loss: 1.202699899673462 for ['[CLS]ensis clark each finish [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.601 (perp=11.130, rec=0.365, cos=0.010), tot_loss_proj:3.005 [t=0.28s]
prediction: ['[CLS] fans fan fan positively [SEP]']
Attempt swap
Moved token
[ 100/ 500] tot_loss=2.680 (perp=12.471, rec=0.182, cos=0.003), tot_loss_proj:4.507 [t=0.30s]
prediction: ['[CLS] fan longtime tolkien heavy [SEP]']
[ 150/ 500] tot_loss=2.703 (perp=12.869, rec=0.128, cos=0.002), tot_loss_proj:3.492 [t=0.29s]
prediction: ['[CLS] fan longtime tolkien longtime [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=1.618 (perp=7.673, rec=0.083, cos=0.001), tot_loss_proj:1.614 [t=0.28s]
prediction: ['[CLS] a longtime tolkien fan [SEP]']
Attempt swap
[ 250/ 500] tot_loss=1.601 (perp=7.673, rec=0.065, cos=0.001), tot_loss_proj:1.604 [t=0.30s]
prediction: ['[CLS] a longtime tolkien fan [SEP]']
[ 300/ 500] tot_loss=1.601 (perp=7.673, rec=0.066, cos=0.001), tot_loss_proj:1.600 [t=0.29s]
prediction: ['[CLS] a longtime tolkien fan [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.604 (perp=7.673, rec=0.069, cos=0.001), tot_loss_proj:1.597 [t=0.30s]
prediction: ['[CLS] a longtime tolkien fan [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.603 (perp=7.673, rec=0.068, cos=0.001), tot_loss_proj:1.609 [t=0.30s]
prediction: ['[CLS] a longtime tolkien fan [SEP]']
[ 450/ 500] tot_loss=1.602 (perp=7.673, rec=0.067, cos=0.001), tot_loss_proj:1.611 [t=0.29s]
prediction: ['[CLS] a longtime tolkien fan [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.597 (perp=7.673, rec=0.062, cos=0.001), tot_loss_proj:1.608 [t=0.30s]
prediction: ['[CLS] a longtime tolkien fan [SEP]']
Done with input #66 of 100.
reference: 
========================
[CLS] a longtime tolkien fan [SEP]
========================
predicted: 
========================
[CLS] a longtime tolkien fan [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 81.537 | p: 81.142 | r: 82.185
rouge2     | fm: 46.257 | p: 46.017 | r: 46.593
rougeL     | fm: 69.980 | p: 69.700 | r: 70.462
rougeLsum  | fm: 69.987 | p: 69.668 | r: 70.425
r1fm+r2fm = 127.794

input #66 time: 0:03:55 | total time: 3:58:08


Running input #67 of 100.
reference: 
========================
heartwarming , nonjudgmental kind 
========================
*********************************
*********************************
average of cosine similarity 0.999292690019056
highest_index [0]
highest [0.999292690019056]
Debug: ids_shape = 12, pads = [12]
Debug: input ids = tensor([[  101,  2540,  9028,  6562,  1010,  2512,  9103,  2094, 21693, 21050,
          2785,   102]], device='cuda:0')
Debug: ref = ['[CLS] heartwarming, nonjudgmental kind [SEP]']
[Init] best rec loss: 1.834237813949585 for ['[CLS] art |erved reinided annual later strangeruce beyond [SEP]']
[Init] best rec loss: 1.7785722017288208 for ['[CLS] ^ banksgger biology mont neck administration action pool neo [SEP]']
[Init] best rec loss: 1.7256571054458618 for ['[CLS] mortonructured hendricks partial banned time jae published leave psalm [SEP]']
[Init] best rec loss: 1.69612717628479 for ['[CLS] practically squadron pacific cheated rick under countryorð⁄₄ [SEP]']
[Init] best rec loss: 1.6957756280899048 for ['[CLS]lusionunt sign utah america zeppelin light katy outbreak betray [SEP]']
[Init] best rec loss: 1.6956020593643188 for ['[CLS] [ script part song log principal custom enlisted cabinet charged [SEP]']
[Init] best rec loss: 1.676373839378357 for ['[CLS] marcus cause rudder straight mustered ordinary competitive population getting believe [SEP]']
[Init] best rec loss: 1.6748206615447998 for ['[CLS] vessel definitearound attendant visionrained league nearest sets nut [SEP]']
[Init] best perm rec loss: 1.6736490726470947 for ['[CLS]around nut definiterained vessel league vision attendant nearest sets [SEP]']
[Init] best perm rec loss: 1.6704751253128052 for ['[CLS]around nearestrained definite attendant sets league vision nut vessel [SEP]']
[Init] best perm rec loss: 1.6678307056427002 for ['[CLS] leaguearound vesselrained attendant sets nearest vision definite nut [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=3.686 (perp=14.808, rec=0.719, cos=0.006), tot_loss_proj:4.500 [t=0.25s]
prediction: ['[CLS] championship combatshed stale dim prescribed rather snow reluctantly level [SEP]']
Attempt swap
Moved sequence
[ 100/ 500] tot_loss=3.241 (perp=12.880, rec=0.644, cos=0.020), tot_loss_proj:4.029 [t=0.28s]
prediction: ['[CLS] someone dim identification inclusion combatshed wrath cabin less brady [SEP]']
[ 150/ 500] tot_loss=3.110 (perp=12.596, rec=0.588, cos=0.004), tot_loss_proj:4.414 [t=0.26s]
prediction: ['[CLS] someone kind transferred inclusion combaterate wrath body frank brady [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=3.137 (perp=12.821, rec=0.570, cos=0.003), tot_loss_proj:4.482 [t=0.26s]
prediction: ['[CLS] non kind wrath inclusion combatwar transferred kindright brady [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=3.071 (perp=12.493, rec=0.563, cos=0.009), tot_loss_proj:4.379 [t=0.26s]
prediction: ['[CLS] non kind wrath v8 nonwarhoff kindgrade brady [SEP]']
[ 300/ 500] tot_loss=2.883 (perp=11.610, rec=0.546, cos=0.015), tot_loss_proj:4.246 [t=0.25s]
prediction: ['[CLS] non kind misty v8 nonwarable kind non brady [SEP]']
Attempt swap
Moved token
[ 350/ 500] tot_loss=2.739 (perp=11.079, rec=0.521, cos=0.002), tot_loss_proj:4.184 [t=0.25s]
prediction: ['[CLS] non v8 kind misty nonwarable kind non brady [SEP]']
Attempt swap
[ 400/ 500] tot_loss=2.825 (perp=11.501, rec=0.516, cos=0.008), tot_loss_proj:4.265 [t=0.25s]
prediction: ['[CLS] non v8 kind misty nonwarable kindental brady [SEP]']
[ 450/ 500] tot_loss=3.213 (perp=13.519, rec=0.501, cos=0.008), tot_loss_proj:4.352 [t=0.26s]
prediction: ['[CLS] non v8 kind misty∘warental kindental faux [SEP]']
Attempt swap
Swapped tokens
[ 500/ 500] tot_loss=3.252 (perp=13.714, rec=0.495, cos=0.014), tot_loss_proj:4.279 [t=0.26s]
prediction: ['[CLS] non v8 kind∘ mistywarental kindental faux [SEP]']
Done with input #67 of 100.
reference: 
========================
[CLS] heartwarming, nonjudgmental kind [SEP]
========================
predicted: 
========================
[CLS] non v8 kind∘ sortwarental kindental faux [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 46.154 | p: 37.500 | r: 60.000
rouge2     | fm: 0.000 | p: 0.000 | r: 0.000
rougeL     | fm: 46.154 | p: 37.500 | r: 60.000
rougeLsum  | fm: 46.154 | p: 37.500 | r: 60.000
r1fm+r2fm = 46.154

[Aggregate metrics]:
rouge1     | fm: 80.967 | p: 80.478 | r: 81.828
rouge2     | fm: 45.367 | p: 45.105 | r: 45.735
rougeL     | fm: 69.551 | p: 69.273 | r: 70.201
rougeLsum  | fm: 69.573 | p: 69.154 | r: 70.369
r1fm+r2fm = 126.333

input #67 time: 0:03:27 | total time: 4:01:36


Running input #68 of 100.
reference: 
========================
uncouth , incomprehensible , vicious and absurd 
========================
*********************************
*********************************
average of cosine similarity 0.9992789242841313
highest_index [0]
highest [0.9992789242841313]
Debug: ids_shape = 15, pads = [15]
Debug: input ids = tensor([[  101,  4895,  3597, 14317,  1010,  4297, 25377,  2890, 10222, 19307,
          1010, 13925,  1998, 18691,   102]], device='cuda:0')
Debug: ref = ['[CLS] uncouth, incomprehensible, vicious and absurd [SEP]']
[Init] best rec loss: 1.9883288145065308 for ['[CLS] view top well bam senate upon campus grade podium elected essentialget combat [SEP]']
[Init] best rec loss: 1.9612467288970947 for ['[CLS] brothers tensionquitable tyler twist yes year brought % almost barely pain emirates [SEP]']
[Init] best rec loss: 1.6541982889175415 for ['[CLS] gun ʐ station affairs substance enough sleepsrableoper manual background michael deadline [SEP]']
[Init] best rec loss: 1.6288515329360962 for ['[CLS] °f force recreationalyde fighting extras who livestock guaranteed singles short gloves kitchen [SEP]']
[Init] best rec loss: 1.6066203117370605 for ['[CLS] collecting congresses hundred slightest summit survive [CLS] although fred diego fantastic relief? [SEP]']
[Init] best rec loss: 1.5576926469802856 for ['[CLS] $ offs dreams national been wallszzsar council frederick but lankan comprehensive [SEP]']
[Init] best rec loss: 1.459212064743042 for ['[CLS] feelings stole besides spoil bit prone decay spider insteadane about ars payments [SEP]']
[Init] best rec loss: 1.1719552278518677 for ['[CLS]. form beth floor view medal comfortiferous riding councils diedyn possibly [SEP]']
[Init] best perm rec loss: 1.1586320400238037 for ['[CLS] floor medal comfort viewiferous beth riding possiblyyn form. councils died [SEP]']
[Init] best perm rec loss: 1.1505303382873535 for ['[CLS] form possiblyyn diediferous. floor view councils riding comfort medal beth [SEP]']
[Init] best perm rec loss: 1.1384704113006592 for ['[CLS] possiblyiferous.yn riding medal form councils view floor comfort beth died [SEP]']
[Init] best perm rec loss: 1.1313153505325317 for ['[CLS]iferous beth died form. floor councils riding medalyn view possibly comfort [SEP]']
[Init] best perm rec loss: 1.1300716400146484 for ['[CLS]yniferous comfort beth floor form possibly medal riding view. councils died [SEP]']
[Init] best perm rec loss: 1.1216068267822266 for ['[CLS]yn comfort riding possiblyiferous beth councils form floor medal. view died [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.759 (perp=12.369, rec=0.282, cos=0.003), tot_loss_proj:3.022 [t=0.26s]
prediction: ['[CLS] un preceding system absurd off confront pathetic term presented andsible vicious series [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.298 (perp=10.476, rec=0.201, cos=0.001), tot_loss_proj:2.875 [t=0.25s]
prediction: ['[CLS] uncocent absurd un absurd festival zealand presented,sible vicious absurd [SEP]']
[ 150/ 500] tot_loss=1.989 (perp=9.202, rec=0.144, cos=0.005), tot_loss_proj:2.430 [t=0.27s]
prediction: ['[CLS] uncouth absurd un absurd festival, very andsible vicioussible [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.041 (perp=9.613, rec=0.118, cos=0.001), tot_loss_proj:2.444 [t=0.26s]
prediction: ['[CLS]hencouth absurd un absurd plus,sible and dumb vicioussible [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=1.916 (perp=9.046, rec=0.106, cos=0.001), tot_loss_proj:2.352 [t=0.25s]
prediction: ['[CLS],couth absurd un absurd plushensible and insane vicioussible [SEP]']
[ 300/ 500] tot_loss=1.903 (perp=9.046, rec=0.094, cos=0.000), tot_loss_proj:2.356 [t=0.26s]
prediction: ['[CLS],couth absurd un absurd plushensible and insane vicioussible [SEP]']
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=1.725 (perp=8.142, rec=0.096, cos=0.000), tot_loss_proj:2.023 [t=0.26s]
prediction: ['[CLS] uncouth absurd, absurdishhensible and insane vicioussible [SEP]']
Attempt swap
Moved token
[ 400/ 500] tot_loss=1.785 (perp=8.428, rec=0.098, cos=0.001), tot_loss_proj:2.112 [t=0.27s]
prediction: ['[CLS] uncouth absurd, absurdish inc and dumb vicioushensible [SEP]']
[ 450/ 500] tot_loss=1.771 (perp=8.428, rec=0.085, cos=0.000), tot_loss_proj:2.103 [t=0.27s]
prediction: ['[CLS] uncouth absurd, absurdish inc and dumb vicioushensible [SEP]']
Attempt swap
Moved token
[ 500/ 500] tot_loss=1.372 (perp=6.452, rec=0.081, cos=0.000), tot_loss_proj:1.687 [t=0.24s]
prediction: ['[CLS] uncouth absurd, absurd, and inc dumb vicioushensible [SEP]']
Done with input #68 of 100.
reference: 
========================
[CLS] uncouth, incomprehensible, vicious and absurd [SEP]
========================
predicted: 
========================
[CLS] uncouth absurd, absurdishhen inc and insane vicioussible [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 62.500 | p: 55.556 | r: 71.429
rouge2     | fm: 14.286 | p: 12.500 | r: 16.667
rougeL     | fm: 50.000 | p: 44.444 | r: 57.143
rougeLsum  | fm: 50.000 | p: 44.444 | r: 57.143
r1fm+r2fm = 76.786

[Aggregate metrics]:
rouge1     | fm: 80.736 | p: 80.085 | r: 81.661
rouge2     | fm: 45.170 | p: 44.824 | r: 45.537
rougeL     | fm: 69.371 | p: 68.903 | r: 70.146
rougeLsum  | fm: 69.423 | p: 68.900 | r: 70.263
r1fm+r2fm = 125.906

input #68 time: 0:03:26 | total time: 4:05:02


Running input #69 of 100.
reference: 
========================
a real winner -- smart , funny , subtle , and resonant . 
========================
*********************************
*********************************
average of cosine similarity 0.9992908222867736
highest_index [0]
highest [0.9992908222867736]
Debug: ids_shape = 18, pads = [18]
Debug: input ids = tensor([[  101,  1037,  2613,  3453,  1011,  1011,  6047,  1010,  6057,  1010,
         11259,  1010,  1998, 24501,  7856,  3372,  1012,   102]],
       device='cuda:0')
Debug: ref = ['[CLS] a real winner - - smart, funny, subtle, and resonant. [SEP]']
[Init] best rec loss: 1.8529300689697266 for ['[CLS] california worth additional aston choices successfully apprenticeship rothschild mob kick or model sole living promoting cooper [SEP]']
[Init] best rec loss: 1.7803916931152344 for ['[CLS] pretty campus department slow behind alias laborphobia really abilityrama sl offices markers schedule maximum [SEP]']
[Init] best rec loss: 1.777809977531433 for ['[CLS] link mark andgi gutierrez exile planwriter bot amateur innings dreaming chestots those watershed [SEP]']
[Init] best rec loss: 1.6611499786376953 for ['[CLS] squadong it code recording why what qualifyingjonpsy bad bound paintings nuclear only panchayat [SEP]']
[Init] best rec loss: 1.5930790901184082 for ["[CLS] wait pale s force'an tyne km honey teaching contemporaryable finn over thanked favourite [SEP]"]
[Init] best perm rec loss: 1.5880229473114014 for ["[CLS] finn teaching honey s contemporary thanked favourite over wait km an force tyneable'pale [SEP]"]
[Init] best perm rec loss: 1.587775468826294 for ["[CLS]'thanked finn favourite contemporary pale teaching wait km an s over tyne honeyable force [SEP]"]
[Init] best perm rec loss: 1.585741400718689 for ["[CLS] over honey contemporary s favourite thanked tyne km finn pale'force teaching waitable an [SEP]"]
[Init] best perm rec loss: 1.5847747325897217 for ["[CLS] honey forceable s pale an teaching contemporary km finn thanked favourite over wait'tyne [SEP]"]
[Init] best perm rec loss: 1.5815293788909912 for ["[CLS] thanked s tyne favourite'pale teaching km finn contemporary force anable honey over wait [SEP]"]
[Init] best perm rec loss: 1.58074951171875 for ["[CLS] pale s over teaching km thanked favourite'finn tyne forceable honey wait contemporary an [SEP]"]
[Init] best perm rec loss: 1.5794026851654053 for ["[CLS] tyneable s wait favourite pale finn force contemporary thanked over km'teaching honey an [SEP]"]
[Init] best perm rec loss: 1.5793405771255493 for ["[CLS] over honey contemporary tyne teaching favourite finn sable force'an wait km pale thanked [SEP]"]
Nsteps: 500
[  50/ 500] tot_loss=3.250 (perp=12.441, rec=0.747, cos=0.015), tot_loss_proj:4.019 [t=0.21s]
prediction: ['[CLS] or lawsuits any offended of against terrorist " losing picture dull unconstitutional talk maddox plate kitchen [SEP]']
Attempt swap
Moved sequence
[ 100/ 500] tot_loss=3.207 (perp=12.910, rec=0.620, cos=0.006), tot_loss_proj:4.138 [t=0.21s]
prediction: ['[CLS] or bubbaun of hell againstvere " losing phone dull courthouse talkwo ass nobody [SEP]']
[ 150/ 500] tot_loss=2.916 (perp=11.724, rec=0.570, cos=0.002), tot_loss_proj:4.022 [t=0.21s]
prediction: ['[CLS] or amongun of hell against contributed " might ago barely dead cop atlanta ass nobody [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.565 (perp=10.102, rec=0.538, cos=0.006), tot_loss_proj:3.896 [t=0.21s]
prediction: ['[CLS] or for blond of her against contributed ", vinci without dead anything ; smart invalid [SEP]']
Attempt swap
Moved token
[ 250/ 500] tot_loss=2.537 (perp=10.091, rec=0.507, cos=0.012), tot_loss_proj:3.968 [t=0.21s]
prediction: ['[CLS] among blond of hell or against definitely ", deeply without dead anything ; smart invalid [SEP]']
[ 300/ 500] tot_loss=2.607 (perp=10.532, rec=0.493, cos=0.008), tot_loss_proj:4.073 [t=0.21s]
prediction: ['[CLS] for nobody of mark orory definitely ", bce without dead anything ; criminalun [SEP]']
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=2.490 (perp=10.084, rec=0.470, cos=0.004), tot_loss_proj:3.910 [t=0.21s]
prediction: ['[CLS] for nobody - hell deeplyory definitely and, or without dead anything ; criminal middle [SEP]']
Attempt swap
Swapped tokens
[ 400/ 500] tot_loss=2.234 (perp=8.775, rec=0.461, cos=0.018), tot_loss_proj:3.733 [t=0.21s]
prediction: ['[CLS] for nobody - my obviously, and definitely, or without dead why ; criminal middle [SEP]']
[ 450/ 500] tot_loss=2.282 (perp=9.103, rec=0.456, cos=0.006), tot_loss_proj:3.535 [t=0.21s]
prediction: ['[CLS] easily nobody - my obviously,, real,, without dead results ; criminal middle [SEP]']
Attempt swap
Moved token
[ 500/ 500] tot_loss=2.174 (perp=8.666, rec=0.437, cos=0.003), tot_loss_proj:3.771 [t=0.21s]
prediction: ['[CLS] for nobody - my obviously,, creative, pretty, & results ; criminal middle [SEP]']
Done with input #69 of 100.
reference: 
========================
[CLS] a real winner - - smart, funny, subtle, and resonant. [SEP]
========================
predicted: 
========================
[CLS] for nobody - my obviously,, real, pretty, & results ; criminal middle [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 28.571 | p: 27.273 | r: 30.000
rouge2     | fm: 0.000 | p: 0.000 | r: 0.000
rougeL     | fm: 28.571 | p: 27.273 | r: 30.000
rougeLsum  | fm: 28.571 | p: 27.273 | r: 30.000
r1fm+r2fm = 28.571

[Aggregate metrics]:
rouge1     | fm: 80.114 | p: 79.457 | r: 80.973
rouge2     | fm: 44.381 | p: 44.131 | r: 44.811
rougeL     | fm: 68.749 | p: 68.297 | r: 69.487
rougeLsum  | fm: 68.622 | p: 68.131 | r: 69.506
r1fm+r2fm = 124.495

input #69 time: 0:02:38 | total time: 4:07:41


Running input #70 of 100.
reference: 
========================
gets clunky on the screen 
========================
*********************************
*********************************
average of cosine similarity 0.9993748902467187
highest_index [0]
highest [0.9993748902467187]
Debug: ids_shape = 9, pads = [9]
Debug: input ids = tensor([[  101,  4152, 18856, 16814,  2100,  2006,  1996,  3898,   102]],
       device='cuda:0')
Debug: ref = ['[CLS] gets clunky on the screen [SEP]']
[Init] best rec loss: 1.7679567337036133 for ['[CLS] [CLS]el yourself lined dos written redwood [SEP]']
[Init] best rec loss: 1.536930799484253 for ['[CLS] monk laundry lights contractsbell gala viet [SEP]']
[Init] best rec loss: 1.408258080482483 for ['[CLS] buenos sol aspects powerful otherpass wallace [SEP]']
[Init] best rec loss: 1.3998394012451172 for ['[CLS] piano myth casualty immediately vocal right bottle [SEP]']
[Init] best rec loss: 1.2374904155731201 for ['[CLS] detention technological effects blood sharma herself mark [SEP]']
[Init] best rec loss: 1.2105610370635986 for ['[CLS] meg admitthermal success prize debut falcon [SEP]']
[Init] best rec loss: 1.1166847944259644 for ['[CLS] modern bob party িbution guy muscle [SEP]']
[Init] best perm rec loss: 1.1123631000518799 for ['[CLS] modern bobbution guy ি muscle party [SEP]']
[Init] best perm rec loss: 1.110944390296936 for ['[CLS] িbution guy party muscle modern bob [SEP]']
[Init] best perm rec loss: 1.1077313423156738 for ['[CLS] muscle িbution party guy modern bob [SEP]']
[Init] best perm rec loss: 1.1058592796325684 for ['[CLS] bob guy modern muscle িbution party [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.268 (perp=9.260, rec=0.397, cos=0.019), tot_loss_proj:2.905 [t=0.26s]
prediction: ['[CLS] load rash. = wreck location weighted [SEP]']
Attempt swap
Moved sequence
[ 100/ 500] tot_loss=2.525 (perp=11.214, rec=0.279, cos=0.004), tot_loss_proj:3.269 [t=0.26s]
prediction: ['[CLS]unk rash is gets against on screen [SEP]']
[ 150/ 500] tot_loss=2.597 (perp=11.916, rec=0.206, cos=0.008), tot_loss_proj:3.661 [t=0.26s]
prediction: ['[CLS]unky is gets against cl screen [SEP]']
Attempt swap
Moved token
[ 200/ 500] tot_loss=2.090 (perp=9.604, rec=0.164, cos=0.005), tot_loss_proj:2.428 [t=0.26s]
prediction: ['[CLS] clunky on getsunk screen [SEP]']
Attempt swap
Moved sequence
[ 250/ 500] tot_loss=1.985 (perp=9.336, rec=0.116, cos=0.001), tot_loss_proj:2.769 [t=0.26s]
prediction: ['[CLS] clunky gets cl on screen [SEP]']
[ 300/ 500] tot_loss=1.974 (perp=9.336, rec=0.106, cos=0.001), tot_loss_proj:2.763 [t=0.26s]
prediction: ['[CLS] clunky gets cl on screen [SEP]']
Attempt swap
Moved token
[ 350/ 500] tot_loss=1.654 (perp=7.786, rec=0.096, cos=0.001), tot_loss_proj:2.477 [t=0.27s]
prediction: ['[CLS] cl clunky gets on screen [SEP]']
Attempt swap
Moved sequence
[ 400/ 500] tot_loss=1.515 (perp=6.898, rec=0.130, cos=0.006), tot_loss_proj:1.740 [t=0.26s]
prediction: ['[CLS] cl gets clunky on screen [SEP]']
[ 450/ 500] tot_loss=1.476 (perp=6.898, rec=0.095, cos=0.001), tot_loss_proj:1.753 [t=0.25s]
prediction: ['[CLS] cl gets clunky on screen [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.473 (perp=6.898, rec=0.093, cos=0.001), tot_loss_proj:1.750 [t=0.26s]
prediction: ['[CLS] cl gets clunky on screen [SEP]']
Done with input #70 of 100.
reference: 
========================
[CLS] gets clunky on the screen [SEP]
========================
predicted: 
========================
[CLS] cl clunky gets on screen [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 85.714 | p: 85.714 | r: 85.714
rouge2     | fm: 16.667 | p: 16.667 | r: 16.667
rougeL     | fm: 71.429 | p: 71.429 | r: 71.429
rougeLsum  | fm: 71.429 | p: 71.429 | r: 71.429
r1fm+r2fm = 102.381

[Aggregate metrics]:
rouge1     | fm: 80.163 | p: 79.596 | r: 81.080
rouge2     | fm: 43.871 | p: 43.599 | r: 44.236
rougeL     | fm: 68.761 | p: 68.254 | r: 69.589
rougeLsum  | fm: 68.740 | p: 68.189 | r: 69.524
r1fm+r2fm = 124.033

input #70 time: 0:03:30 | total time: 4:11:11


Running input #71 of 100.
reference: 
========================
there 's not a single jump-in-your-seat moment and 
========================
*********************************
*********************************
average of cosine similarity 0.9993403548184049
highest_index [0]
highest [0.9993403548184049]
Debug: ids_shape = 17, pads = [17]
Debug: input ids = tensor([[ 101, 2045, 1005, 1055, 2025, 1037, 2309, 5376, 1011, 1999, 1011, 2115,
         1011, 2835, 2617, 1998,  102]], device='cuda:0')
Debug: ref = ["[CLS] there's not a single jump - in - your - seat moment and [SEP]"]
[Init] best rec loss: 1.8314898014068604 for ['[CLS] numerous boot maintenance archive welded societies nam recover news placement 1400 hunter bridegative between [SEP]']
[Init] best rec loss: 1.6892049312591553 for ['[CLS] exceed proof streak romeo cardinalsifice shy quality settlershaft unit copyright shawn rapid expensive [SEP]']
[Init] best rec loss: 1.4518476724624634 for ['[CLS] promising della ticket bbc cut claireda spielberg amsterdam tomb counts july adding annoyance elias [SEP]']
[Init] best rec loss: 1.4424015283584595 for ['[CLS] bad drainage monster seatystvision recorded abbotrcus distinguish luke safety smaller petition contracts [SEP]']
[Init] best rec loss: 1.4241973161697388 for ['[CLS] bonus sam deck resolve carson defense lots cancer sparhawk reasonable innocence pills no colony secret [SEP]']
[Init] best rec loss: 1.4165459871292114 for ['[CLS] emptied advertising dominant orange gap mini clothes subsequent history series dakotaminatehab goesu [SEP]']
[Init] best rec loss: 1.3269407749176025 for ['[CLS]gies amir plus locomotive contacthane flat all highest helmet postal operations political blindness colors [SEP]']
[Init] best rec loss: 1.3191274404525757 for ['[CLS] jean bed queens fewer of professor fall ram surhear marshall over liam molly creatures [SEP]']
[Init] best perm rec loss: 1.3124475479125977 for ['[CLS] ramhear liam bed fall jean fewer professor over creatures queens molly sur marshall of [SEP]']
[Init] best perm rec loss: 1.312007188796997 for ['[CLS] molly liam fewer professor of queens jean creatures sur marshall bed over ram fallhear [SEP]']
[Init] best perm rec loss: 1.3116501569747925 for ['[CLS]hear marshall sur ram of fewer professor liam creatures molly bed over queens fall jean [SEP]']
[Init] best perm rec loss: 1.309478759765625 for ['[CLS] sur jeanhear fall fewer marshall molly liam professor ram queens bed of creatures over [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.810 (perp=11.994, rec=0.405, cos=0.006), tot_loss_proj:3.864 [t=0.25s]
prediction: ["[CLS]station that had t any legislation trouble dinner stryker state a words wanted'over [SEP]"]
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.175 (perp=9.422, rec=0.287, cos=0.004), tot_loss_proj:3.009 [t=0.27s]
prediction: ["[CLS] spray there has not your jump minute legislation'okay a single want moment on [SEP]"]
[ 150/ 500] tot_loss=2.032 (perp=9.165, rec=0.198, cos=0.002), tot_loss_proj:2.850 [t=0.26s]
prediction: ['[CLS] spray there has not your jump jump jump sort okay a single seat moment and [SEP]']
Attempt swap
Moved sequence
[ 200/ 500] tot_loss=1.547 (perp=6.722, rec=0.201, cos=0.001), tot_loss_proj:2.405 [t=0.26s]
prediction: ['[CLS] you there is not a single jump moment and your jump jump jumpisance or [SEP]']
Attempt swap
Moved token
[ 250/ 500] tot_loss=1.513 (perp=6.796, rec=0.153, cos=0.001), tot_loss_proj:2.438 [t=0.27s]
prediction: ['[CLS] you there s not a single jump moment and your jump jumpisance / jump [SEP]']
[ 300/ 500] tot_loss=1.534 (perp=6.976, rec=0.138, cos=0.001), tot_loss_proj:2.163 [t=0.28s]
prediction: ['[CLS] and there s not a single jump moment - your jump seatisance / jump [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.510 (perp=6.976, rec=0.115, cos=0.001), tot_loss_proj:2.168 [t=0.27s]
prediction: ['[CLS] and there s not a single jump moment - your jump seatisance / jump [SEP]']
Attempt swap
Moved token
[ 400/ 500] tot_loss=1.460 (perp=6.701, rec=0.119, cos=0.001), tot_loss_proj:1.888 [t=0.25s]
prediction: ['[CLS] and there s not a single jump moment - your jump seat jumpisance - [SEP]']
[ 450/ 500] tot_loss=1.443 (perp=6.701, rec=0.102, cos=0.000), tot_loss_proj:1.894 [t=0.27s]
prediction: ['[CLS] and there s not a single jump moment - your jump seat jumpisance - [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.427 (perp=6.701, rec=0.086, cos=0.000), tot_loss_proj:1.897 [t=0.26s]
prediction: ['[CLS] and there s not a single jump moment - your jump seat jumpisance - [SEP]']
Done with input #71 of 100.
reference: 
========================
[CLS] there's not a single jump - in - your - seat moment and [SEP]
========================
predicted: 
========================
[CLS] and there s not a single jump moment - your jump seat jumpisance - [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 88.889 | p: 85.714 | r: 92.308
rouge2     | fm: 40.000 | p: 38.462 | r: 41.667
rougeL     | fm: 74.074 | p: 71.429 | r: 76.923
rougeLsum  | fm: 74.074 | p: 71.429 | r: 76.923
r1fm+r2fm = 128.889

[Aggregate metrics]:
rouge1     | fm: 80.234 | p: 79.652 | r: 81.191
rouge2     | fm: 43.832 | p: 43.588 | r: 44.205
rougeL     | fm: 69.069 | p: 68.521 | r: 69.761
rougeLsum  | fm: 68.831 | p: 68.300 | r: 69.713
r1fm+r2fm = 124.066

input #71 time: 0:03:30 | total time: 4:14:42


Running input #72 of 100.
reference: 
========================
has a tougher time balancing its violence with kafka-inspired philosophy 
========================
*********************************
*********************************
average of cosine similarity 0.9992329827199963
highest_index [0]
highest [0.9992329827199963]
Debug: ids_shape = 17, pads = [17]
Debug: input ids = tensor([[  101,  2038,  1037,  7823,  2121,  2051, 20120,  2049,  4808,  2007,
         10556, 24316,  2050,  1011,  4427,  4695,   102]], device='cuda:0')
Debug: ref = ['[CLS] has a tougher time balancing its violence with kafka - inspired philosophy [SEP]']
[Init] best rec loss: 1.660793662071228 for ['[CLS] fall cdpock eclipsefle anatomicalesis son quick sunshine percentage liquor katraz grandpa [SEP]']
[Init] best rec loss: 1.5929793119430542 for ['[CLS]ei credit cross chestduction mobile cis donekar rights grab bach route dot please [SEP]']
[Init] best rec loss: 1.4265902042388916 for ['[CLS] sutton matthew asher happily other virus variety killed an ear tar fixurn deepara [SEP]']
[Init] best rec loss: 1.4027378559112549 for ['[CLS]nted seriously public caesar failure major eli city edo memberf runway koppen join useless [SEP]']
[Init] best rec loss: 1.3698697090148926 for ['[CLS] stand parameters sunday fence turn strange breast ground and videos attic sets fell anotherraphic [SEP]']
[Init] best rec loss: 1.3598313331604004 for ['[CLS] shot stoppedwk wide maintenance upheld excused formation trojan al clit buckingham sand aching dams [SEP]']
[Init] best rec loss: 1.2334275245666504 for ['[CLS] office sat prime beneath applicationsism test ward humor spoke meal canada day school transportation [SEP]']
[Init] best rec loss: 1.2262848615646362 for ['[CLS] easier unified familiar sy ringo demand self injury outern board end craft dawn gods [SEP]']
[Init] best rec loss: 1.216339349746704 for ['[CLS] them relations quickly sip abexed without connected counties glacier digitalhic professor teller page [SEP]']
[Init] best rec loss: 1.190377116203308 for ['[CLS] things substitute air favors bishop defined werewolf anywaylusion ghana tonnesboard favorfin cy [SEP]']
[Init] best perm rec loss: 1.1880836486816406 for ['[CLS] ghana favorslusion air cy anywayboard werewolf tonnes things favor substitutefin bishop defined [SEP]']
[Init] best perm rec loss: 1.1873072385787964 for ['[CLS] favor ghana air bishop tonnes substitutelusion werewolf cy things anywayboard defined favorsfin [SEP]']
[Init] best perm rec loss: 1.1828306913375854 for ['[CLS] ghana bishop defined substitute tonnes favors anywaylusion things werewolffin favor air cyboard [SEP]']
[Init] best perm rec loss: 1.1827526092529297 for ['[CLS]fin favorslusion substitute anyway werewolf bishop air definedboard things favor ghana cy tonnes [SEP]']
[Init] best perm rec loss: 1.1818435192108154 for ['[CLS] ghana cy favorlusion defined substituteboard tonnes air things bishop favors anywayfin werewolf [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=3.244 (perp=13.972, rec=0.440, cos=0.010), tot_loss_proj:3.899 [t=0.25s]
prediction: ['[CLS] falls pressure numb start harder node framework grass pass suffer prison worriedesian rope decline [SEP]']
Attempt swap
Moved token
[ 100/ 500] tot_loss=2.786 (perp=12.262, rec=0.330, cos=0.003), tot_loss_proj:3.368 [t=0.26s]
prediction: ['[CLS] has hours numb harder having node tough no pass more time balanceesian hate violence [SEP]']
[ 150/ 500] tot_loss=2.627 (perp=11.807, rec=0.264, cos=0.002), tot_loss_proj:3.160 [t=0.27s]
prediction: ['[CLS] has seven badly harder a vertebrae tough ) hard more time balance main philosophy violence [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.381 (perp=10.942, rec=0.190, cos=0.003), tot_loss_proj:3.336 [t=0.28s]
prediction: ['[CLS] has its badlyer a combat tough its tough has time balancing latin philosophy violence [SEP]']
Attempt swap
Moved token
[ 250/ 500] tot_loss=2.269 (perp=10.433, rec=0.179, cos=0.004), tot_loss_proj:2.999 [t=0.25s]
prediction: ['[CLS] has its decline tougher a its tough its a time balancingbacks philosophy violence [SEP]']
[ 300/ 500] tot_loss=2.120 (perp=9.991, rec=0.121, cos=0.001), tot_loss_proj:2.997 [t=0.26s]
prediction: ['[CLS] has its badly tougher a with tough its a time balancingavio philosophy violence [SEP]']
Attempt swap
Moved token
[ 350/ 500] tot_loss=1.961 (perp=9.261, rec=0.109, cos=0.000), tot_loss_proj:2.716 [t=0.27s]
prediction: ['[CLS] has its badly tougher a with its tough a time balancingavio philosophy violence [SEP]']
Attempt swap
Swapped tokens
[ 400/ 500] tot_loss=1.770 (perp=8.280, rec=0.113, cos=0.001), tot_loss_proj:2.289 [t=0.31s]
prediction: ['[CLS] has a badly tougher philosophy with its tough a time balancing themed a violence [SEP]']
[ 450/ 500] tot_loss=1.748 (perp=8.280, rec=0.091, cos=0.000), tot_loss_proj:2.287 [t=0.29s]
prediction: ['[CLS] has a badly tougher philosophy with its tough a time balancing themed a violence [SEP]']
Attempt swap
Swapped tokens
[ 500/ 500] tot_loss=1.635 (perp=7.755, rec=0.083, cos=0.000), tot_loss_proj:2.215 [t=0.30s]
prediction: ['[CLS] has a badly tougher philosophy with its tough a time balancing themed violence a [SEP]']
Done with input #72 of 100.
reference: 
========================
[CLS] has a tougher time balancing its violence with kafka - inspired philosophy [SEP]
========================
predicted: 
========================
[CLS] has a badly tougher philosophy with its tough a time balancing themed violence a [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 75.862 | p: 68.750 | r: 84.615
rouge2     | fm: 22.222 | p: 20.000 | r: 25.000
rougeL     | fm: 55.172 | p: 50.000 | r: 61.538
rougeLsum  | fm: 55.172 | p: 50.000 | r: 61.538
r1fm+r2fm = 98.084

[Aggregate metrics]:
rouge1     | fm: 80.189 | p: 79.465 | r: 81.247
rouge2     | fm: 43.522 | p: 43.233 | r: 44.001
rougeL     | fm: 68.723 | p: 68.061 | r: 69.558
rougeLsum  | fm: 68.598 | p: 67.973 | r: 69.552
r1fm+r2fm = 123.711

input #72 time: 0:03:35 | total time: 4:18:18


Running input #73 of 100.
reference: 
========================
bad filmmaking 
========================
*********************************
*********************************
average of cosine similarity 0.9991746597716884
highest_index [0]
highest [0.9991746597716884]
Debug: ids_shape = 4, pads = [4]
Debug: input ids = tensor([[  101,  2919, 24466,   102]], device='cuda:0')
Debug: ref = ['[CLS] bad filmmaking [SEP]']
[Init] best rec loss: 1.8824208974838257 for ['[CLS] lands anglo [SEP]']
[Init] best rec loss: 1.7401381731033325 for ['[CLS] capitol living [SEP]']
[Init] best rec loss: 1.6489461660385132 for ['[CLS] symphony apparatus [SEP]']
[Init] best rec loss: 1.5039418935775757 for ['[CLS] denied airline [SEP]']
[Init] best rec loss: 1.4107468128204346 for ['[CLS] tierney sector [SEP]']
[Init] best perm rec loss: 1.407352328300476 for ['[CLS] sector tierney [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.185 (perp=9.723, rec=0.234, cos=0.007), tot_loss_proj:2.016 [t=0.30s]
prediction: ['[CLS] bad filmmaking [SEP]']
Attempt swap
[ 100/ 500] tot_loss=2.026 (perp=9.723, rec=0.079, cos=0.002), tot_loss_proj:2.024 [t=0.29s]
prediction: ['[CLS] bad filmmaking [SEP]']
[ 150/ 500] tot_loss=2.026 (perp=9.723, rec=0.080, cos=0.001), tot_loss_proj:2.028 [t=0.31s]
prediction: ['[CLS] bad filmmaking [SEP]']
Attempt swap
[ 200/ 500] tot_loss=2.011 (perp=9.723, rec=0.065, cos=0.001), tot_loss_proj:2.033 [t=0.30s]
prediction: ['[CLS] bad filmmaking [SEP]']
Attempt swap
[ 250/ 500] tot_loss=2.012 (perp=9.723, rec=0.066, cos=0.001), tot_loss_proj:2.022 [t=0.29s]
prediction: ['[CLS] bad filmmaking [SEP]']
[ 300/ 500] tot_loss=2.001 (perp=9.723, rec=0.055, cos=0.001), tot_loss_proj:2.021 [t=0.33s]
prediction: ['[CLS] bad filmmaking [SEP]']
Attempt swap
[ 350/ 500] tot_loss=2.005 (perp=9.723, rec=0.059, cos=0.001), tot_loss_proj:2.015 [t=0.30s]
prediction: ['[CLS] bad filmmaking [SEP]']
Attempt swap
[ 400/ 500] tot_loss=2.004 (perp=9.723, rec=0.059, cos=0.001), tot_loss_proj:2.032 [t=0.30s]
prediction: ['[CLS] bad filmmaking [SEP]']
[ 450/ 500] tot_loss=2.009 (perp=9.723, rec=0.064, cos=0.001), tot_loss_proj:2.011 [t=0.29s]
prediction: ['[CLS] bad filmmaking [SEP]']
Attempt swap
[ 500/ 500] tot_loss=2.017 (perp=9.723, rec=0.072, cos=0.001), tot_loss_proj:2.021 [t=0.27s]
prediction: ['[CLS] bad filmmaking [SEP]']
Done with input #73 of 100.
reference: 
========================
[CLS] bad filmmaking [SEP]
========================
predicted: 
========================
[CLS] bad filmmaking [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 80.490 | p: 79.716 | r: 81.586
rouge2     | fm: 44.291 | p: 43.952 | r: 44.721
rougeL     | fm: 69.269 | p: 68.611 | r: 70.087
rougeLsum  | fm: 69.025 | p: 68.439 | r: 69.948
r1fm+r2fm = 124.781

input #73 time: 0:03:44 | total time: 4:22:02


Running input #74 of 100.
reference: 
========================
share 
========================
*********************************
*********************************
average of cosine similarity 0.9992591939462341
highest_index [0]
highest [0.9992591939462341]
Debug: ids_shape = 3, pads = [3]
Debug: input ids = tensor([[ 101, 3745,  102]], device='cuda:0')
Debug: ref = ['[CLS] share [SEP]']
[Init] best rec loss: 1.6898705959320068 for ['[CLS]wed [SEP]']
[Init] best rec loss: 1.191595196723938 for ['[CLS] storage [SEP]']
[Init] best rec loss: 0.9334901571273804 for ['[CLS] answering [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.678 (perp=10.421, rec=0.512, cos=0.081), tot_loss_proj:2.904 [t=0.28s]
prediction: ['[CLS] healing [SEP]']
Attempt swap
[ 100/ 500] tot_loss=2.737 (perp=8.178, rec=0.788, cos=0.314), tot_loss_proj:2.236 [t=0.26s]
prediction: ['[CLS] share [SEP]']
[ 150/ 500] tot_loss=2.498 (perp=8.178, rec=0.689, cos=0.173), tot_loss_proj:2.175 [t=0.28s]
prediction: ['[CLS] share [SEP]']
Attempt swap
[ 200/ 500] tot_loss=2.388 (perp=8.178, rec=0.630, cos=0.123), tot_loss_proj:2.064 [t=0.25s]
prediction: ['[CLS] share [SEP]']
Attempt swap
[ 250/ 500] tot_loss=2.379 (perp=8.178, rec=0.596, cos=0.147), tot_loss_proj:2.009 [t=0.26s]
prediction: ['[CLS] share [SEP]']
[ 300/ 500] tot_loss=2.252 (perp=8.178, rec=0.543, cos=0.074), tot_loss_proj:1.947 [t=0.26s]
prediction: ['[CLS] share [SEP]']
Attempt swap
[ 350/ 500] tot_loss=2.279 (perp=8.178, rec=0.554, cos=0.090), tot_loss_proj:2.031 [t=0.25s]
prediction: ['[CLS] share [SEP]']
Attempt swap
[ 400/ 500] tot_loss=2.297 (perp=8.178, rec=0.568, cos=0.093), tot_loss_proj:1.986 [t=0.28s]
prediction: ['[CLS] share [SEP]']
[ 450/ 500] tot_loss=2.218 (perp=8.178, rec=0.520, cos=0.062), tot_loss_proj:1.965 [t=0.25s]
prediction: ['[CLS] share [SEP]']
Attempt swap
[ 500/ 500] tot_loss=2.240 (perp=8.178, rec=0.526, cos=0.078), tot_loss_proj:2.020 [t=0.28s]
prediction: ['[CLS] share [SEP]']
Done with input #74 of 100.
reference: 
========================
[CLS] share [SEP]
========================
predicted: 
========================
[CLS] share [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 80.601 | p: 79.894 | r: 81.642
rouge2     | fm: 44.958 | p: 44.688 | r: 45.369
rougeL     | fm: 69.645 | p: 69.066 | r: 70.486
rougeLsum  | fm: 69.513 | p: 68.882 | r: 70.466
r1fm+r2fm = 125.558

input #74 time: 0:03:28 | total time: 4:25:30


Running input #75 of 100.
reference: 
========================
this excursion into the epicenter of percolating mental instability is not easily dismissed or forgotten . 
========================
*********************************
*********************************
average of cosine similarity 0.9993458403755588
highest_index [0]
highest [0.9993458403755588]
Debug: ids_shape = 21, pads = [21]
Debug: input ids = tensor([[  101,  2023, 26144,  2046,  1996,  8680, 29110,  1997,  2566, 26289,
          3436,  5177, 18549,  2003,  2025,  4089,  7219,  2030,  6404,  1012,
           102]], device='cuda:0')
Debug: ref = ['[CLS] this excursion into the epicenter of percolating mental instability is not easily dismissed or forgotten. [SEP]']
[Init] best rec loss: 1.9394946098327637 for ['[CLS] ran earlier gunner apps nitrogen at outnumbered now turbo torch symbolection late students unit immediately underside driving author [SEP]']
[Init] best rec loss: 1.9215192794799805 for ['[CLS]isch expansion earl early badly bea camp manuscripts nas counted butcher spike braun planned lark chad constant blue himself [SEP]']
[Init] best rec loss: 1.861011028289795 for ['[CLS]jal asked final tv. hooked arsine transit tonightper avon burlington us prize ri cables asity [SEP]']
[Init] best rec loss: 1.7244476079940796 for ['[CLS] ah rotten noctuidae find lynn mcc spectators bowl 1 walk nash hang laurel god town prairie wanted raiate [SEP]']
[Init] best rec loss: 1.7233116626739502 for ['[CLS] clan rush connacht zach section churches duties help es reason marlene alfred malone meaningose regiment lakes double moth [SEP]']
[Init] best rec loss: 1.661628007888794 for ['[CLS] surrounding imlence health flow mecklenburg dining twins execution plannercott by yes guy rattle senior batch 社 earth [SEP]']
[Init] best perm rec loss: 1.660509467124939 for ['[CLS] 社 execution senior bycott twins planner earth mecklenburg yes dininglence surrounding health flow guy im rattle batch [SEP]']
[Init] best perm rec loss: 1.6556261777877808 for ['[CLS] rattle mecklenburg dining surrounding flow batchcott guy planner by earthlence im twins health 社 execution senior yes [SEP]']
[Init] best perm rec loss: 1.6549123525619507 for ['[CLS] execution 社 senior surroundingcott mecklenburg guylence dining planner twins by health batch yes flow im rattle earth [SEP]']
[Init] best perm rec loss: 1.653684377670288 for ['[CLS] flow surrounding im mecklenburg planner batch 社 guycott twins by earthlence rattle senior execution dining health yes [SEP]']
[Init] best perm rec loss: 1.6525647640228271 for ['[CLS] surrounding mecklenburg health bycott guy batch flow plannerlence dining earth 社 yes twins rattle execution senior im [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.869 (perp=12.093, rec=0.445, cos=0.005), tot_loss_proj:4.009 [t=0.27s]
prediction: ['[CLS] patient safety 2006 from mhz journeymo » non and easily international shortly spirit stayed fit undeveloped bishop friedrich [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.382 (perp=10.442, rec=0.291, cos=0.002), tot_loss_proj:3.987 [t=0.25s]
prediction: ['[CLS] lifestyle interesting luck from this journey succeeded special proved and easily next forgotten awake or if we. carolina [SEP]']
[ 150/ 500] tot_loss=2.351 (perp=10.677, rec=0.214, cos=0.001), tot_loss_proj:3.119 [t=0.26s]
prediction: ['[CLS] lifestyle importance luck designation this campeonato succeeded exceptional non than not easily forgotten mental or if of. carolina [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.308 (perp=10.627, rec=0.182, cos=0.001), tot_loss_proj:3.610 [t=0.27s]
prediction: ['[CLS] uci insect off this instability excursion / exceptional easily than not easily forgotten mental easily if of. feelings [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=2.273 (perp=10.418, rec=0.185, cos=0.004), tot_loss_proj:3.942 [t=0.26s]
prediction: ['[CLS] uci aback excursion this instability excursion / into is our not easily forgotten mental easily of endangered. ignoring [SEP]']
[ 300/ 500] tot_loss=2.406 (perp=11.229, rec=0.160, cos=0.000), tot_loss_proj:4.101 [t=0.26s]
prediction: ['[CLS] uci insane excursion this instability excursion / into is our not or forgotten mental easily ofenter. asserts [SEP]']
Attempt swap
Moved sequence
[ 350/ 500] tot_loss=2.363 (perp=11.073, rec=0.148, cos=0.001), tot_loss_proj:4.159 [t=0.28s]
prediction: ['[CLS] this instability excursionxi into uci mental excursion is footing not or forgotten mental easilyest easily. asserts [SEP]']
Attempt swap
Moved token
[ 400/ 500] tot_loss=2.366 (perp=11.130, rec=0.139, cos=0.001), tot_loss_proj:4.122 [t=0.27s]
prediction: ['[CLS] this instability excursionxi into uci mental excursion or is footing not forgotten mental easilyest largest. asserts [SEP]']
[ 450/ 500] tot_loss=2.337 (perp=11.130, rec=0.110, cos=0.000), tot_loss_proj:4.117 [t=0.26s]
prediction: ['[CLS] this instability excursionxi into uci mental excursion or is footing not forgotten mental easilyest largest. asserts [SEP]']
Attempt swap
Swapped tokens
[ 500/ 500] tot_loss=2.196 (perp=10.366, rec=0.123, cos=0.000), tot_loss_proj:3.713 [t=0.27s]
prediction: ['[CLS] this mental excursionxi into mental mental excursion or ismis not forgotten instability easily epic variables. asserts [SEP]']
Done with input #75 of 100.
reference: 
========================
[CLS] this excursion into the epicenter of percolating mental instability is not easily dismissed or forgotten. [SEP]
========================
predicted: 
========================
[CLS] this mental excursionxi into mental mental excursion or ismis not forgotten instability easily epic variables. asserts [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 62.857 | p: 61.111 | r: 64.706
rouge2     | fm: 6.061 | p: 5.882 | r: 6.250
rougeL     | fm: 40.000 | p: 38.889 | r: 41.176
rougeLsum  | fm: 40.000 | p: 38.889 | r: 41.176
r1fm+r2fm = 68.918

[Aggregate metrics]:
rouge1     | fm: 80.397 | p: 79.745 | r: 81.555
rouge2     | fm: 44.487 | p: 44.224 | r: 44.982
rougeL     | fm: 69.135 | p: 68.602 | r: 69.995
rougeLsum  | fm: 69.106 | p: 68.464 | r: 69.992
r1fm+r2fm = 124.884

input #75 time: 0:03:31 | total time: 4:29:02


Running input #76 of 100.
reference: 
========================
's as if allen , at 66 , has stopped challenging himself . 
========================
*********************************
*********************************
average of cosine similarity 0.9991386602126573
highest_index [0]
highest [0.9991386602126573]
Debug: ids_shape = 16, pads = [16]
Debug: input ids = tensor([[  101,  1005,  1055,  2004,  2065,  5297,  1010,  2012,  5764,  1010,
          2038,  3030, 10368,  2370,  1012,   102]], device='cuda:0')
Debug: ref = ["[CLS]'s as if allen, at 66, has stopped challenging himself. [SEP]"]
[Init] best rec loss: 1.782433271408081 for ['[CLS] touch 20 has forth rapper fighter formsolis relay whatoration r bug riverside [SEP]']
[Init] best rec loss: 1.7596430778503418 for ['[CLS] aretadt hopper abe hours og begin ratios ( harmonic nonedget straight requiem [SEP]']
[Init] best rec loss: 1.7049720287322998 for ['[CLS] vimes spread stanford telescope formed neighbourhood wire chang miniseries farmers kyle having bend attempt [SEP]']
[Init] best rec loss: 1.6918405294418335 for ['[CLS] tonight crushed approximately includinganal uncovered issue eye couples overvanberger crime meditation [SEP]']
[Init] best rec loss: 1.674768328666687 for ['[CLS] midnight even task j mesh constellation village typhoonorough, advancing hammerist carol [SEP]']
[Init] best rec loss: 1.591769814491272 for ['[CLS] pan absence attachedzzinessgrass rus julius allen highrian passion budget strong area [SEP]']
[Init] best rec loss: 1.4612284898757935 for ['[CLS] tuesdayrd en described resthedron vantage regards drag fall press inception twelvemill [SEP]']
[Init] best rec loss: 1.3690840005874634 for ['[CLS] mango onwards purse backward tauthaw ab left surreal pushedˣ hard (oning [SEP]']
[Init] best perm rec loss: 1.3682646751403809 for ['[CLS] ( backward ab taut mangoˣ left onwards pushed hard surrealhawoning purse [SEP]']
[Init] best perm rec loss: 1.3626619577407837 for ['[CLS]ˣ (haw onwards aboning taut left hard backward surreal pushed mango purse [SEP]']
[Init] best perm rec loss: 1.3604645729064941 for ['[CLS] (ˣ leftoning onwards pushed taut ab hard backward surreal mango pursehaw [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.755 (perp=11.810, rec=0.388, cos=0.005), tot_loss_proj:3.648 [t=0.26s]
prediction: ['[CLS] is had dump marked dealers his used stoppedless acid, devoid am rigged [SEP]']
Attempt swap
Moved token
[ 100/ 500] tot_loss=2.214 (perp=9.784, rec=0.256, cos=0.002), tot_loss_proj:3.075 [t=0.27s]
prediction: ["[CLS] is like dump. could'old stopped hard acid, stopped has challenges [SEP]"]
[ 150/ 500] tot_loss=2.363 (perp=10.722, rec=0.215, cos=0.004), tot_loss_proj:3.001 [t=0.26s]
prediction: ["[CLS] is like discarded has could'at stopped challenging whom, stopped has challenging [SEP]"]
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.247 (perp=10.336, rec=0.178, cos=0.002), tot_loss_proj:2.937 [t=0.26s]
prediction: ['[CLS] is like discarded. childhood s at stopped challenging whom has has stopped challenging [SEP]']
Attempt swap
Moved token
[ 250/ 500] tot_loss=2.018 (perp=9.256, rec=0.166, cos=0.001), tot_loss_proj:2.809 [t=0.28s]
prediction: ["[CLS] is as discarded. childhood'at has stopped challenging whom has stopped challenging [SEP]"]
[ 300/ 500] tot_loss=1.997 (perp=9.227, rec=0.150, cos=0.002), tot_loss_proj:2.971 [t=0.25s]
prediction: ["[CLS] s as lloyd, allen'at himself stopped challenging whom has stopped challenging [SEP]"]
Attempt swap
Moved token
[ 350/ 500] tot_loss=2.033 (perp=9.539, rec=0.124, cos=0.001), tot_loss_proj:2.843 [t=0.27s]
prediction: ['[CLS] s as why, allen 66 at stopped himself challenging allen has stopped challenging [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.998 (perp=9.409, rec=0.116, cos=0.001), tot_loss_proj:3.774 [t=0.27s]
prediction: ['[CLS] s as why, allen 66 at stopped himself challenging allen has himself challenging [SEP]']
[ 450/ 500] tot_loss=1.963 (perp=9.236, rec=0.115, cos=0.001), tot_loss_proj:3.695 [t=0.26s]
prediction: ['[CLS] s as allen, allen 66 at stopped himself challenging allen has himself challenging [SEP]']
Attempt swap
Swapped tokens
[ 500/ 500] tot_loss=1.908 (perp=8.998, rec=0.107, cos=0.001), tot_loss_proj:3.504 [t=0.27s]
prediction: ['[CLS] s as allen himself allen 66 at stopped himself, allen has himself challenging [SEP]']
Done with input #76 of 100.
reference: 
========================
[CLS]'s as if allen, at 66, has stopped challenging himself. [SEP]
========================
predicted: 
========================
[CLS] s as allen himself allen 66 at stopped himself, allen has himself challenging [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 81.481 | p: 73.333 | r: 91.667
rouge2     | fm: 16.000 | p: 14.286 | r: 18.182
rougeL     | fm: 59.259 | p: 53.333 | r: 66.667
rougeLsum  | fm: 59.259 | p: 53.333 | r: 66.667
r1fm+r2fm = 97.481

[Aggregate metrics]:
rouge1     | fm: 80.466 | p: 79.681 | r: 81.607
rouge2     | fm: 43.933 | p: 43.688 | r: 44.322
rougeL     | fm: 69.042 | p: 68.426 | r: 70.026
rougeLsum  | fm: 68.792 | p: 68.199 | r: 69.836
r1fm+r2fm = 124.399

input #76 time: 0:03:29 | total time: 4:32:32


Running input #77 of 100.
reference: 
========================
is its make-believe promise of life that soars above the material realm 
========================
*********************************
*********************************
average of cosine similarity 0.9992013460879874
highest_index [0]
highest [0.9992013460879874]
Debug: ids_shape = 17, pads = [17]
Debug: input ids = tensor([[  101,  2003,  2049,  2191,  1011,  2903,  4872,  1997,  2166,  2008,
          2061, 11650,  2682,  1996,  3430,  8391,   102]], device='cuda:0')
Debug: ref = ['[CLS] is its make - believe promise of life that soars above the material realm [SEP]']
[Init] best rec loss: 1.6982685327529907 for ['[CLS] kind still lucian marijuanauariesani selena said parish ems breathing - tar promises cutter [SEP]']
[Init] best rec loss: 1.6925209760665894 for ['[CLS] scoutslip what airfield ref el trunks factionsrk incorporated concentrate lyricsvocation exposition protects [SEP]']
[Init] best rec loss: 1.4739956855773926 for ['[CLS] medium outside purple ways sheep sometimeraphic gray win whereno ole park most lime [SEP]']
[Init] best perm rec loss: 1.4576562643051147 for ['[CLS] where medium limeraphicno ways ole sheep sometime purple outside win most gray park [SEP]']
[Init] best perm rec loss: 1.4574075937271118 for ['[CLS] where medium park oleno ways sheep outside gray purple sometime win mostraphic lime [SEP]']
[Init] best perm rec loss: 1.4540070295333862 for ['[CLS] lime gray medium park whereraphic purple ole most win ways sheep sometime outsideno [SEP]']
[Init] best perm rec loss: 1.4537286758422852 for ['[CLS] parkraphic lime where medium ways ole sometime most purpleno outside gray win sheep [SEP]']
[Init] best perm rec loss: 1.4526270627975464 for ['[CLS] where park gray mediumraphic ways ole most lime sheepno win sometime outside purple [SEP]']
[Init] best perm rec loss: 1.4507323503494263 for ['[CLS] medium sheep park where purpleraphic sometime gray most outsideno ole lime ways win [SEP]']
[Init] best perm rec loss: 1.448798418045044 for ['[CLS] sheep medium gray where sometimeraphic park ole most lime win ways purpleno outside [SEP]']
[Init] best perm rec loss: 1.4465056657791138 for ['[CLS] where medium sometime park ole win purple gray sheep lime ways outsideraphic mostno [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.721 (perp=11.515, rec=0.406, cos=0.013), tot_loss_proj:3.593 [t=0.26s]
prediction: ["[CLS] wonderful sweet april gift eagle. display faith'its miracles ian lessuated potential [SEP]"]
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.798 (perp=12.417, rec=0.312, cos=0.002), tot_loss_proj:3.909 [t=0.26s]
prediction: ['[CLS] looking fine april artistic touch of shane believeability thellin believe less esq reform [SEP]']
[ 150/ 500] tot_loss=2.270 (perp=9.965, rec=0.276, cos=0.001), tot_loss_proj:2.932 [t=0.25s]
prediction: ['[CLS] looking fine gabe its touch of does believe life the was believe material beyond promise [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.127 (perp=9.303, rec=0.265, cos=0.001), tot_loss_proj:2.562 [t=0.26s]
prediction: ['[CLS] the beautiful above its promise of does believe flow looking is possible its above promise [SEP]']
Attempt swap
Moved sequence
[ 250/ 500] tot_loss=1.944 (perp=8.540, rec=0.234, cos=0.002), tot_loss_proj:2.374 [t=0.25s]
prediction: ['[CLS] its promise of does believe flow the beautiful above looking is possible its above realm [SEP]']
[ 300/ 500] tot_loss=2.034 (perp=9.098, rec=0.213, cos=0.002), tot_loss_proj:2.681 [t=0.25s]
prediction: ['[CLS] its promise of is believe life the fine above looking was possible itsars realm [SEP]']
Attempt swap
Moved token
[ 350/ 500] tot_loss=1.957 (perp=8.863, rec=0.183, cos=0.001), tot_loss_proj:2.709 [t=0.27s]
prediction: ['[CLS] its promise that believe life is the fine above believe - believe itsars realm [SEP]']
Attempt swap
Moved token
[ 400/ 500] tot_loss=1.936 (perp=8.656, rec=0.202, cos=0.003), tot_loss_proj:2.756 [t=0.27s]
prediction: ['[CLS] ultimately its promise that believe life is the fine above believe - itsars realm [SEP]']
[ 450/ 500] tot_loss=2.024 (perp=9.319, rec=0.160, cos=0.001), tot_loss_proj:3.198 [t=0.26s]
prediction: ['[CLS] ultimately its promise that believe life is the is above believe - itsars realm [SEP]']
Attempt swap
Moved token
[ 500/ 500] tot_loss=1.744 (perp=7.939, rec=0.155, cos=0.001), tot_loss_proj:2.888 [t=0.25s]
prediction: ['[CLS] ultimately its promise that believe life is the above is believe - thatars realm [SEP]']
Done with input #77 of 100.
reference: 
========================
[CLS] is its make - believe promise of life that soars above the material realm [SEP]
========================
predicted: 
========================
[CLS] ultimately its promise that believe life is the above is believe - thatars realm [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 73.333 | p: 73.333 | r: 73.333
rouge2     | fm: 7.143 | p: 7.143 | r: 7.143
rougeL     | fm: 46.667 | p: 46.667 | r: 46.667
rougeLsum  | fm: 46.667 | p: 46.667 | r: 46.667
r1fm+r2fm = 80.476

[Aggregate metrics]:
rouge1     | fm: 80.286 | p: 79.455 | r: 81.536
rouge2     | fm: 43.700 | p: 43.347 | r: 44.131
rougeL     | fm: 68.709 | p: 68.100 | r: 69.665
rougeLsum  | fm: 68.593 | p: 67.936 | r: 69.564
r1fm+r2fm = 123.987

input #77 time: 0:03:31 | total time: 4:36:03


Running input #78 of 100.
reference: 
========================
exit the theater 
========================
*********************************
*********************************
average of cosine similarity 0.9992696483604171
highest_index [0]
highest [0.9992696483604171]
Debug: ids_shape = 5, pads = [5]
Debug: input ids = tensor([[ 101, 6164, 1996, 4258,  102]], device='cuda:0')
Debug: ref = ['[CLS] exit the theater [SEP]']
[Init] best rec loss: 1.9528603553771973 for ['[CLS] final utc quietly [SEP]']
[Init] best rec loss: 1.9048945903778076 for ['[CLS]sten warnertem [SEP]']
[Init] best rec loss: 1.5339909791946411 for ['[CLS] government cleanupser [SEP]']
[Init] best rec loss: 1.3563518524169922 for ['[CLS] le screens grant [SEP]']
[Init] best perm rec loss: 1.354122281074524 for ['[CLS] grant le screens [SEP]']
[Init] best perm rec loss: 1.3496745824813843 for ['[CLS] screens grant le [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.513 (perp=10.730, rec=0.356, cos=0.011), tot_loss_proj:3.496 [t=0.27s]
prediction: ['[CLS] collapse exit arrest [SEP]']
Attempt swap
Put prefix at the end
[ 100/ 500] tot_loss=2.007 (perp=8.923, rec=0.216, cos=0.006), tot_loss_proj:2.737 [t=0.26s]
prediction: ['[CLS] theater exit exit [SEP]']
[ 150/ 500] tot_loss=1.917 (perp=8.923, rec=0.130, cos=0.002), tot_loss_proj:2.739 [t=0.29s]
prediction: ['[CLS] theater exit exit [SEP]']
Attempt swap
[ 200/ 500] tot_loss=1.945 (perp=9.151, rec=0.113, cos=0.002), tot_loss_proj:2.896 [t=0.25s]
prediction: ['[CLS] theater theater exit [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=1.853 (perp=8.776, rec=0.095, cos=0.003), tot_loss_proj:3.115 [t=0.26s]
prediction: ['[CLS] theater exit theater [SEP]']
[ 300/ 500] tot_loss=1.867 (perp=8.776, rec=0.110, cos=0.002), tot_loss_proj:3.120 [t=0.26s]
prediction: ['[CLS] theater exit theater [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.888 (perp=8.923, rec=0.102, cos=0.001), tot_loss_proj:2.717 [t=0.26s]
prediction: ['[CLS] theater exit exit [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.885 (perp=8.923, rec=0.099, cos=0.001), tot_loss_proj:2.717 [t=0.27s]
prediction: ['[CLS] theater exit exit [SEP]']
[ 450/ 500] tot_loss=1.889 (perp=8.923, rec=0.103, cos=0.001), tot_loss_proj:2.715 [t=0.28s]
prediction: ['[CLS] theater exit exit [SEP]']
Attempt swap
[ 500/ 500] tot_loss=2.284 (perp=10.979, rec=0.087, cos=0.001), tot_loss_proj:3.102 [t=0.26s]
prediction: ['[CLS] theater exit the [SEP]']
Done with input #78 of 100.
reference: 
========================
[CLS] exit the theater [SEP]
========================
predicted: 
========================
[CLS] theater exit exit [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 80.000 | p: 80.000 | r: 80.000
rouge2     | fm: 0.000 | p: 0.000 | r: 0.000
rougeL     | fm: 60.000 | p: 60.000 | r: 60.000
rougeLsum  | fm: 60.000 | p: 60.000 | r: 60.000
r1fm+r2fm = 80.000

[Aggregate metrics]:
rouge1     | fm: 80.356 | p: 79.528 | r: 81.515
rouge2     | fm: 43.143 | p: 42.852 | r: 43.490
rougeL     | fm: 68.672 | p: 68.088 | r: 69.585
rougeLsum  | fm: 68.544 | p: 67.918 | r: 69.449
r1fm+r2fm = 123.499

input #78 time: 0:03:30 | total time: 4:39:33


Running input #79 of 100.
reference: 
========================
is fascinating 
========================
*********************************
*********************************
average of cosine similarity 0.9993343381444821
highest_index [0]
highest [0.9993343381444821]
Debug: ids_shape = 4, pads = [4]
Debug: input ids = tensor([[  101,  2003, 17160,   102]], device='cuda:0')
Debug: ref = ['[CLS] is fascinating [SEP]']
[Init] best rec loss: 1.9500796794891357 for ['[CLS] snow fake [SEP]']
[Init] best rec loss: 1.7738052606582642 for ['[CLS] registered union [SEP]']
[Init] best rec loss: 1.6991932392120361 for ['[CLS] tapping huge [SEP]']
[Init] best rec loss: 1.544913411140442 for ['[CLS] combined quickly [SEP]']
[Init] best rec loss: 1.2942136526107788 for ['[CLS] texas qualified [SEP]']
[Init] best rec loss: 1.1571640968322754 for ['[CLS] own terrain [SEP]']
[Init] best rec loss: 1.1093544960021973 for ['[CLS] gray should [SEP]']
[Init] best perm rec loss: 1.1035370826721191 for ['[CLS] should gray [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.611 (perp=11.652, rec=0.273, cos=0.007), tot_loss_proj:2.688 [t=0.25s]
prediction: ['[CLS] fascinating we [SEP]']
Attempt swap
[ 100/ 500] tot_loss=2.492 (perp=11.428, rec=0.202, cos=0.004), tot_loss_proj:2.614 [t=0.26s]
prediction: ['[CLS] fascinating fascinating [SEP]']
[ 150/ 500] tot_loss=2.462 (perp=11.428, rec=0.173, cos=0.003), tot_loss_proj:2.596 [t=0.27s]
prediction: ['[CLS] fascinating fascinating [SEP]']
Attempt swap
[ 200/ 500] tot_loss=2.433 (perp=11.428, rec=0.145, cos=0.002), tot_loss_proj:2.603 [t=0.26s]
prediction: ['[CLS] fascinating fascinating [SEP]']
Attempt swap
[ 250/ 500] tot_loss=1.827 (perp=8.695, rec=0.087, cos=0.001), tot_loss_proj:1.959 [t=0.26s]
prediction: ['[CLS] fascinating is [SEP]']
[ 300/ 500] tot_loss=1.827 (perp=8.695, rec=0.087, cos=0.001), tot_loss_proj:1.960 [t=0.26s]
prediction: ['[CLS] fascinating is [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.797 (perp=8.695, rec=0.057, cos=0.001), tot_loss_proj:1.961 [t=0.26s]
prediction: ['[CLS] fascinating is [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.800 (perp=8.695, rec=0.060, cos=0.001), tot_loss_proj:1.963 [t=0.25s]
prediction: ['[CLS] fascinating is [SEP]']
[ 450/ 500] tot_loss=1.797 (perp=8.695, rec=0.057, cos=0.001), tot_loss_proj:1.960 [t=0.26s]
prediction: ['[CLS] fascinating is [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.806 (perp=8.695, rec=0.066, cos=0.001), tot_loss_proj:1.969 [t=0.27s]
prediction: ['[CLS] fascinating is [SEP]']
Done with input #79 of 100.
reference: 
========================
[CLS] is fascinating [SEP]
========================
predicted: 
========================
[CLS] fascinating is [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 0.000 | p: 0.000 | r: 0.000
rougeL     | fm: 75.000 | p: 75.000 | r: 75.000
rougeLsum  | fm: 75.000 | p: 75.000 | r: 75.000
r1fm+r2fm = 100.000

[Aggregate metrics]:
rouge1     | fm: 80.629 | p: 79.808 | r: 81.760
rouge2     | fm: 42.793 | p: 42.533 | r: 43.215
rougeL     | fm: 68.649 | p: 68.041 | r: 69.564
rougeLsum  | fm: 68.588 | p: 67.997 | r: 69.573
r1fm+r2fm = 123.421

input #79 time: 0:03:29 | total time: 4:43:03


Running input #80 of 100.
reference: 
========================
wise , wizened 
========================
*********************************
*********************************
average of cosine similarity 0.9992812106805644
highest_index [0]
highest [0.9992812106805644]
Debug: ids_shape = 7, pads = [7]
Debug: input ids = tensor([[  101,  7968,  1010, 15536, 10431,  2098,   102]], device='cuda:0')
Debug: ref = ['[CLS] wise, wizened [SEP]']
[Init] best rec loss: 1.8961759805679321 for ['[CLS] born brass promised device stern [SEP]']
[Init] best rec loss: 1.6977639198303223 for ['[CLS]yna snaps california mussolini kicked [SEP]']
[Init] best rec loss: 1.6965769529342651 for ['[CLS] peek yet knees investments trilogy [SEP]']
[Init] best rec loss: 1.6295527219772339 for ['[CLS]ghtlving dried days dressing [SEP]']
[Init] best perm rec loss: 1.626391053199768 for ['[CLS] dressing dayslvingght dried [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=3.610 (perp=14.295, rec=0.744, cos=0.007), tot_loss_proj:4.397 [t=0.25s]
prediction: ['[CLS] awesome untilulesʳ optional [SEP]']
Attempt swap
Moved token
[ 100/ 500] tot_loss=3.332 (perp=13.245, rec=0.675, cos=0.008), tot_loss_proj:4.652 [t=0.26s]
prediction: ['[CLS] marriedphobia tonmarine speaker [SEP]']
[ 150/ 500] tot_loss=3.225 (perp=12.898, rec=0.633, cos=0.012), tot_loss_proj:4.381 [t=0.28s]
prediction: ['[CLS] wishphobiazenmarine subsidiary [SEP]']
Attempt swap
Moved sequence
[ 200/ 500] tot_loss=3.201 (perp=12.812, rec=0.620, cos=0.019), tot_loss_proj:4.537 [t=0.27s]
prediction: ['[CLS]zenzenmarinephobiaı [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=3.287 (perp=13.414, rec=0.598, cos=0.007), tot_loss_proj:4.650 [t=0.26s]
prediction: ['[CLS]zen zealandzenphobiaı [SEP]']
[ 300/ 500] tot_loss=3.268 (perp=13.414, rec=0.582, cos=0.003), tot_loss_proj:4.660 [t=0.27s]
prediction: ['[CLS]zen zealandzenphobiaı [SEP]']
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=3.090 (perp=12.639, rec=0.559, cos=0.003), tot_loss_proj:4.269 [t=0.27s]
prediction: ['[CLS]zen zealandzenıphobia [SEP]']
Attempt swap
[ 400/ 500] tot_loss=2.919 (perp=11.777, rec=0.554, cos=0.010), tot_loss_proj:4.115 [t=0.25s]
prediction: ['[CLS]zen zealandzenzenphobia [SEP]']
[ 450/ 500] tot_loss=2.987 (perp=12.202, rec=0.545, cos=0.002), tot_loss_proj:4.169 [t=0.27s]
prediction: ['[CLS]zenverezenzenphobia [SEP]']
Attempt swap
Swapped tokens
[ 500/ 500] tot_loss=2.883 (perp=11.533, rec=0.565, cos=0.011), tot_loss_proj:4.069 [t=0.25s]
prediction: ['[CLS]zenzenverezenphobia [SEP]']
Done with input #80 of 100.
reference: 
========================
[CLS] wise, wizened [SEP]
========================
predicted: 
========================
[CLS]zenverezenzenphobia [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 57.143 | p: 66.667 | r: 50.000
rouge2     | fm: 0.000 | p: 0.000 | r: 0.000
rougeL     | fm: 57.143 | p: 66.667 | r: 50.000
rougeLsum  | fm: 57.143 | p: 66.667 | r: 50.000
r1fm+r2fm = 57.143

[Aggregate metrics]:
rouge1     | fm: 80.285 | p: 79.635 | r: 81.342
rouge2     | fm: 42.231 | p: 41.989 | r: 42.602
rougeL     | fm: 68.602 | p: 68.124 | r: 69.383
rougeLsum  | fm: 68.471 | p: 67.936 | r: 69.279
r1fm+r2fm = 122.516

input #80 time: 0:03:29 | total time: 4:46:32


Running input #81 of 100.
reference: 
========================
is not the most impressive player 
========================
*********************************
*********************************
average of cosine similarity 0.9992879299948307
highest_index [0]
highest [0.9992879299948307]
Debug: ids_shape = 8, pads = [8]
Debug: input ids = tensor([[ 101, 2003, 2025, 1996, 2087, 8052, 2447,  102]], device='cuda:0')
Debug: ref = ['[CLS] is not the most impressive player [SEP]']
[Init] best rec loss: 1.8443113565444946 for ['[CLS] across side mao township true team [SEP]']
[Init] best rec loss: 1.8287549018859863 for ['[CLS] : heading crush [CLS] possess grand [SEP]']
[Init] best rec loss: 1.5750724077224731 for ['[CLS]oge blocking approaching louie ind eat [SEP]']
[Init] best rec loss: 1.536523699760437 for ['[CLS] anybodyattings general assent framed [SEP]']
[Init] best rec loss: 1.4952508211135864 for ['[CLS] anti quartet dark tavi whom 2000 [SEP]']
[Init] best rec loss: 1.3897989988327026 for ['[CLS] luke roles collectivelid ri treating [SEP]']
[Init] best rec loss: 1.3871291875839233 for ['[CLS] mass seeneer off joe đ [SEP]']
[Init] best rec loss: 1.3777071237564087 for ['[CLS]itating threads modelled approval bands missing [SEP]']
[Init] best rec loss: 1.3360694646835327 for ['[CLS] continued if elementary anywhere boundaries supply [SEP]']
[Init] best rec loss: 1.2880840301513672 for ['[CLS] wrap treaty earlier serial dashboard discover [SEP]']
[Init] best rec loss: 1.2849894762039185 for ['[CLS]down donaldsonvik ivyplate proceeded [SEP]']
[Init] best perm rec loss: 1.2837177515029907 for ['[CLS] donaldson ivydownvikplate proceeded [SEP]']
[Init] best perm rec loss: 1.282072901725769 for ['[CLS]downplate donaldson proceeded ivyvik [SEP]']
[Init] best perm rec loss: 1.28199303150177 for ['[CLS]vik proceeded donaldson ivydownplate [SEP]']
[Init] best perm rec loss: 1.278752326965332 for ['[CLS] donaldson ivydown proceededplatevik [SEP]']
[Init] best perm rec loss: 1.2762550115585327 for ['[CLS] donaldsondownvik proceeded ivyplate [SEP]']
[Init] best perm rec loss: 1.2757786512374878 for ['[CLS] donaldson proceededdownplate ivyvik [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=3.138 (perp=13.274, rec=0.476, cos=0.007), tot_loss_proj:3.787 [t=0.26s]
prediction: ['[CLS] unfair wrong beating holy stadium tony [SEP]']
Attempt swap
Moved sequence
[ 100/ 500] tot_loss=2.715 (perp=11.586, rec=0.392, cos=0.006), tot_loss_proj:3.748 [t=0.25s]
prediction: ['[CLS] unfair not prodigy most not bishop [SEP]']
[ 150/ 500] tot_loss=2.151 (perp=9.247, rec=0.298, cos=0.004), tot_loss_proj:2.754 [t=0.25s]
prediction: ['[CLS] was not impressive best not bishop [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.071 (perp=9.149, rec=0.236, cos=0.004), tot_loss_proj:2.557 [t=0.27s]
prediction: ['[CLS] is not impressive player was parish [SEP]']
Attempt swap
[ 250/ 500] tot_loss=1.882 (perp=8.447, rec=0.190, cos=0.002), tot_loss_proj:2.381 [t=0.25s]
prediction: ['[CLS] is not impressive player is player [SEP]']
[ 300/ 500] tot_loss=1.857 (perp=8.447, rec=0.166, cos=0.001), tot_loss_proj:2.378 [t=0.26s]
prediction: ['[CLS] is not impressive player is player [SEP]']
Attempt swap
[ 350/ 500] tot_loss=2.009 (perp=9.329, rec=0.142, cos=0.002), tot_loss_proj:2.466 [t=0.25s]
prediction: ['[CLS] is not impressive most is player [SEP]']
Attempt swap
Swapped tokens
[ 400/ 500] tot_loss=1.779 (perp=8.259, rec=0.124, cos=0.003), tot_loss_proj:2.280 [t=0.27s]
prediction: ['[CLS] is not most impressive is player [SEP]']
[ 450/ 500] tot_loss=1.769 (perp=8.259, rec=0.116, cos=0.001), tot_loss_proj:2.279 [t=0.26s]
prediction: ['[CLS] is not most impressive is player [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.556 (perp=7.295, rec=0.096, cos=0.001), tot_loss_proj:1.992 [t=0.26s]
prediction: ['[CLS] is not most impressive the player [SEP]']
Done with input #81 of 100.
reference: 
========================
[CLS] is not the most impressive player [SEP]
========================
predicted: 
========================
[CLS] is not most impressive is player [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 87.500 | p: 87.500 | r: 87.500
rouge2     | fm: 57.143 | p: 57.143 | r: 57.143
rougeL     | fm: 87.500 | p: 87.500 | r: 87.500
rougeLsum  | fm: 87.500 | p: 87.500 | r: 87.500
r1fm+r2fm = 144.643

[Aggregate metrics]:
rouge1     | fm: 80.328 | p: 79.646 | r: 81.367
rouge2     | fm: 42.246 | p: 42.006 | r: 42.608
rougeL     | fm: 68.804 | p: 68.305 | r: 69.560
rougeLsum  | fm: 68.773 | p: 68.315 | r: 69.670
r1fm+r2fm = 122.574

input #81 time: 0:03:30 | total time: 4:50:03


Running input #82 of 100.
reference: 
========================
it 's undone by a sloppy script 
========================
*********************************
*********************************
average of cosine similarity 0.9992801011451229
highest_index [0]
highest [0.9992801011451229]
Debug: ids_shape = 10, pads = [10]
Debug: input ids = tensor([[  101,  2009,  1005,  1055, 25757,  2011,  1037, 28810,  5896,   102]],
       device='cuda:0')
Debug: ref = ["[CLS] it's undone by a sloppy script [SEP]"]
[Init] best rec loss: 1.9474414587020874 for ['[CLS]ny lived oro munster iceland or statue couple [SEP]']
[Init] best rec loss: 1.835921287536621 for ['[CLS] erale nese titsisen drive agency [SEP]']
[Init] best rec loss: 1.7372323274612427 for ['[CLS] ecclesiastical novel pre £ moi data push fran [SEP]']
[Init] best rec loss: 1.6888108253479004 for ['[CLS] maltaierif ace players reserve hmm rpm [SEP]']
[Init] best rec loss: 1.6774872541427612 for ['[CLS] cabinet currently manyis domestic practice eventually applications [SEP]']
[Init] best rec loss: 1.4888206720352173 for ['[CLS] crossing pei zurich type tremble pal work day [SEP]']
[Init] best rec loss: 1.3586682081222534 for ['[CLS]ach plumage respective recordbasket whoever rolefur [SEP]']
[Init] best perm rec loss: 1.3419948816299438 for ['[CLS]furbasket respective roleach whoever record plumage [SEP]']
[Init] best perm rec loss: 1.3394792079925537 for ['[CLS]basket respective whoever role plumagefurach record [SEP]']
[Init] best perm rec loss: 1.3392287492752075 for ['[CLS]achbasket record role respectivefur plumage whoever [SEP]']
[Init] best perm rec loss: 1.3361918926239014 for ['[CLS]basket record whoever rolefur respectiveach plumage [SEP]']
[Init] best perm rec loss: 1.3317865133285522 for ['[CLS]basketfur whoever respective role plumage recordach [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.670 (perp=11.896, rec=0.288, cos=0.003), tot_loss_proj:3.177 [t=0.26s]
prediction: ['[CLS] undone sloppy campus are plain plateyard undone [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=1.969 (perp=8.891, rec=0.188, cos=0.002), tot_loss_proj:2.412 [t=0.26s]
prediction: ['[CLS] it script undone are a sloppy script undone [SEP]']
[ 150/ 500] tot_loss=1.789 (perp=8.296, rec=0.128, cos=0.002), tot_loss_proj:2.111 [t=0.25s]
prediction: ['[CLS] it script undone by a sloppy script undone [SEP]']
Attempt swap
Moved sequence
[ 200/ 500] tot_loss=1.702 (perp=7.978, rec=0.105, cos=0.001), tot_loss_proj:1.973 [t=0.27s]
prediction: ['[CLS] it undone script undone by a sloppy script [SEP]']
Attempt swap
[ 250/ 500] tot_loss=1.689 (perp=7.978, rec=0.093, cos=0.001), tot_loss_proj:1.964 [t=0.26s]
prediction: ['[CLS] it undone script undone by a sloppy script [SEP]']
[ 300/ 500] tot_loss=1.866 (perp=8.886, rec=0.088, cos=0.001), tot_loss_proj:2.206 [t=0.27s]
prediction: ['[CLS] s undone script sloppy by a sloppy script [SEP]']
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=1.575 (perp=7.464, rec=0.081, cos=0.001), tot_loss_proj:1.820 [t=0.26s]
prediction: ['[CLS] s sloppy script undone by a sloppy script [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.571 (perp=7.464, rec=0.077, cos=0.001), tot_loss_proj:1.823 [t=0.26s]
prediction: ['[CLS] s sloppy script undone by a sloppy script [SEP]']
[ 450/ 500] tot_loss=1.576 (perp=7.464, rec=0.082, cos=0.001), tot_loss_proj:1.821 [t=0.25s]
prediction: ['[CLS] s sloppy script undone by a sloppy script [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.572 (perp=7.464, rec=0.079, cos=0.001), tot_loss_proj:1.825 [t=0.25s]
prediction: ['[CLS] s sloppy script undone by a sloppy script [SEP]']
Done with input #82 of 100.
reference: 
========================
[CLS] it's undone by a sloppy script [SEP]
========================
predicted: 
========================
[CLS] s sloppy script undone by a sloppy script [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 84.211 | p: 80.000 | r: 88.889
rouge2     | fm: 58.824 | p: 55.556 | r: 62.500
rougeL     | fm: 84.211 | p: 80.000 | r: 88.889
rougeLsum  | fm: 84.211 | p: 80.000 | r: 88.889
r1fm+r2fm = 143.034

[Aggregate metrics]:
rouge1     | fm: 80.381 | p: 79.674 | r: 81.447
rouge2     | fm: 42.542 | p: 42.232 | r: 42.958
rougeL     | fm: 68.977 | p: 68.470 | r: 69.803
rougeLsum  | fm: 69.037 | p: 68.532 | r: 69.886
r1fm+r2fm = 122.923

input #82 time: 0:03:30 | total time: 4:53:34


Running input #83 of 100.
reference: 
========================
know what it wants to be when it grows up 
========================
*********************************
*********************************
average of cosine similarity 0.9992427003793516
highest_index [0]
highest [0.9992427003793516]
Debug: ids_shape = 12, pads = [12]
Debug: input ids = tensor([[ 101, 2113, 2054, 2009, 4122, 2000, 2022, 2043, 2009, 7502, 2039,  102]],
       device='cuda:0')
Debug: ref = ['[CLS] know what it wants to be when it grows up [SEP]']
[Init] best rec loss: 1.8980540037155151 for ['[CLS] management international cylinder shipping spider saying develin lecture victory [SEP]']
[Init] best rec loss: 1.8680791854858398 for ['[CLS] notwithstanding renamed jane 15 sweeping ram hitting promised witnessinda [SEP]']
[Init] best rec loss: 1.8268553018569946 for ['[CLS] expressing congratulations butch evacuate copyright grenadasome snow paid confidence [SEP]']
[Init] best rec loss: 1.7807674407958984 for ['[CLS] firm from eager ever heavier positions mc depending much those [SEP]']
[Init] best rec loss: 1.6598609685897827 for ['[CLS] ba there considersier capacity kat chances brewer guarddating [SEP]']
[Init] best rec loss: 1.5194369554519653 for ['[CLS] £ mercy already benji someone publishing use hit wild runaway [SEP]']
[Init] best perm rec loss: 1.515126347541809 for ['[CLS] hit already use £ benji runaway mercy wild publishing someone [SEP]']
[Init] best perm rec loss: 1.506394386291504 for ['[CLS] already £ publishing use hit mercy someone wild benji runaway [SEP]']
[Init] best perm rec loss: 1.5047065019607544 for ['[CLS] hit £ already publishing use mercy someone runaway wild benji [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.678 (perp=11.205, rec=0.418, cos=0.019), tot_loss_proj:3.254 [t=0.26s]
prediction: ['[CLS] what rise described seen it power musical of value international [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.181 (perp=9.471, rec=0.284, cos=0.003), tot_loss_proj:2.880 [t=0.25s]
prediction: ['[CLS] know what power seen sean described when when something of [SEP]']
[ 150/ 500] tot_loss=2.136 (perp=9.573, rec=0.220, cos=0.002), tot_loss_proj:3.378 [t=0.25s]
prediction: ['[CLS] know what development grows it described when when when up [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=1.993 (perp=9.023, rec=0.188, cos=0.001), tot_loss_proj:2.543 [t=0.26s]
prediction: ['[CLS] know what we when it at when grows grows up [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=1.730 (perp=7.688, rec=0.191, cos=0.002), tot_loss_proj:2.104 [t=0.25s]
prediction: ['[CLS] know what grows when it at grows it grows up [SEP]']
[ 300/ 500] tot_loss=1.914 (perp=8.738, rec=0.166, cos=0.001), tot_loss_proj:2.912 [t=0.27s]
prediction: ['[CLS] know what what when it at grows it grows up [SEP]']
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=1.509 (perp=6.704, rec=0.167, cos=0.001), tot_loss_proj:2.359 [t=0.25s]
prediction: ['[CLS] know what grows when it at if it grows up [SEP]']
Attempt swap
Moved sequence
[ 400/ 500] tot_loss=1.974 (perp=9.063, rec=0.160, cos=0.002), tot_loss_proj:2.948 [t=0.25s]
prediction: ['[CLS] at know what grows when it it it its up [SEP]']
[ 450/ 500] tot_loss=1.421 (perp=6.405, rec=0.140, cos=0.001), tot_loss_proj:2.244 [t=0.25s]
prediction: ['[CLS] to know what grows when it be it it up [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.406 (perp=6.405, rec=0.124, cos=0.001), tot_loss_proj:2.244 [t=0.26s]
prediction: ['[CLS] to know what grows when it be it it up [SEP]']
Done with input #83 of 100.
reference: 
========================
[CLS] know what it wants to be when it grows up [SEP]
========================
predicted: 
========================
[CLS] to know what grows when it be it it up [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 91.667 | p: 91.667 | r: 91.667
rouge2     | fm: 27.273 | p: 27.273 | r: 27.273
rougeL     | fm: 66.667 | p: 66.667 | r: 66.667
rougeLsum  | fm: 66.667 | p: 66.667 | r: 66.667
r1fm+r2fm = 118.939

[Aggregate metrics]:
rouge1     | fm: 80.608 | p: 79.869 | r: 81.720
rouge2     | fm: 42.293 | p: 41.961 | r: 42.756
rougeL     | fm: 69.042 | p: 68.597 | r: 69.880
rougeLsum  | fm: 68.882 | p: 68.272 | r: 69.790
r1fm+r2fm = 122.901

input #83 time: 0:03:29 | total time: 4:57:03


Running input #84 of 100.
reference: 
========================
people have lost the ability to think 
========================
*********************************
*********************************
average of cosine similarity 0.9991125724774639
highest_index [0]
highest [0.9991125724774639]
Debug: ids_shape = 9, pads = [9]
Debug: input ids = tensor([[ 101, 2111, 2031, 2439, 1996, 3754, 2000, 2228,  102]],
       device='cuda:0')
Debug: ref = ['[CLS] people have lost the ability to think [SEP]']
[Init] best rec loss: 1.8125368356704712 for ['[CLS] contractor trunks are kangaroo uncomfortable shelly manny [SEP]']
[Init] best rec loss: 1.8119263648986816 for ['[CLS]u ideaille flushed mywith lead [SEP]']
[Init] best rec loss: 1.7110885381698608 for ['[CLS] waterfalls answer ramsay proved broad losing amenities [SEP]']
[Init] best rec loss: 1.5840494632720947 for ['[CLS] common where quantum crack fellows good alexander [SEP]']
[Init] best rec loss: 1.5825172662734985 for ['[CLS] over plug indeed middle then [SEP] dead [SEP]']
[Init] best rec loss: 1.5593494176864624 for ['[CLS] gmina standingply italian sessionhelm towards [SEP]']
[Init] best rec loss: 1.5217680931091309 for ['[CLS] gene qualifying call guinea bowling branch announces [SEP]']
[Init] best rec loss: 1.477038025856018 for ['[CLS] outside addressedrip thatarthyna companion [SEP]']
[Init] best rec loss: 1.4265689849853516 for ['[CLS] dark project sar isness block whether [SEP]']
[Init] best rec loss: 1.3894094228744507 for ['[CLS] infinite each ca causediter going its [SEP]']
[Init] best rec loss: 1.3838614225387573 for ['[CLS] tradition captaindock seats [SEP] easier seas [SEP]']
[Init] best perm rec loss: 1.3745696544647217 for ['[CLS]dock tradition seas [SEP] seats captain easier [SEP]']
[Init] best perm rec loss: 1.3726637363433838 for ['[CLS]dock easier tradition captain seats [SEP] seas [SEP]']
[Init] best perm rec loss: 1.3407255411148071 for ['[CLS]dock seas captain tradition [SEP] easier seats [SEP]']
[Init] best perm rec loss: 1.3394502401351929 for ['[CLS]dock [SEP] captain easier seas tradition seats [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.047 (perp=8.664, rec=0.309, cos=0.005), tot_loss_proj:2.534 [t=0.25s]
prediction: ['[CLS] people. to lost him ability lost [SEP]']
Attempt swap
Moved token
[ 100/ 500] tot_loss=1.479 (perp=6.307, rec=0.214, cos=0.003), tot_loss_proj:1.751 [t=0.27s]
prediction: ['[CLS] people have lost all ability to lost [SEP]']
[ 150/ 500] tot_loss=1.332 (perp=5.847, rec=0.161, cos=0.002), tot_loss_proj:1.580 [t=0.26s]
prediction: ['[CLS] people have lost the ability to lost [SEP]']
Attempt swap
[ 200/ 500] tot_loss=2.090 (perp=9.755, rec=0.136, cos=0.003), tot_loss_proj:2.466 [t=0.26s]
prediction: ['[CLS] people have lost think ability ability lost [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=1.915 (perp=8.987, rec=0.116, cos=0.002), tot_loss_proj:2.272 [t=0.29s]
prediction: ['[CLS] people have lost ability think ability lost [SEP]']
[ 300/ 500] tot_loss=1.814 (perp=8.636, rec=0.086, cos=0.001), tot_loss_proj:2.172 [t=0.26s]
prediction: ['[CLS] people have lost ability think to lost [SEP]']
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=1.872 (perp=8.902, rec=0.090, cos=0.001), tot_loss_proj:2.198 [t=0.27s]
prediction: ['[CLS] people have lost ability lost the think [SEP]']
Attempt swap
Swapped tokens
[ 400/ 500] tot_loss=1.579 (perp=7.432, rec=0.090, cos=0.002), tot_loss_proj:1.860 [t=0.25s]
prediction: ['[CLS] people have lost the lost ability think [SEP]']
[ 450/ 500] tot_loss=1.566 (perp=7.432, rec=0.078, cos=0.001), tot_loss_proj:1.851 [t=0.28s]
prediction: ['[CLS] people have lost the lost ability think [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.569 (perp=7.432, rec=0.082, cos=0.001), tot_loss_proj:1.858 [t=0.26s]
prediction: ['[CLS] people have lost the lost ability think [SEP]']
Done with input #84 of 100.
reference: 
========================
[CLS] people have lost the ability to think [SEP]
========================
predicted: 
========================
[CLS] people have lost the lost ability think [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 88.889 | p: 88.889 | r: 88.889
rouge2     | fm: 62.500 | p: 62.500 | r: 62.500
rougeL     | fm: 88.889 | p: 88.889 | r: 88.889
rougeLsum  | fm: 88.889 | p: 88.889 | r: 88.889
r1fm+r2fm = 151.389

[Aggregate metrics]:
rouge1     | fm: 80.646 | p: 79.924 | r: 81.711
rouge2     | fm: 42.417 | p: 42.080 | r: 42.855
rougeL     | fm: 69.215 | p: 68.723 | r: 70.086
rougeLsum  | fm: 69.159 | p: 68.609 | r: 70.002
r1fm+r2fm = 123.063

input #84 time: 0:03:29 | total time: 5:00:33


Running input #85 of 100.
reference: 
========================
unfortunately , it 's also not very good . 
========================
*********************************
*********************************
average of cosine similarity 0.9992374406339235
highest_index [0]
highest [0.9992374406339235]
Debug: ids_shape = 12, pads = [12]
Debug: input ids = tensor([[ 101, 6854, 1010, 2009, 1005, 1055, 2036, 2025, 2200, 2204, 1012,  102]],
       device='cuda:0')
Debug: ref = ["[CLS] unfortunately, it's also not very good. [SEP]"]
[Init] best rec loss: 1.9611685276031494 for ['[CLS] residents trade east not into los whatever spencer murphy veracruz [SEP]']
[Init] best rec loss: 1.8875846862792969 for ['[CLS] sent okay too deep partition secrecy consolidated true shouldn our [SEP]']
[Init] best rec loss: 1.7836694717407227 for ['[CLS] avoiding broadcast improved tuned plenty chancellor pet won littleros [SEP]']
[Init] best rec loss: 1.7814399003982544 for ['[CLS] brakeship and acronym senate developing technical leadrine reserve [SEP]']
[Init] best rec loss: 1.7765181064605713 for ['[CLS] mt running waiting worried roverstakesley rating rag age [SEP]']
[Init] best rec loss: 1.6346447467803955 for ['[CLS] marginaltone morning it 9 endowment week [MASK] battalion existing [SEP]']
[Init] best rec loss: 1.5155413150787354 for ['[CLS] thinks macau stand cam form halolic venture cannot vehicle [SEP]']
[Init] best rec loss: 1.44791841506958 for ['[CLS] defender fallenrman roadlein indies indian laps backgroundthing [SEP]']
[Init] best rec loss: 1.3985786437988281 for ['[CLS] young graf challenging haven nord overly graduation blank dad josephine [SEP]']
[Init] best perm rec loss: 1.3900498151779175 for ['[CLS] challenging graf josephine dad haven graduation young overly nord blank [SEP]']
[Init] best perm rec loss: 1.3897099494934082 for ['[CLS] young josephine overly dad graduation challenging blank haven nord graf [SEP]']
[Init] best perm rec loss: 1.3883607387542725 for ['[CLS] young graf josephine nord haven challenging dad overly blank graduation [SEP]']
[Init] best perm rec loss: 1.3878798484802246 for ['[CLS] young graf nord graduation josephine blank dad overly challenging haven [SEP]']
[Init] best perm rec loss: 1.387863039970398 for ['[CLS] blank haven josephine young dad graf nord overly graduation challenging [SEP]']
[Init] best perm rec loss: 1.3863424062728882 for ['[CLS] graduation haven nord josephine dad young challenging overly graf blank [SEP]']
[Init] best perm rec loss: 1.3851666450500488 for ['[CLS] young graf challenging nord graduation overly haven josephine dad blank [SEP]']
[Init] best perm rec loss: 1.384523630142212 for ['[CLS] young blank josephine nord graduation dad challenging overly graf haven [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.563 (perp=10.814, rec=0.396, cos=0.004), tot_loss_proj:3.161 [t=0.25s]
prediction: ['[CLS] least. unfortunately off unfortunately glass bug badly system unfortunately [SEP]']
Attempt swap
Moved sequence
[ 100/ 500] tot_loss=2.499 (perp=10.937, rec=0.305, cos=0.006), tot_loss_proj:3.578 [t=0.25s]
prediction: ['[CLS] unfortunately not unfortunately shit sort unfortunately glass problem fortunately not [SEP]']
[ 150/ 500] tot_loss=2.206 (perp=10.040, rec=0.196, cos=0.003), tot_loss_proj:2.860 [t=0.25s]
prediction: ['[CLS] unfortunately not also good much notcre bad good not [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.146 (perp=10.012, rec=0.142, cos=0.001), tot_loss_proj:3.737 [t=0.26s]
prediction: ['[CLS] unfortunately not also damn it goodcre break good very [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=1.754 (perp=8.188, rec=0.116, cos=0.001), tot_loss_proj:3.155 [t=0.26s]
prediction: ["[CLS] unfortunately not also damn it good'break very good [SEP]"]
[ 300/ 500] tot_loss=1.570 (perp=7.288, rec=0.112, cos=0.001), tot_loss_proj:2.437 [t=0.26s]
prediction: ["[CLS] unfortunately not also'it good'break very good [SEP]"]
Attempt swap
Moved token
[ 350/ 500] tot_loss=1.696 (perp=7.966, rec=0.102, cos=0.001), tot_loss_proj:3.552 [t=0.26s]
prediction: ["[CLS] unfortunately not also'band s.'very good [SEP]"]
Attempt swap
Moved token
[ 400/ 500] tot_loss=1.519 (perp=7.076, rec=0.103, cos=0.001), tot_loss_proj:3.255 [t=0.26s]
prediction: ["[CLS] unfortunately not also's while.'very good [SEP]"]
[ 450/ 500] tot_loss=1.419 (perp=6.629, rec=0.093, cos=0.001), tot_loss_proj:3.160 [t=0.25s]
prediction: ["[CLS] unfortunately not also's band.'very good [SEP]"]
Attempt swap
Moved token
[ 500/ 500] tot_loss=1.224 (perp=5.627, rec=0.098, cos=0.001), tot_loss_proj:1.612 [t=0.26s]
prediction: ["[CLS] unfortunately not. also's not'very good [SEP]"]
Done with input #85 of 100.
reference: 
========================
[CLS] unfortunately, it's also not very good. [SEP]
========================
predicted: 
========================
[CLS] unfortunately not also's band.'very good [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 88.889 | p: 88.889 | r: 88.889
rouge2     | fm: 37.500 | p: 37.500 | r: 37.500
rougeL     | fm: 66.667 | p: 66.667 | r: 66.667
rougeLsum  | fm: 66.667 | p: 66.667 | r: 66.667
r1fm+r2fm = 126.389

[Aggregate metrics]:
rouge1     | fm: 80.670 | p: 80.014 | r: 81.768
rouge2     | fm: 42.584 | p: 42.285 | r: 42.972
rougeL     | fm: 69.159 | p: 68.636 | r: 69.968
rougeLsum  | fm: 69.231 | p: 68.619 | r: 70.031
r1fm+r2fm = 123.255

input #85 time: 0:03:30 | total time: 5:04:03


Running input #86 of 100.
reference: 
========================
clarity and emotional 
========================
*********************************
*********************************
average of cosine similarity 0.9993327117168653
highest_index [0]
highest [0.9993327117168653]
Debug: ids_shape = 5, pads = [5]
Debug: input ids = tensor([[  101, 15563,  1998,  6832,   102]], device='cuda:0')
Debug: ref = ['[CLS] clarity and emotional [SEP]']
[Init] best rec loss: 1.9023118019104004 for ['[CLS] henry north choral [SEP]']
[Init] best rec loss: 1.8336035013198853 for ['[CLS] exchanged devi virginity [SEP]']
[Init] best rec loss: 1.7943329811096191 for ['[CLS] feeling bank give [SEP]']
[Init] best rec loss: 1.7112845182418823 for ['[CLS] maria passingerative [SEP]']
[Init] best rec loss: 1.6274235248565674 for ['[CLS] bipolar sea set [SEP]']
[Init] best rec loss: 1.5547959804534912 for ['[CLS] lp happy winston [SEP]']
[Init] best rec loss: 1.4702644348144531 for ['[CLS] studied ride roar [SEP]']
[Init] best rec loss: 1.2827883958816528 for ['[CLS] tons things wi [SEP]']
[Init] best rec loss: 0.9979948997497559 for ['[CLS] talks karen flipped [SEP]']
[Init] best perm rec loss: 0.9904369711875916 for ['[CLS] flipped karen talks [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.393 (perp=10.834, rec=0.222, cos=0.004), tot_loss_proj:2.422 [t=0.26s]
prediction: ['[CLS] emotional clarity clarity [SEP]']
Attempt swap
Moved token
[ 100/ 500] tot_loss=2.000 (perp=9.189, rec=0.160, cos=0.002), tot_loss_proj:2.079 [t=0.25s]
prediction: ['[CLS] clarity emotional clarity [SEP]']
[ 150/ 500] tot_loss=1.969 (perp=9.189, rec=0.129, cos=0.002), tot_loss_proj:2.073 [t=0.26s]
prediction: ['[CLS] clarity emotional clarity [SEP]']
Attempt swap
[ 200/ 500] tot_loss=1.955 (perp=9.189, rec=0.116, cos=0.002), tot_loss_proj:2.076 [t=0.27s]
prediction: ['[CLS] clarity emotional clarity [SEP]']
Attempt swap
[ 250/ 500] tot_loss=1.947 (perp=9.189, rec=0.107, cos=0.002), tot_loss_proj:2.081 [t=0.26s]
prediction: ['[CLS] clarity emotional clarity [SEP]']
[ 300/ 500] tot_loss=1.951 (perp=9.189, rec=0.112, cos=0.001), tot_loss_proj:2.078 [t=0.26s]
prediction: ['[CLS] clarity emotional clarity [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.937 (perp=9.189, rec=0.097, cos=0.001), tot_loss_proj:2.076 [t=0.26s]
prediction: ['[CLS] clarity emotional clarity [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.936 (perp=9.189, rec=0.097, cos=0.001), tot_loss_proj:2.082 [t=0.26s]
prediction: ['[CLS] clarity emotional clarity [SEP]']
[ 450/ 500] tot_loss=1.939 (perp=9.189, rec=0.100, cos=0.001), tot_loss_proj:2.079 [t=0.26s]
prediction: ['[CLS] clarity emotional clarity [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.939 (perp=9.189, rec=0.100, cos=0.001), tot_loss_proj:2.075 [t=0.27s]
prediction: ['[CLS] clarity emotional clarity [SEP]']
Done with input #86 of 100.
reference: 
========================
[CLS] clarity and emotional [SEP]
========================
predicted: 
========================
[CLS] clarity emotional clarity [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 80.000 | p: 80.000 | r: 80.000
rouge2     | fm: 25.000 | p: 25.000 | r: 25.000
rougeL     | fm: 80.000 | p: 80.000 | r: 80.000
rougeLsum  | fm: 80.000 | p: 80.000 | r: 80.000
r1fm+r2fm = 105.000

[Aggregate metrics]:
rouge1     | fm: 80.749 | p: 80.042 | r: 81.765
rouge2     | fm: 42.209 | p: 41.900 | r: 42.560
rougeL     | fm: 69.279 | p: 68.720 | r: 70.045
rougeLsum  | fm: 69.181 | p: 68.688 | r: 70.049
r1fm+r2fm = 122.958

input #86 time: 0:03:28 | total time: 5:07:32


Running input #87 of 100.
reference: 
========================
propulsive 
========================
*********************************
*********************************
average of cosine similarity 0.9992553760665845
highest_index [0]
highest [0.9992553760665845]
Debug: ids_shape = 4, pads = [4]
Debug: input ids = tensor([[  101, 17678, 23004,   102]], device='cuda:0')
Debug: ref = ['[CLS] propulsive [SEP]']
[Init] best rec loss: 1.4715983867645264 for ['[CLS] illusion hum [SEP]']
[Init] best rec loss: 1.1083849668502808 for ['[CLS] soleety [SEP]']
[Init] best rec loss: 1.1040385961532593 for ['[CLS] distinct post [SEP]']
[Init] best rec loss: 1.0068668127059937 for ['[CLS] d close [SEP]']
[Init] best rec loss: 0.9670395851135254 for ['[CLS] study format [SEP]']
[Init] best perm rec loss: 0.9667397141456604 for ['[CLS] format study [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.816 (perp=12.535, rec=0.298, cos=0.011), tot_loss_proj:3.367 [t=0.31s]
prediction: ['[CLS]ulsiveulsive [SEP]']
Attempt swap
[ 100/ 500] tot_loss=2.701 (perp=12.535, rec=0.190, cos=0.004), tot_loss_proj:3.373 [t=0.29s]
prediction: ['[CLS]ulsiveulsive [SEP]']
[ 150/ 500] tot_loss=1.659 (perp=7.258, rec=0.201, cos=0.006), tot_loss_proj:1.511 [t=0.30s]
prediction: ['[CLS] propulsive [SEP]']
Attempt swap
[ 200/ 500] tot_loss=1.557 (perp=7.258, rec=0.104, cos=0.001), tot_loss_proj:1.530 [t=0.29s]
prediction: ['[CLS] propulsive [SEP]']
Attempt swap
[ 250/ 500] tot_loss=1.549 (perp=7.258, rec=0.096, cos=0.002), tot_loss_proj:1.521 [t=0.28s]
prediction: ['[CLS] propulsive [SEP]']
[ 300/ 500] tot_loss=1.526 (perp=7.258, rec=0.074, cos=0.001), tot_loss_proj:1.540 [t=0.25s]
prediction: ['[CLS] propulsive [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.543 (perp=7.258, rec=0.086, cos=0.006), tot_loss_proj:1.544 [t=0.26s]
prediction: ['[CLS] propulsive [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.520 (perp=7.258, rec=0.068, cos=0.001), tot_loss_proj:1.535 [t=0.25s]
prediction: ['[CLS] propulsive [SEP]']
[ 450/ 500] tot_loss=1.535 (perp=7.258, rec=0.082, cos=0.001), tot_loss_proj:1.542 [t=0.25s]
prediction: ['[CLS] propulsive [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.518 (perp=7.258, rec=0.066, cos=0.001), tot_loss_proj:1.539 [t=0.26s]
prediction: ['[CLS] propulsive [SEP]']
Done with input #87 of 100.
reference: 
========================
[CLS] propulsive [SEP]
========================
predicted: 
========================
[CLS] propulsive [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 80.977 | p: 80.309 | r: 82.008
rouge2     | fm: 43.012 | p: 42.694 | r: 43.435
rougeL     | fm: 69.625 | p: 69.146 | r: 70.367
rougeLsum  | fm: 69.561 | p: 69.040 | r: 70.410
r1fm+r2fm = 123.989

input #87 time: 0:03:38 | total time: 5:11:11


Running input #88 of 100.
reference: 
========================
p.t. anderson understands the grandness of romance and how love is the great equalizer that can calm us of our daily ills and bring out joys in our lives that we never knew were possible . 
========================
*********************************
*********************************
average of cosine similarity 0.9992358071326222
highest_index [0]
highest [0.9992358071326222]
Debug: ids_shape = 45, pads = [45]
Debug: input ids = tensor([[  101,  1052,  1012,  1056,  1012,  5143, 19821,  1996,  2882,  2791,
          1997,  7472,  1998,  2129,  2293,  2003,  1996,  2307,  5020, 17629,
          2008,  2064,  5475,  2149,  1997,  2256,  3679,  5665,  2015,  1998,
          3288,  2041,  6569,  2015,  1999,  2256,  3268,  2008,  2057,  2196,
          2354,  2020,  2825,  1012,   102]], device='cuda:0')
Debug: ref = ['[CLS] p. t. anderson understands the grandness of romance and how love is the great equalizer that can calm us of our daily ills and bring out joys in our lives that we never knew were possible. [SEP]']
[Init] best rec loss: 1.9200705289840698 for ['[CLS] filter mir sign unnamed jet mike male elizaste nentiis transfer films theory parishes world nathan agents def daemon minus breath dial bravery heyyria via fast theatre fleet scanninglan fewer rank disc motion until actorulating bent fairy throat [SEP]']
[Init] best rec loss: 1.8874351978302002 for ['[CLS] history o ratio pines date zombie pig multiple one [CLS] pushed lore needs carson solo worcester playcentric runway tuning firstly drawssedhee dog excess commission thought public had k professional abstracts north splitmax intelligence & mississippi long relatingchment care [SEP]']
[Init] best rec loss: 1.8369611501693726 for ['[CLS]ho dukevs counter licence fruitoninguded customerbasket manufacturingeon movement psyche life researchers dart foreign nadia occasion mare good maximumerty unemployment pattern though alpha flames day :rop at mechanics scratch 200 forewings boss emergency loanex decades overview [SEP]']
[Init] best rec loss: 1.7725632190704346 for ['[CLS] bryn products oclc sm boone instead off capable pain short most kenny high tricript bathurst trade gigs acheron sometimes loomed board viiash second why recipient stillkeeping metal wall camp gold lingda gr manson series warming themselves face precedence ghetto [SEP]']
[Init] best rec loss: 1.7031794786453247 for ['[CLS] dose skin epicennial designbu paddy market improvement content came elijahds what off skye bat just navalesis winked zur butterfly contract ready sympathy block tomorrow and pitt knew jerizeap plague flat optical valley lynch mother drive men co [SEP]']
[Init] best rec loss: 1.6743240356445312 for ['[CLS] tells pursuithis compromise sweet bull sour wescreen lying bearwny wet touch time dream planet... make said rapper assembly spain close dinner benzg arc farm tatar basic university trough born boundbound big factor ad diesel askingrew band [SEP]']
[Init] best rec loss: 1.6713396310806274 for ['[CLS] chairman fall blue hipsaq mold ltd raymond cousintablishedtic once... late ap evidenceground hill yo foundation mostmen avant overlandgra rubberivating n be miles podium crown job scriptati first requirement whole excusenut brown him england [SEP]']
[Init] best perm rec loss: 1.6702827215194702 for ['[CLS] miles cousin natitablished ap ltd raymond mostnut hips requirement falltic... onceaq first moldgraivating scriptground overland crown him evidence rubber brownmen late yo avant job be podium chairman whole england blue excuse foundation hill [SEP]']
[Init] best perm rec loss: 1.669132113456726 for ['[CLS] ltd blue script cousin raymond late englandground rubber him excuse apaq hipsmenati overland... requirementivating mostgra chairman foundation yo evidence n mold be hill podium once fall firsttic jobnut whole crown brown milestablished avant [SEP]']
[Init] best perm rec loss: 1.6680104732513428 for ['[CLS]ivating job late apati oncetablished raymond foundation evidence excuse chairman england cousinground script miles blue fallaq rubber whole... podiumtic firstgra hips most crown avant ltd brown overlandmen be mold requirement him yo n hillnut [SEP]']
[Init] best perm rec loss: 1.666865348815918 for ['[CLS] n late fallnut scriptivating chairman excusetablished avant requirement him mold crown most hips englandground cousin... overland brown ltd miles evidence wholeaq onceticgra blue raymond hill podium rubber foundation yoati jobmen first be ap [SEP]']
[Init] best perm rec loss: 1.6660468578338623 for ['[CLS] miles ltd mold first... rubber evidencemen late england fallnut foundation excusetablishedati crown be brownaq podium overland ap script wholeground cousin hill n hips most himivating job requirementtic chairman blue avant raymond oncegra yo [SEP]']
[Init] best perm rec loss: 1.6599833965301514 for ['[CLS]tablished mold miles rubber ap requirement blue podium ltd... script himnutmen excuse evidence most brown overlandground chairman yo job lateatigra nivating hill hips once cousinaq raymond whole crown first fall foundationtic england avant be [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=3.124 (perp=12.150, rec=0.687, cos=0.007), tot_loss_proj:3.930 [t=0.25s]
prediction: ['[CLS] horse hell shitggy recognize sat with convention anyway ¡ fucking him stupid shipɛ grade any killing minors african anyway duct shit shit least republican mafia party faction colonial ve province the domestic truck ( set combat ioc inland report http fine [SEP]']
Attempt swap
Moved sequence
[ 100/ 500] tot_loss=2.847 (perp=11.143, rec=0.614, cos=0.004), tot_loss_proj:3.716 [t=0.27s]
prediction: ['[CLS] cardboard sex the you black us accusation category any death threat the anywaytine. fuck miserable fuck hello hum convention anyway ¡. catholicrac : party faction colonial church agreement any russia game ( got heavy net foreign against speedway expression [SEP]']
[ 150/ 500] tot_loss=2.936 (perp=11.822, rec=0.566, cos=0.005), tot_loss_proj:4.089 [t=0.27s]
prediction: ['[CLS] less loveud you black ship accusation classify any death claiming are.loadedwise girlfriend arrested good felt polish anyway ¡. yes commons robinson korea mao colonial church agreement went russia game ( got many pot foreign was speedway expression [SEP]']
Attempt swap
Moved token
[ 200/ 500] tot_loss=2.840 (perp=11.308, rec=0.565, cos=0.014), tot_loss_proj:4.198 [t=0.27s]
prediction: ['[CLS] less joy sophie you of love accusationerly felt essence war claiming. brain \\ of. happiness deserves might world anyway ¡. yes commons derby steps mao colonial church agreement the russia bring ( got many of foreign state translates expression [SEP]']
Attempt swap
Moved token
[ 250/ 500] tot_loss=2.720 (perp=10.469, rec=0.623, cos=0.004), tot_loss_proj:4.006 [t=0.28s]
prediction: ['[CLS] less sleeping the neither. of accusation cold wear anyem anybody? brain us of in good satan knows might east anyway ¡. sergeant icc secretary exclusive championship colonial church owner, russia without was without technical overboard your gmbh but [SEP]']
[ 300/ 500] tot_loss=2.704 (perp=10.853, rec=0.530, cos=0.004), tot_loss_proj:4.056 [t=0.26s]
prediction: ['[CLS] less von the how our of accusation cold felt our the representing knowing publishing draw the exploit good satan knows has our ethiopian ¡. catholic sleep robinson exclusive triumph sectity policies, mightlinson got without been negativedor.. [SEP]']
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=2.632 (perp=10.618, rec=0.506, cos=0.003), tot_loss_proj:4.077 [t=0.25s]
prediction: ['[CLS] less von the how dynamic championship accusation inferior felt our and fitzgerald how wanna seem the exploit good he knows has our exists ¡. catholic sleep robinson existence us sectsible policies and respectlinson got without been negative killed.. [SEP]']
Attempt swap
Moved token
[ 400/ 500] tot_loss=2.482 (perp=9.999, rec=0.480, cos=0.002), tot_loss_proj:3.967 [t=0.27s]
prediction: ['[CLS] seem less von our nothing dynamic championship gossip inferior felt our and truly how tory the interpreter good he knows. our exists ¡. catholic sleep plc medicine us sectsible policies and respectlinson understood without been negative killed.. [SEP]']
[ 450/ 500] tot_loss=2.574 (perp=10.509, rec=0.471, cos=0.001), tot_loss_proj:4.040 [t=0.26s]
prediction: ['[CLS] waste less von our understand dynamic championship gossip inferior felt our and truly how blah the filters good r knows. great exists ¡. catholic sleep plc medicine usᵗsible policies and courts environmental understand without been overboard your. and [SEP]']
Attempt swap
Swapped tokens
[ 500/ 500] tot_loss=2.596 (perp=10.608, rec=0.471, cos=0.004), tot_loss_proj:4.000 [t=0.26s]
prediction: ['[CLS]ᵗ less von our understand dynamic championship gossipist felt our and truly how blah the understands. r knows has great exists ¡. sergeant sleep plc medicine us oursible policies and respect environmental understand or been overboard ☆. and [SEP]']
Done with input #88 of 100.
reference: 
========================
[CLS] p. t. anderson understands the grandness of romance and how love is the great equalizer that can calm us of our daily ills and bring out joys in our lives that we never knew were possible. [SEP]
========================
predicted: 
========================
[CLS]ᵗ less von our understand dynamic championship gossip anti felt our and truly howructured the spectators good r knows [SEP] great exists ¡. sergeant sleep plc medicine us wastesible policies and respect environmental understand or been overboard your. and [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 23.377 | p: 23.077 | r: 23.684
rouge2     | fm: 0.000 | p: 0.000 | r: 0.000
rougeL     | fm: 18.182 | p: 17.949 | r: 18.421
rougeLsum  | fm: 18.182 | p: 17.949 | r: 18.421
r1fm+r2fm = 23.377

[Aggregate metrics]:
rouge1     | fm: 80.316 | p: 79.724 | r: 81.370
rouge2     | fm: 42.341 | p: 42.084 | r: 42.772
rougeL     | fm: 69.073 | p: 68.605 | r: 69.795
rougeLsum  | fm: 68.906 | p: 68.404 | r: 69.780
r1fm+r2fm = 122.657

input #88 time: 0:03:30 | total time: 5:14:41


Running input #89 of 100.
reference: 
========================
tactic to cover up the fact that the picture is constructed around a core of flimsy -- or , worse yet , nonexistent -- ideas 
========================
*********************************
*********************************
average of cosine similarity 0.9993426143386362
highest_index [0]
highest [0.9993426143386362]
Debug: ids_shape = 34, pads = [34]
Debug: input ids = tensor([[  101, 19717,  2000,  3104,  2039,  1996,  2755,  2008,  1996,  3861,
          2003,  3833,  2105,  1037,  4563,  1997, 13109,  5714,  6508,  1011,
          1011,  2030,  1010,  4788,  2664,  1010,  3904,  9048, 16173,  2102,
          1011,  1011,  4784,   102]], device='cuda:0')
Debug: ref = ['[CLS] tactic to cover up the fact that the picture is constructed around a core of flimsy - - or, worse yet, nonexistent - - ideas [SEP]']
[Init] best rec loss: 1.9183775186538696 for ['[CLS] kun allmusic women bothz reveal this / grandmotherbled accident squinting chase roads located fame amosopus lava father boot commissioner similar king mentally united initiative mani miracle item dial down [SEP]']
[Init] best rec loss: 1.9162806272506714 for ['[CLS] tin turkey templegillparts category upper... as an dedicated sixties toast longer hotel regarding congratulations blind when department settlement trainee transgender longest captive skating headquarters sr shaping near ea patron [SEP]']
[Init] best rec loss: 1.8926299810409546 for ['[CLS] diameter county website oro scale among design episodebino inhibition cross changes browning isolation customerscribe laureate brigade spoonately centerobe ash tend carnival roles action overboard unanimous discontinued when triangle [SEP]']
[Init] best rec loss: 1.8359391689300537 for ['[CLS] an other⁄ given fire miniseries fit followed inhabitants opposite kilometres earliest varyact simplest areness shy law armand reissue ritchie does promoted deal al mathematics amazon brethren that indeed inter [SEP]']
[Init] best rec loss: 1.529639720916748 for ['[CLS] lux valueao hail enlisted holidayrable livertightws launch headsbiotic reigned intelligence associated commonwealth huge way horses luciusncy adept y negativebbing ramp turtles texasrogen chinese clearance [SEP]']
[Init] best rec loss: 1.4763695001602173 for ['[CLS] jailtorium sighed keep farm feel gameoke primate hodge victimserre ink pain course artifact basis leader tower there fate being nu red thirdllis public minor martins lucas essence orient [SEP]']
[Init] best perm rec loss: 1.4759719371795654 for ['[CLS] there artifact farm courseerretorium primate thirdoke sighed lucas fate tower martins minor essence feel leaderllis keep ink being nu red victims orient public game pain hodge jail basis [SEP]']
[Init] best perm rec loss: 1.4655603170394897 for ['[CLS] red there hodge feel nu third farm ink fatelliserre basis being course primate tower artifact essence keep jail sighedoke game minor lucas victimstorium leader pain public orient martins [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.843 (perp=12.374, rec=0.361, cos=0.007), tot_loss_proj:3.347 [t=0.25s]
prediction: ['[CLS] tonight really bed infiltrate smoke using air weak lessum apparentlyled - worse militarygated photo security his numbers caden slapped neither emergency? kidnapping stupid mexican artificial tent -ed [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.299 (perp=10.054, rec=0.287, cos=0.001), tot_loss_proj:2.710 [t=0.27s]
prediction: ['[CLS] tonight tactic bed cover paper using the bottom - blownmissive - - worse agoing psychology ; his - caden accused neither emergency? kidnapping stupid hodgees - - artificial [SEP]']
[ 150/ 500] tot_loss=2.367 (perp=10.611, rec=0.244, cos=0.001), tot_loss_proj:2.845 [t=0.27s]
prediction: ['[CLS] tactic tactic up cover silly the the bottom underum devoid - or worsetgoing picture / bounty - quite accused none emergency? dragging less hodge - - - anti [SEP]']
Attempt swap
Moved token
[ 200/ 500] tot_loss=2.113 (perp=9.485, rec=0.214, cos=0.002), tot_loss_proj:2.806 [t=0.26s]
prediction: ['[CLS] tactic tactic cover up the up the sick under fact devoid - or worsetgoing picture & any - quiteted none worse yet against less hodge - - -ety [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=2.076 (perp=9.436, rec=0.188, cos=0.001), tot_loss_proj:2.592 [t=0.26s]
prediction: ['[CLS] tactic tactic cover up the up the sick by fact devoid - or worse yetgoing picture & - ideas yet against none anything yet made they hodge - - - hole [SEP]']
[ 300/ 500] tot_loss=2.058 (perp=9.487, rec=0.160, cos=0.001), tot_loss_proj:2.587 [t=0.28s]
prediction: ['[CLS] tactic tactic cover up the up the sickely fact devoid - or worse yetvation picture ideas - ideas yet against none yet yet made they hodge - - - hole [SEP]']
Attempt swap
Moved token
[ 350/ 500] tot_loss=2.129 (perp=9.882, rec=0.152, cos=0.001), tot_loss_proj:2.686 [t=0.26s]
prediction: ['[CLS] somewhat tactic cover up the up the -ely fact against devoid fl or worse yetvation picture ideas - ideas yet none yet yet constructed they zeke - - - hole [SEP]']
Attempt swap
Swapped tokens
[ 400/ 500] tot_loss=1.947 (perp=9.023, rec=0.142, cos=0.000), tot_loss_proj:2.411 [t=0.26s]
prediction: ['[CLS] somewhat tactic cover up around up the - of fact against devoid ( or worse yetsten picture ideas - ideas yet none yet - constructed they zeke - - - hole [SEP]']
[ 450/ 500] tot_loss=2.086 (perp=9.742, rec=0.137, cos=0.000), tot_loss_proj:2.571 [t=0.27s]
prediction: ['[CLS] somewhat tactic cover up around up the - of fact against devoid ( or worse yetsten picture ideas of ideas yet nonexi - constructed they zeke - - -sten [SEP]']
Attempt swap
Moved token
[ 500/ 500] tot_loss=1.921 (perp=8.929, rec=0.135, cos=0.000), tot_loss_proj:2.962 [t=0.26s]
prediction: ['[CLS] somewhat tactic cover up around the - up of fact against rum, or worse yetsten picture ideas - ideas yet nonexi - constructed less fails - - -sten [SEP]']
Done with input #89 of 100.
reference: 
========================
[CLS] tactic to cover up the fact that the picture is constructed around a core of flimsy - - or, worse yet, nonexistent - - ideas [SEP]
========================
predicted: 
========================
[CLS] somewhat tactic cover up around the - up of fact against fl, or worse yetsten picture ideas - ideas yet nonexi - constructed they zeke - - -sten [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 62.500 | p: 60.000 | r: 65.217
rouge2     | fm: 8.696 | p: 8.333 | r: 9.091
rougeL     | fm: 41.667 | p: 40.000 | r: 43.478
rougeLsum  | fm: 41.667 | p: 40.000 | r: 43.478
r1fm+r2fm = 71.196

[Aggregate metrics]:
rouge1     | fm: 80.173 | p: 79.534 | r: 81.211
rouge2     | fm: 42.249 | p: 41.874 | r: 42.610
rougeL     | fm: 68.768 | p: 68.250 | r: 69.551
rougeLsum  | fm: 68.729 | p: 68.165 | r: 69.506
r1fm+r2fm = 122.422

input #89 time: 0:03:29 | total time: 5:18:11


Running input #90 of 100.
reference: 
========================
how ridiculous and money-oriented 
========================
*********************************
*********************************
average of cosine similarity 0.9993650968600141
highest_index [0]
highest [0.9993650968600141]
Debug: ids_shape = 8, pads = [8]
Debug: input ids = tensor([[ 101, 2129, 9951, 1998, 2769, 1011, 8048,  102]], device='cuda:0')
Debug: ref = ['[CLS] how ridiculous and money - oriented [SEP]']
[Init] best rec loss: 1.889743447303772 for ['[CLS]uising camps native guard cooled mathematical [SEP]']
[Init] best rec loss: 1.827311396598816 for ['[CLS] event cheap sector value music volumes [SEP]']
[Init] best rec loss: 1.7925564050674438 for ['[CLS] education ace each catholicsor anti [SEP]']
[Init] best rec loss: 1.566908359527588 for ['[CLS] 1 aft include job hu spectacle [SEP]']
[Init] best rec loss: 1.5568602085113525 for ['[CLS] track direct unconscious unable eyes release [SEP]']
[Init] best rec loss: 1.453827977180481 for ['[CLS] whether alfa artwork lamp ground wang [SEP]']
[Init] best rec loss: 1.4401582479476929 for ['[CLS] hectares sessions tiny skin litter positions [SEP]']
[Init] best rec loss: 1.3662965297698975 for ['[CLS] cannot released when male spirited entourage [SEP]']
[Init] best perm rec loss: 1.3659476041793823 for ['[CLS] released male cannot spirited entourage when [SEP]']
[Init] best perm rec loss: 1.3621010780334473 for ['[CLS] cannot released spirited male when entourage [SEP]']
[Init] best perm rec loss: 1.3619078397750854 for ['[CLS] cannot male released spirited entourage when [SEP]']
[Init] best perm rec loss: 1.3608994483947754 for ['[CLS] cannot released male spirited entourage when [SEP]']
[Init] best perm rec loss: 1.3607511520385742 for ['[CLS] cannot released entourage male spirited when [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.635 (perp=11.449, rec=0.339, cos=0.006), tot_loss_proj:3.202 [t=0.27s]
prediction: ['[CLS] price ridiculous how britain complained weeks [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.346 (perp=10.666, rec=0.209, cos=0.003), tot_loss_proj:2.715 [t=0.26s]
prediction: ['[CLS] how ridiculous money pages propaganda voter [SEP]']
[ 150/ 500] tot_loss=2.162 (perp=10.076, rec=0.145, cos=0.002), tot_loss_proj:2.426 [t=0.29s]
prediction: ['[CLS] how ridiculous money oriented orientedized [SEP]']
Attempt swap
Moved sequence
[ 200/ 500] tot_loss=2.314 (perp=10.890, rec=0.134, cos=0.001), tot_loss_proj:2.583 [t=0.27s]
prediction: ['[CLS] how ridiculous jurisdiction money oriented oriented [SEP]']
Attempt swap
Moved token
[ 250/ 500] tot_loss=1.849 (perp=8.560, rec=0.133, cos=0.004), tot_loss_proj:2.072 [t=0.27s]
prediction: ['[CLS] how ridiculous and oriented money oriented [SEP]']
[ 300/ 500] tot_loss=1.800 (perp=8.560, rec=0.087, cos=0.001), tot_loss_proj:2.083 [t=0.27s]
prediction: ['[CLS] how ridiculous and oriented money oriented [SEP]']
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=1.520 (perp=7.167, rec=0.086, cos=0.001), tot_loss_proj:1.687 [t=0.29s]
prediction: ['[CLS] how ridiculous and money oriented oriented [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.525 (perp=7.167, rec=0.091, cos=0.001), tot_loss_proj:1.677 [t=0.28s]
prediction: ['[CLS] how ridiculous and money oriented oriented [SEP]']
[ 450/ 500] tot_loss=1.511 (perp=7.167, rec=0.077, cos=0.001), tot_loss_proj:1.679 [t=0.26s]
prediction: ['[CLS] how ridiculous and money oriented oriented [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.490 (perp=7.167, rec=0.056, cos=0.001), tot_loss_proj:1.679 [t=0.26s]
prediction: ['[CLS] how ridiculous and money oriented oriented [SEP]']
Done with input #90 of 100.
reference: 
========================
[CLS] how ridiculous and money - oriented [SEP]
========================
predicted: 
========================
[CLS] how ridiculous and money oriented oriented [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 93.333 | p: 87.500 | r: 100.000
rouge2     | fm: 92.308 | p: 85.714 | r: 100.000
rougeL     | fm: 93.333 | p: 87.500 | r: 100.000
rougeLsum  | fm: 93.333 | p: 87.500 | r: 100.000
r1fm+r2fm = 185.641

[Aggregate metrics]:
rouge1     | fm: 80.336 | p: 79.647 | r: 81.472
rouge2     | fm: 42.563 | p: 42.224 | r: 43.020
rougeL     | fm: 68.949 | p: 68.437 | r: 69.887
rougeLsum  | fm: 68.900 | p: 68.287 | r: 69.845
r1fm+r2fm = 122.899

input #90 time: 0:03:31 | total time: 5:21:42


Running input #91 of 100.
reference: 
========================
muy loco , but no more ridiculous 
========================
*********************************
*********************************
average of cosine similarity 0.9993538374859836
highest_index [0]
highest [0.9993538374859836]
Debug: ids_shape = 10, pads = [10]
Debug: input ids = tensor([[  101, 14163,  2100, 28046,  1010,  2021,  2053,  2062,  9951,   102]],
       device='cuda:0')
Debug: ref = ['[CLS] muy loco, but no more ridiculous [SEP]']
[Init] best rec loss: 1.8815274238586426 for ['[CLS] gravel effective snarled lighterogammed fans sometime [SEP]']
[Init] best rec loss: 1.5670865774154663 for ['[CLS] landssulłdotenick own get distant [SEP]']
[Init] best rec loss: 1.5520261526107788 for ['[CLS] blind prop notice taxis tang exchanged charge underlying [SEP]']
[Init] best rec loss: 1.24383544921875 for ['[CLS] mollusk jesuit fake software automaticaar verbal machine [SEP]']
[Init] best rec loss: 1.160547137260437 for ['[CLS] doubled regimental baby cement י game ultimate ideal [SEP]']
[Init] best rec loss: 1.1217325925827026 for ['[CLS] apollo lucia umpire skip gentleman grandmothergna line [SEP]']
[Init] best rec loss: 1.0907552242279053 for ['[CLS] choice seedbol transport anti if stairs guys [SEP]']
[Init] best rec loss: 1.0540754795074463 for ['[CLS]lippment revolution ponydern shelter hard unknown [SEP]']
[Init] best perm rec loss: 1.0526354312896729 for ['[CLS] unknown shelterpmentlipdern hard revolution pony [SEP]']
[Init] best perm rec loss: 1.0524038076400757 for ['[CLS]pmentlip pony revolution unknown harddern shelter [SEP]']
[Init] best perm rec loss: 1.05027437210083 for ['[CLS]lip revolution hard ponydern unknownpment shelter [SEP]']
[Init] best perm rec loss: 1.0482343435287476 for ['[CLS]lip unknownpment revolution shelterdern hard pony [SEP]']
[Init] best perm rec loss: 1.0455262660980225 for ['[CLS]lip revolutionpment unknown harddern shelter pony [SEP]']
[Init] best perm rec loss: 1.0435428619384766 for ['[CLS] shelterdern revolution hardlippment unknown pony [SEP]']
[Init] best perm rec loss: 1.0432696342468262 for ['[CLS] harddernlippment shelter revolution unknown pony [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.765 (perp=12.026, rec=0.356, cos=0.004), tot_loss_proj:3.466 [t=0.31s]
prediction: ['[CLS] pig government off. peanut ridiculous maybe ridiculous [SEP]']
Attempt swap
Moved token
[ 100/ 500] tot_loss=2.465 (perp=10.953, rec=0.272, cos=0.002), tot_loss_proj:3.098 [t=0.30s]
prediction: ['[CLS] narrow policies off mu ridiculous ridiculous ridiculous but [SEP]']
[ 150/ 500] tot_loss=2.421 (perp=11.016, rec=0.216, cos=0.002), tot_loss_proj:2.981 [t=0.29s]
prediction: ['[CLS]less loco loco mu ridiculous more loco but [SEP]']
Attempt swap
Moved token
[ 200/ 500] tot_loss=2.277 (perp=10.529, rec=0.170, cos=0.001), tot_loss_proj:2.794 [t=0.30s]
prediction: ['[CLS] yellow loco loco & mu ridiculous more but [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=2.012 (perp=9.231, rec=0.164, cos=0.002), tot_loss_proj:2.740 [t=0.29s]
prediction: ['[CLS] little loco locoy mu but more ridiculous [SEP]']
[ 300/ 500] tot_loss=2.326 (perp=10.944, rec=0.137, cos=0.000), tot_loss_proj:3.031 [t=0.29s]
prediction: ['[CLS] gu loco locoy mu but more ridiculous [SEP]']
Attempt swap
Moved token
[ 350/ 500] tot_loss=1.837 (perp=8.661, rec=0.104, cos=0.000), tot_loss_proj:2.530 [t=0.28s]
prediction: ['[CLS] no loco loco muy but more ridiculous [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.776 (perp=8.386, rec=0.097, cos=0.001), tot_loss_proj:2.458 [t=0.31s]
prediction: ['[CLS] noy loco muy but more ridiculous [SEP]']
[ 450/ 500] tot_loss=1.771 (perp=8.386, rec=0.093, cos=0.000), tot_loss_proj:2.449 [t=0.31s]
prediction: ['[CLS] noy loco muy but more ridiculous [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.760 (perp=8.386, rec=0.082, cos=0.000), tot_loss_proj:2.449 [t=0.30s]
prediction: ['[CLS] noy loco muy but more ridiculous [SEP]']
Done with input #91 of 100.
reference: 
========================
[CLS] muy loco, but no more ridiculous [SEP]
========================
predicted: 
========================
[CLS] noy loco muy but more ridiculous [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 87.500 | p: 87.500 | r: 87.500
rouge2     | fm: 28.571 | p: 28.571 | r: 28.571
rougeL     | fm: 75.000 | p: 75.000 | r: 75.000
rougeLsum  | fm: 75.000 | p: 75.000 | r: 75.000
r1fm+r2fm = 116.071

[Aggregate metrics]:
rouge1     | fm: 80.456 | p: 79.734 | r: 81.492
rouge2     | fm: 42.408 | p: 42.032 | r: 42.854
rougeL     | fm: 69.049 | p: 68.514 | r: 69.848
rougeLsum  | fm: 69.103 | p: 68.479 | r: 69.897
r1fm+r2fm = 122.864

input #91 time: 0:03:57 | total time: 5:25:39


Running input #92 of 100.
reference: 
========================
deceit 
========================
*********************************
*********************************
average of cosine similarity 0.9993137310302475
highest_index [0]
highest [0.9993137310302475]
Debug: ids_shape = 4, pads = [4]
Debug: input ids = tensor([[  101, 11703, 20175,   102]], device='cuda:0')
Debug: ref = ['[CLS] deceit [SEP]']
[Init] best rec loss: 1.81099534034729 for ['[CLS] damncoat [SEP]']
[Init] best rec loss: 1.76921546459198 for ['[CLS] franz counter [SEP]']
[Init] best rec loss: 1.7248120307922363 for ['[CLS] false issue [SEP]']
[Init] best rec loss: 1.6912018060684204 for ['[CLS] mandatory maneuver [SEP]']
[Init] best rec loss: 1.4354274272918701 for ['[CLS] mine may [SEP]']
[Init] best rec loss: 1.4101020097732544 for ['[CLS] wish rich [SEP]']
[Init] best rec loss: 1.2274341583251953 for ['[CLS] tank lonely [SEP]']
[Init] best perm rec loss: 1.2247881889343262 for ['[CLS] lonely tank [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.677 (perp=12.265, rec=0.219, cos=0.004), tot_loss_proj:3.205 [t=0.30s]
prediction: ['[CLS] erroreit [SEP]']
Attempt swap
[ 100/ 500] tot_loss=2.591 (perp=12.265, rec=0.137, cos=0.002), tot_loss_proj:3.200 [t=0.29s]
prediction: ['[CLS] erroreit [SEP]']
[ 150/ 500] tot_loss=1.617 (perp=7.646, rec=0.087, cos=0.001), tot_loss_proj:1.609 [t=0.31s]
prediction: ['[CLS] deceit [SEP]']
Attempt swap
[ 200/ 500] tot_loss=1.598 (perp=7.646, rec=0.068, cos=0.001), tot_loss_proj:1.621 [t=0.30s]
prediction: ['[CLS] deceit [SEP]']
Attempt swap
[ 250/ 500] tot_loss=1.587 (perp=7.646, rec=0.057, cos=0.001), tot_loss_proj:1.618 [t=0.31s]
prediction: ['[CLS] deceit [SEP]']
[ 300/ 500] tot_loss=1.589 (perp=7.646, rec=0.059, cos=0.001), tot_loss_proj:1.604 [t=0.29s]
prediction: ['[CLS] deceit [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.604 (perp=7.646, rec=0.074, cos=0.001), tot_loss_proj:1.611 [t=0.30s]
prediction: ['[CLS] deceit [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.591 (perp=7.646, rec=0.061, cos=0.001), tot_loss_proj:1.613 [t=0.29s]
prediction: ['[CLS] deceit [SEP]']
[ 450/ 500] tot_loss=1.590 (perp=7.646, rec=0.060, cos=0.001), tot_loss_proj:1.606 [t=0.28s]
prediction: ['[CLS] deceit [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.594 (perp=7.646, rec=0.064, cos=0.001), tot_loss_proj:1.617 [t=0.32s]
prediction: ['[CLS] deceit [SEP]']
Done with input #92 of 100.
reference: 
========================
[CLS] deceit [SEP]
========================
predicted: 
========================
[CLS] deceit [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 80.590 | p: 79.857 | r: 81.624
rouge2     | fm: 43.045 | p: 42.694 | r: 43.513
rougeL     | fm: 69.496 | p: 68.926 | r: 70.292
rougeLsum  | fm: 69.396 | p: 68.699 | r: 70.323
r1fm+r2fm = 123.635

input #92 time: 0:03:56 | total time: 5:29:36


Running input #93 of 100.
reference: 
========================
in its understanding , often funny way 
========================
*********************************
*********************************
average of cosine similarity 0.9993323263895036
highest_index [0]
highest [0.9993323263895036]
Debug: ids_shape = 9, pads = [9]
Debug: input ids = tensor([[ 101, 1999, 2049, 4824, 1010, 2411, 6057, 2126,  102]],
       device='cuda:0')
Debug: ref = ['[CLS] in its understanding, often funny way [SEP]']
[Init] best rec loss: 1.9964412450790405 for ['[CLS] pennsylvaniaplay after polish best foe sparhawk [SEP]']
[Init] best rec loss: 1.804612159729004 for ['[CLS]tz being fluent nevernished clinging met [SEP]']
[Init] best rec loss: 1.764283537864685 for ['[CLS] will individual unitsbular absent lights distribution [SEP]']
[Init] best rec loss: 1.7488878965377808 for ['[CLS] overall teachers you helping parkmedia neutral [SEP]']
[Init] best rec loss: 1.7436745166778564 for ['[CLS] lucas war breaks baby my pauline divided [SEP]']
[Init] best rec loss: 1.738550066947937 for ['[CLS] village tallest almost giro opened wine section [SEP]']
[Init] best rec loss: 1.5410614013671875 for ['[CLS] [CLS]rac madonna premiership further jeremy colby [SEP]']
[Init] best perm rec loss: 1.5395601987838745 for ['[CLS]rac jeremy madonna further premiership [CLS] colby [SEP]']
[Init] best perm rec loss: 1.536518931388855 for ['[CLS] madonna premiership jeremy further colby [CLS]rac [SEP]']
[Init] best perm rec loss: 1.5341161489486694 for ['[CLS] madonna premiership colbyrac [CLS] further jeremy [SEP]']
[Init] best perm rec loss: 1.5311251878738403 for ['[CLS] further madonna jeremy [CLS] colby premiershiprac [SEP]']
[Init] best perm rec loss: 1.5273934602737427 for ['[CLS] colby madonna premiership further jeremyrac [CLS] [SEP]']
[Init] best perm rec loss: 1.525164246559143 for ['[CLS] further madonnarac colby jeremy premiership [CLS] [SEP]']
[Init] best perm rec loss: 1.5250414609909058 for ['[CLS] premiership jeremy madonnarac colby further [CLS] [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.394 (perp=10.234, rec=0.338, cos=0.009), tot_loss_proj:2.691 [t=0.29s]
prediction: ['[CLS] musical bishop and also executive understanding good [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.124 (perp=9.215, rec=0.276, cos=0.005), tot_loss_proj:2.348 [t=0.31s]
prediction: ['[CLS] understanding of in funny in understanding funny [SEP]']
[ 150/ 500] tot_loss=1.938 (perp=8.533, rec=0.229, cos=0.002), tot_loss_proj:2.406 [t=0.30s]
prediction: ['[CLS] understanding of in understanding in understanding way [SEP]']
Attempt swap
Moved token
[ 200/ 500] tot_loss=1.788 (perp=8.137, rec=0.159, cos=0.002), tot_loss_proj:2.075 [t=0.31s]
prediction: ['[CLS] understanding of understanding in funny its way [SEP]']
Attempt swap
Moved sequence
[ 250/ 500] tot_loss=1.670 (perp=7.720, rec=0.125, cos=0.002), tot_loss_proj:2.241 [t=0.26s]
prediction: ['[CLS] understanding of often funny in its way [SEP]']
[ 300/ 500] tot_loss=1.643 (perp=7.720, rec=0.098, cos=0.001), tot_loss_proj:2.244 [t=0.27s]
prediction: ['[CLS] understanding of often funny in its way [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.638 (perp=7.720, rec=0.093, cos=0.001), tot_loss_proj:2.245 [t=0.25s]
prediction: ['[CLS] understanding of often funny in its way [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.632 (perp=7.720, rec=0.087, cos=0.001), tot_loss_proj:2.248 [t=0.26s]
prediction: ['[CLS] understanding of often funny in its way [SEP]']
[ 450/ 500] tot_loss=1.630 (perp=7.720, rec=0.085, cos=0.001), tot_loss_proj:2.242 [t=0.26s]
prediction: ['[CLS] understanding of often funny in its way [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.626 (perp=7.720, rec=0.081, cos=0.001), tot_loss_proj:2.244 [t=0.25s]
prediction: ['[CLS] understanding of often funny in its way [SEP]']
Done with input #93 of 100.
reference: 
========================
[CLS] in its understanding, often funny way [SEP]
========================
predicted: 
========================
[CLS] understanding of often funny in its way [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 94.118 | p: 88.889 | r: 100.000
rouge2     | fm: 40.000 | p: 37.500 | r: 42.857
rougeL     | fm: 70.588 | p: 66.667 | r: 75.000
rougeLsum  | fm: 70.588 | p: 66.667 | r: 75.000
r1fm+r2fm = 134.118

[Aggregate metrics]:
rouge1     | fm: 80.752 | p: 79.904 | r: 81.832
rouge2     | fm: 43.006 | p: 42.658 | r: 43.405
rougeL     | fm: 69.468 | p: 68.875 | r: 70.337
rougeLsum  | fm: 69.453 | p: 68.799 | r: 70.375
r1fm+r2fm = 123.758

input #93 time: 0:03:46 | total time: 5:33:23


Running input #94 of 100.
reference: 
========================
a caper that 's neither original nor terribly funny 
========================
*********************************
*********************************
average of cosine similarity 0.9993168061064918
highest_index [0]
highest [0.9993168061064918]
Debug: ids_shape = 13, pads = [13]
Debug: input ids = tensor([[  101,  1037,  4880,  2099,  2008,  1005,  1055,  4445,  2434,  4496,
         16668,  6057,   102]], device='cuda:0')
Debug: ref = ["[CLS] a caper that's neither original nor terribly funny [SEP]"]
[Init] best rec loss: 1.9509918689727783 for ['[CLS] address am forms census depending nolan rumor constantly paste indoorsizan [SEP]']
[Init] best rec loss: 1.9232128858566284 for ['[CLS] milling library casual constructed board equationphs similaron cookies intended [SEP]']
[Init] best rec loss: 1.8279337882995605 for ['[CLS] scent crosses am history single connections set likeuatingface other [SEP]']
[Init] best rec loss: 1.823370099067688 for ['[CLS]ser straight alligator inches subject never splashed pony des withancy [SEP]']
[Init] best rec loss: 1.5088582038879395 for ['[CLS]rite branches experiences guitarist chart wife [CLS] toe grandpa floor no [SEP]']
[Init] best rec loss: 1.4581665992736816 for ['[CLS]pour matters bad slowedble bow spirititercedeer mir [SEP]']
[Init] best rec loss: 1.445499062538147 for ['[CLS]venting rockwell internal expedition plum chronic shocks flowering territorial crushed centre [SEP]']
[Init] best perm rec loss: 1.4265179634094238 for ['[CLS] shocks territorial rockwell internal crushedventing expedition centre flowering chronic plum [SEP]']
[Init] best perm rec loss: 1.4207502603530884 for ['[CLS] internal plum flowering expedition territorial shocks chronic centre crushed rockwellventing [SEP]']
[Init] best perm rec loss: 1.4164097309112549 for ['[CLS] centreventing flowering expedition shocks chronic crushed internal rockwell territorial plum [SEP]']
[Init] best perm rec loss: 1.4148106575012207 for ['[CLS] chronicventing territorial expedition shocks crushed centre internal rockwell flowering plum [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.880 (perp=12.248, rec=0.427, cos=0.004), tot_loss_proj:3.859 [t=0.27s]
prediction: ['[CLS] nor protests atoe inspection transmission antique ss containing junk lifeless [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.980 (perp=13.187, rec=0.338, cos=0.004), tot_loss_proj:4.023 [t=0.27s]
prediction: ['[CLS] nor comic barrject cape dead rules nor neither nor nor [SEP]']
[ 150/ 500] tot_loss=2.318 (perp=10.423, rec=0.232, cos=0.001), tot_loss_proj:3.143 [t=0.25s]
prediction: ['[CLS] nor original a a caper facto nor nor neither neither [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=1.959 (perp=8.736, rec=0.210, cos=0.002), tot_loss_proj:2.449 [t=0.27s]
prediction: ['[CLS] nor funny neither a caper gross nor a currently neither [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=1.632 (perp=7.270, rec=0.177, cos=0.001), tot_loss_proj:2.165 [t=0.26s]
prediction: ['[CLS] nor funny neither a caper nor nor a s neither [SEP]']
[ 300/ 500] tot_loss=1.612 (perp=7.270, rec=0.157, cos=0.001), tot_loss_proj:2.161 [t=0.28s]
prediction: ['[CLS] nor funny neither a caper nor nor a s neither [SEP]']
Attempt swap
Moved token
[ 350/ 500] tot_loss=1.641 (perp=7.530, rec=0.134, cos=0.001), tot_loss_proj:2.214 [t=0.27s]
prediction: ['[CLS] nor funny neither a caper or a s nor neither [SEP]']
Attempt swap
Moved sequence
[ 400/ 500] tot_loss=1.499 (perp=6.826, rec=0.133, cos=0.001), tot_loss_proj:2.183 [t=0.25s]
prediction: ["[CLS] nor funny that a caper or a'neither s [SEP]"]
[ 450/ 500] tot_loss=1.527 (perp=6.972, rec=0.132, cos=0.001), tot_loss_proj:2.232 [t=0.27s]
prediction: ['[CLS] nor funny that a caper or a nor neither s [SEP]']
Attempt swap
Moved sequence
[ 500/ 500] tot_loss=1.495 (perp=6.833, rec=0.128, cos=0.001), tot_loss_proj:2.137 [t=0.26s]
prediction: ['[CLS] nor funny that a caper or neither a - s [SEP]']
Done with input #94 of 100.
reference: 
========================
[CLS] a caper that's neither original nor terribly funny [SEP]
========================
predicted: 
========================
[CLS] nor funny that a caper or a nor neither s [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 78.261 | p: 75.000 | r: 81.818
rouge2     | fm: 9.524 | p: 9.091 | r: 10.000
rougeL     | fm: 43.478 | p: 41.667 | r: 45.455
rougeLsum  | fm: 43.478 | p: 41.667 | r: 45.455
r1fm+r2fm = 87.785

[Aggregate metrics]:
rouge1     | fm: 80.750 | p: 79.962 | r: 81.883
rouge2     | fm: 42.648 | p: 42.285 | r: 43.075
rougeL     | fm: 69.216 | p: 68.640 | r: 70.056
rougeLsum  | fm: 69.161 | p: 68.503 | r: 70.113
r1fm+r2fm = 123.398

input #94 time: 0:03:29 | total time: 5:36:52


Running input #95 of 100.
reference: 
========================
( denis ' ) story becomes a hopeless , unsatisfying muddle 
========================
*********************************
*********************************
average of cosine similarity 0.9991574288984811
highest_index [0]
highest [0.9991574288984811]
Debug: ids_shape = 17, pads = [17]
Debug: input ids = tensor([[  101,  1006, 11064,  1005,  1007,  2466,  4150,  1037, 20625,  1010,
          4895, 16846,  2483, 14116,  8494, 10362,   102]], device='cuda:0')
Debug: ref = ["[CLS] ( denis') story becomes a hopeless, unsatisfying muddle [SEP]"]
[Init] best rec loss: 1.9503685235977173 for ['[CLS] condition leon cup began habitc boy floor imperial major i sud inside missed over [SEP]']
[Init] best rec loss: 1.9354028701782227 for ['[CLS] bree theological teaching maybe backed past starvinglusion pigs twitcharable badic flower able [SEP]']
[Init] best rec loss: 1.8349953889846802 for ['[CLS] channel congestion approach nude scottful performing blackout suffered introduced ground sr planted evans declared [SEP]']
[Init] best rec loss: 1.8156688213348389 for ['[CLS]ian media ye deposit cook point tied ranking original mean believe cellular neon scrolls next [SEP]']
[Init] best rec loss: 1.7410764694213867 for ['[CLS] rage campulsion exitscribe thought countrer pain rubin shop second bowler vinyl fitch [SEP]']
[Init] best rec loss: 1.6973210573196411 for ['[CLS] paul jai employer smell han roosevelt extinct scar duty volga charley sprint back fashioned paige [SEP]']
[Init] best rec loss: 1.6551605463027954 for ['[CLS] fire some phillip margin khz number grace discipline formula plains when secretarygrave duke hell [SEP]']
[Init] best rec loss: 1.560425043106079 for ['[CLS] end output rather overall of byron inventor earth interests novel adult heir night bryan remaining [SEP]']
[Init] best rec loss: 1.3368700742721558 for ['[CLS] pressure ] completenne damp trailer block wireے tech sister private cut monty hanging [SEP]']
[Init] best perm rec loss: 1.3238457441329956 for ['[CLS] trailer private cut dampnne hanging block ] complete wire sisterے pressure monty tech [SEP]']
[Init] best perm rec loss: 1.3197247982025146 for ['[CLS] ] damp tech trailer cut privateے hanging block complete monty pressure sister wirenne [SEP]']
[Init] best perm rec loss: 1.319675087928772 for ['[CLS] damp pressure techے monty sister private cut complete trailer hanging ] wire blocknne [SEP]']
[Init] best perm rec loss: 1.3172006607055664 for ['[CLS] ] cut private technne monty trailer block sister wire dampے hanging complete pressure [SEP]']
[Init] best perm rec loss: 1.3129985332489014 for ['[CLS] monty sister private cut tech trailer wire block hangingnne damp ]ے complete pressure [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.532 (perp=10.997, rec=0.328, cos=0.005), tot_loss_proj:2.776 [t=0.26s]
prediction: ['[CLS] hopeless old private housing, hopeless junk tunnel violent abusive worst hopeless? a hopeless [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.514 (perp=11.315, rec=0.249, cos=0.002), tot_loss_proj:2.804 [t=0.26s]
prediction: ["[CLS] hopeless dead an housing story hopeless dna tunnel'becomes hopeless hopeless - became hopeless [SEP]"]
[ 150/ 500] tot_loss=2.234 (perp=10.183, rec=0.196, cos=0.001), tot_loss_proj:2.576 [t=0.26s]
prediction: ['[CLS] battered deadless housing story hopeless dnased, becomes restrictions hopeless. becomes hopeless [SEP]']
Attempt swap
Moved sequence
[ 200/ 500] tot_loss=2.518 (perp=11.711, rec=0.175, cos=0.001), tot_loss_proj:2.883 [t=0.26s]
prediction: ['[CLS] deadless housing story hopeless formssed sad, becomes limit hopelessdle becomes hopeless [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=2.586 (perp=12.111, rec=0.162, cos=0.001), tot_loss_proj:2.979 [t=0.26s]
prediction: ['[CLS] bed hopeless cdp story hopeless formsfying sad, becomesno adle becomes hopeless [SEP]']
[ 300/ 500] tot_loss=2.639 (perp=12.459, rec=0.146, cos=0.001), tot_loss_proj:3.041 [t=0.27s]
prediction: ['[CLS] bed muders story hopeless formsfyingsat,satfying adle becomes hopeless [SEP]']
Attempt swap
Moved token
[ 350/ 500] tot_loss=2.508 (perp=11.871, rec=0.132, cos=0.001), tot_loss_proj:2.892 [t=0.27s]
prediction: ['[CLS] beders story hopeless formsfying mud un,satfying adle becomes hopeless [SEP]']
Attempt swap
Swapped tokens
[ 400/ 500] tot_loss=2.291 (perp=10.850, rec=0.120, cos=0.001), tot_loss_proj:2.595 [t=0.26s]
prediction: ["[CLS] 'ers story denis formsfying a un,satfying muddle becomes hopeless [SEP]"]
[ 450/ 500] tot_loss=2.287 (perp=10.850, rec=0.116, cos=0.001), tot_loss_proj:2.598 [t=0.28s]
prediction: ["[CLS] 'ers story denis formsfying a un,satfying muddle becomes hopeless [SEP]"]
Attempt swap
Swapped tokens
[ 500/ 500] tot_loss=1.926 (perp=9.064, rec=0.112, cos=0.001), tot_loss_proj:2.283 [t=0.27s]
prediction: ["[CLS]'' un denis formsfying a story,satfying muddle becomes hopeless [SEP]"]
Done with input #95 of 100.
reference: 
========================
[CLS] ( denis') story becomes a hopeless, unsatisfying muddle [SEP]
========================
predicted: 
========================
[CLS]'' un denis formsfying a story,satfying muddle becomes hopeless [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 80.000 | p: 72.727 | r: 88.889
rouge2     | fm: 0.000 | p: 0.000 | r: 0.000
rougeL     | fm: 60.000 | p: 54.545 | r: 66.667
rougeLsum  | fm: 60.000 | p: 54.545 | r: 66.667
r1fm+r2fm = 80.000

[Aggregate metrics]:
rouge1     | fm: 80.697 | p: 79.859 | r: 81.911
rouge2     | fm: 42.202 | p: 41.870 | r: 42.621
rougeL     | fm: 69.068 | p: 68.416 | r: 69.995
rougeLsum  | fm: 68.971 | p: 68.282 | r: 70.009
r1fm+r2fm = 122.899

input #95 time: 0:03:31 | total time: 5:40:24


Running input #96 of 100.
reference: 
========================
force himself on people and into situations that would make lesser men run for cover 
========================
*********************************
*********************************
average of cosine similarity 0.9992866562400882
highest_index [0]
highest [0.9992866562400882]
Debug: ids_shape = 17, pads = [17]
Debug: input ids = tensor([[ 101, 2486, 2370, 2006, 2111, 1998, 2046, 8146, 2008, 2052, 2191, 8276,
         2273, 2448, 2005, 3104,  102]], device='cuda:0')
Debug: ref = ['[CLS] force himself on people and into situations that would make lesser men run for cover [SEP]']
[Init] best rec loss: 1.794450283050537 for ['[CLS] all extended factor darethesis receiver depths fr disclosure billfold cool gall afford theoretical [SEP]']
[Init] best rec loss: 1.7930576801300049 for ['[CLS] earning poly dishes every mistaken as ok loose sage families morse platt we charm acts [SEP]']
[Init] best rec loss: 1.6783851385116577 for ['[CLS] robinson fits fr home professor shanghai virgin firing inside machlmancize arm decade [SEP]']
[Init] best rec loss: 1.6411691904067993 for ['[CLS] smashwords memoir spent soundinggn save statue time projectile typical living pondered august wig era [SEP]']
[Init] best rec loss: 1.634422779083252 for ['[CLS]rg woken became jenny marx further league performance union holding winning instrumental re distance conscious [SEP]']
[Init] best rec loss: 1.4709296226501465 for ['[CLS] hand hopefullysas super profit laundry readings context places liaison mollusk talents rest feature date [SEP]']
[Init] best perm rec loss: 1.466230869293213 for ['[CLS] profitsas hand rest hopefully places laundry mollusk date liaison feature talents readings context super [SEP]']
[Init] best perm rec loss: 1.4635539054870605 for ['[CLS] readings liaison profit places contextsas rest hopefully laundry feature hand talents date super mollusk [SEP]']
[Init] best perm rec loss: 1.4631778001785278 for ['[CLS] date hopefully laundry talents super profit restsas feature mollusk hand places context liaison readings [SEP]']
[Init] best perm rec loss: 1.4482918977737427 for ['[CLS] super profit context readings laundry places datesas feature hand liaison hopefully rest talents mollusk [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.876 (perp=12.325, rec=0.403, cos=0.008), tot_loss_proj:3.790 [t=0.26s]
prediction: ['[CLS] nice on improved region allows than over brakes at few lady railway structure amazing airfield [SEP]']
Attempt swap
Moved sequence
[ 100/ 500] tot_loss=2.513 (perp=10.964, rec=0.318, cos=0.002), tot_loss_proj:3.736 [t=0.29s]
prediction: ['[CLS] nice to improved on online companies might reggae potential people situations atane on force [SEP]']
[ 150/ 500] tot_loss=2.420 (perp=10.786, rec=0.260, cos=0.002), tot_loss_proj:3.167 [t=0.27s]
prediction: ['[CLS] nice to several on online people force might potential people situations intoane on force [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.319 (perp=10.493, rec=0.216, cos=0.005), tot_loss_proj:3.117 [t=0.27s]
prediction: ['[CLS] nice himself improved people himself into situations through potential people situations intoane on force [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=2.073 (perp=9.329, rec=0.206, cos=0.001), tot_loss_proj:3.385 [t=0.26s]
prediction: ['[CLS] nice himself better on himself on situations to potential people cover intoane people force [SEP]']
[ 300/ 500] tot_loss=2.201 (perp=10.073, rec=0.185, cos=0.001), tot_loss_proj:3.486 [t=0.27s]
prediction: ['[CLS] nice himself lesser on himself on situations that lesser men cover intoane people force [SEP]']
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=1.978 (perp=8.927, rec=0.192, cos=0.001), tot_loss_proj:2.837 [t=0.27s]
prediction: ['[CLS] equipped himself running on himself on situations into lesser men cover into lesser people force [SEP]']
Attempt swap
Moved token
[ 400/ 500] tot_loss=2.013 (perp=9.181, rec=0.176, cos=0.001), tot_loss_proj:2.950 [t=0.26s]
prediction: ['[CLS] okay himself running on himself on situations into lesser men would cover lesser people force [SEP]']
[ 450/ 500] tot_loss=2.043 (perp=9.432, rec=0.156, cos=0.001), tot_loss_proj:2.821 [t=0.27s]
prediction: ['[CLS] please himself running both himself on situations into lesser men would cover lesser people force [SEP]']
Attempt swap
Moved token
[ 500/ 500] tot_loss=1.977 (perp=9.124, rec=0.151, cos=0.000), tot_loss_proj:2.901 [t=0.26s]
prediction: ['[CLS] please running himself both himself on situations into lesser men would cover lesser people force [SEP]']
Done with input #96 of 100.
reference: 
========================
[CLS] force himself on people and into situations that would make lesser men run for cover [SEP]
========================
predicted: 
========================
[CLS] please running himself both himself on situations into lesser men would cover lesser people force [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 70.588 | p: 70.588 | r: 70.588
rouge2     | fm: 12.500 | p: 12.500 | r: 12.500
rougeL     | fm: 47.059 | p: 47.059 | r: 47.059
rougeLsum  | fm: 47.059 | p: 47.059 | r: 47.059
r1fm+r2fm = 83.088

[Aggregate metrics]:
rouge1     | fm: 80.600 | p: 79.749 | r: 81.858
rouge2     | fm: 41.723 | p: 41.370 | r: 42.241
rougeL     | fm: 68.893 | p: 68.236 | r: 69.838
rougeLsum  | fm: 68.790 | p: 68.166 | r: 69.771
r1fm+r2fm = 122.323

input #96 time: 0:03:31 | total time: 5:43:55


Running input #97 of 100.
reference: 
========================
and unforgettable characters 
========================
*********************************
*********************************
average of cosine similarity 0.9991660915875737
highest_index [0]
highest [0.9991660915875737]
Debug: ids_shape = 8, pads = [8]
Debug: input ids = tensor([[  101,  1998,  4895, 29278, 18150, 10880,  3494,   102]],
       device='cuda:0')
Debug: ref = ['[CLS] and unforgettable characters [SEP]']
[Init] best rec loss: 1.292439579963684 for ['[CLS] ring halt populated rich striking miranda [SEP]']
[Init] best rec loss: 1.0615618228912354 for ['[CLS]ten which 2016 victoria pass test [SEP]']
[Init] best perm rec loss: 1.0516561269760132 for ['[CLS] victoriaten test pass 2016 which [SEP]']
[Init] best perm rec loss: 1.0462650060653687 for ['[CLS] pass testten which 2016 victoria [SEP]']
[Init] best perm rec loss: 1.0451700687408447 for ['[CLS] pass test victoriaten 2016 which [SEP]']
[Init] best perm rec loss: 1.0422724485397339 for ['[CLS] victoria test 2016 pass whichten [SEP]']
[Init] best perm rec loss: 1.0385247468948364 for ['[CLS] pass test which victoria 2016ten [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.791 (perp=11.981, rec=0.387, cos=0.008), tot_loss_proj:4.101 [t=0.27s]
prediction: ['[CLS] sets never at paradise catching & [SEP]']
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.604 (perp=11.634, rec=0.272, cos=0.005), tot_loss_proj:4.279 [t=0.29s]
prediction: ['[CLS] componentsforforget un & [SEP]']
[ 150/ 500] tot_loss=2.543 (perp=11.633, rec=0.214, cos=0.002), tot_loss_proj:3.868 [t=0.25s]
prediction: ['[CLS] charactersforfortableforget [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.344 (perp=10.858, rec=0.170, cos=0.002), tot_loss_proj:3.134 [t=0.25s]
prediction: ['[CLS] charactersforgettablefor and [SEP]']
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=1.929 (perp=8.970, rec=0.133, cos=0.002), tot_loss_proj:2.240 [t=0.26s]
prediction: ['[CLS]forforgettable characters and [SEP]']
[ 300/ 500] tot_loss=1.907 (perp=8.970, rec=0.109, cos=0.004), tot_loss_proj:2.244 [t=0.27s]
prediction: ['[CLS]forforgettable characters and [SEP]']
Attempt swap
[ 350/ 500] tot_loss=1.895 (perp=8.970, rec=0.100, cos=0.001), tot_loss_proj:2.248 [t=0.26s]
prediction: ['[CLS]forforgettable characters and [SEP]']
Attempt swap
Swapped tokens
[ 400/ 500] tot_loss=1.747 (perp=8.364, rec=0.073, cos=0.001), tot_loss_proj:2.539 [t=0.25s]
prediction: ['[CLS]for ungettable and characters [SEP]']
[ 450/ 500] tot_loss=1.749 (perp=8.364, rec=0.075, cos=0.001), tot_loss_proj:2.540 [t=0.25s]
prediction: ['[CLS]for ungettable and characters [SEP]']
Attempt swap
Swapped tokens
[ 500/ 500] tot_loss=1.179 (perp=5.514, rec=0.075, cos=0.001), tot_loss_proj:1.397 [t=0.27s]
prediction: ['[CLS] unforgettable and characters [SEP]']
Done with input #97 of 100.
reference: 
========================
[CLS] and unforgettable characters [SEP]
========================
predicted: 
========================
[CLS] unforgettable and characters [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 25.000 | p: 25.000 | r: 25.000
rougeL     | fm: 80.000 | p: 80.000 | r: 80.000
rougeLsum  | fm: 80.000 | p: 80.000 | r: 80.000
r1fm+r2fm = 125.000

[Aggregate metrics]:
rouge1     | fm: 80.843 | p: 80.006 | r: 82.059
rouge2     | fm: 41.636 | p: 41.266 | r: 42.069
rougeL     | fm: 69.031 | p: 68.340 | r: 69.952
rougeLsum  | fm: 68.991 | p: 68.254 | r: 70.029
r1fm+r2fm = 122.479

input #97 time: 0:03:29 | total time: 5:47:25


Running input #98 of 100.
reference: 
========================
unfulfilling 
========================
*********************************
*********************************
average of cosine similarity 0.9991916976323434
highest_index [0]
highest [0.9991916976323434]
Debug: ids_shape = 6, pads = [6]
Debug: input ids = tensor([[  101,  4895,  3993,  8873, 13112,   102]], device='cuda:0')
Debug: ref = ['[CLS] unfulfilling [SEP]']
[Init] best rec loss: 1.1314566135406494 for ['[CLS] prohibited jed ada nos [SEP]']
[Init] best rec loss: 1.1208252906799316 for ['[CLS] nature " open victims [SEP]']
[Init] best perm rec loss: 1.1160643100738525 for ['[CLS] " victims nature open [SEP]']
Nsteps: 500
[  50/ 500] tot_loss=2.670 (perp=11.705, rec=0.326, cos=0.004), tot_loss_proj:3.197 [t=0.26s]
prediction: ['[CLS]fulen let empty [SEP]']
Attempt swap
Moved token
[ 100/ 500] tot_loss=2.518 (perp=11.405, rec=0.234, cos=0.003), tot_loss_proj:4.106 [t=0.27s]
prediction: ['[CLS]fulfulful ella [SEP]']
[ 150/ 500] tot_loss=2.891 (perp=13.405, rec=0.203, cos=0.007), tot_loss_proj:4.467 [t=0.25s]
prediction: ['[CLS]llingfulfulpling [SEP]']
Attempt swap
Swapped tokens
[ 200/ 500] tot_loss=2.399 (perp=11.115, rec=0.172, cos=0.003), tot_loss_proj:3.949 [t=0.25s]
prediction: ['[CLS]fulllingful negotiations [SEP]']
Attempt swap
Moved token
[ 250/ 500] tot_loss=2.013 (perp=9.262, rec=0.158, cos=0.003), tot_loss_proj:2.362 [t=0.26s]
prediction: ['[CLS]ful unfilling [SEP]']
[ 300/ 500] tot_loss=1.967 (perp=9.262, rec=0.114, cos=0.001), tot_loss_proj:2.357 [t=0.25s]
prediction: ['[CLS]ful unfilling [SEP]']
Attempt swap
Moved token
[ 350/ 500] tot_loss=1.089 (perp=4.948, rec=0.099, cos=0.001), tot_loss_proj:1.076 [t=0.26s]
prediction: ['[CLS] unfulfilling [SEP]']
Attempt swap
[ 400/ 500] tot_loss=1.081 (perp=4.948, rec=0.091, cos=0.001), tot_loss_proj:1.074 [t=0.25s]
prediction: ['[CLS] unfulfilling [SEP]']
[ 450/ 500] tot_loss=1.063 (perp=4.948, rec=0.073, cos=0.000), tot_loss_proj:1.068 [t=0.25s]
prediction: ['[CLS] unfulfilling [SEP]']
Attempt swap
[ 500/ 500] tot_loss=1.049 (perp=4.948, rec=0.059, cos=0.000), tot_loss_proj:1.069 [t=0.25s]
prediction: ['[CLS] unfulfilling [SEP]']
Done with input #98 of 100.
reference: 
========================
[CLS] unfulfilling [SEP]
========================
predicted: 
========================
[CLS] unfulfilling [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 100.000 | p: 100.000 | r: 100.000
rouge2     | fm: 100.000 | p: 100.000 | r: 100.000
rougeL     | fm: 100.000 | p: 100.000 | r: 100.000
rougeLsum  | fm: 100.000 | p: 100.000 | r: 100.000
r1fm+r2fm = 200.000

[Aggregate metrics]:
rouge1     | fm: 81.000 | p: 80.136 | r: 82.284
rouge2     | fm: 42.308 | p: 41.918 | r: 42.752
rougeL     | fm: 69.287 | p: 68.694 | r: 70.195
rougeLsum  | fm: 69.164 | p: 68.508 | r: 70.099
r1fm+r2fm = 123.308

input #98 time: 0:03:28 | total time: 5:50:54


Running input #99 of 100.
reference: 
========================
walked out muttering words like `` horrible '' and `` terrible , '' but had so much fun dissing the film that they did n't mind the ticket cost 
========================
*********************************
*********************************
average of cosine similarity 0.9993095816453156
highest_index [0]
highest [0.9993095816453156]
Debug: ids_shape = 38, pads = [38]
Debug: input ids = tensor([[  101,  2939,  2041, 22581,  2616,  2066,  1036,  1036,  9202,  1005,
          1005,  1998,  1036,  1036,  6659,  1010,  1005,  1005,  2021,  2018,
          2061,  2172,  4569,  4487, 18965,  1996,  2143,  2008,  2027,  2106,
          1050,  1005,  1056,  2568,  1996,  7281,  3465,   102]],
       device='cuda:0')
Debug: ref = ["[CLS] walked out muttering words like ` ` horrible'' and ` ` terrible,'' but had so much fun dissing the film that they didn't mind the ticket cost [SEP]"]
[Init] best rec loss: 1.5378683805465698 for ['[CLS]tering aleباد were edition transmission now church around era equal enough specific mad ed alba contributors does printed sustainabilitynal gig squad bunch demonstration position should chu rankingdie outer matingurgent fenton stuff immediate [SEP]']
[Init] best rec loss: 1.4138236045837402 for ['[CLS] true rules study britain key division [SEP] o canteen flu meadows another throw beating no alignment master than fair este department profit cam endurance here media tragedy mother bird vest super lineage intersection drum { nan [SEP]']
[Init] best rec loss: 1.3704537153244019 for ['[CLS] nature pena chuck turns wig department never lay clancy synth maxi past verse my pueblo part ancient angel flash work colin sufficient vowel * scale cast energy strawberry intra lookical million bates assent laughter forth [SEP]']
[Init] best rec loss: 1.3552578687667847 for ['[CLS] football trust o guide every into integral? maybe200nington just layne what alaska priory incidentffled gymnastics manufactured lines kim survived told particularlygui discipline lonely # level pointsuna loves leaving it providence [SEP]']
[Init] best rec loss: 1.3264182806015015 for ['[CLS] jail england serves maggie point acid travis dual hell [MASK] judgment urban caledonia story lost fallsnum steps titleisinge classified mine live byron guitarists s mineral considered pow pride signage [SEP] avalon street heavily [SEP]']
[Init] best rec loss: 1.2738163471221924 for ['[CLS] claireˈ ratings cushionts bearing orient slight actually harper taste screens when thunder synonym ferns [MASK] garcia still services bet earliest re himself right barbie knowledge forced opposed currentlyaging distance te temps dentalte [SEP]']
[Init] best rec loss: 1.2734564542770386 for ["[CLS]jevic authority could victory gregory u yearquin choshing though horton else final obama authored speeding'recreation as disgust grace heroin his door pronunciationlu heel japanese nehru lower distinguished mere buenos base whether [SEP]"]
[Init] best perm rec loss: 1.2730098962783813 for ["[CLS] grace whether mere u nehrushing could recreation heroin base year distinguished victory pronunciation gregory authority disgust final as lower speeding hortonquin his'japanese though heel obamalu else buenosjevic authored cho door [SEP]"]
[Init] best perm rec loss: 1.27029550075531 for ["[CLS] final base as victory though disgust door gregory obama distinguished mere u yearquin'authored heel japanese could horton lower else nehru authority cho heroin pronunciation buenos grace whetherlujevicshing his speeding recreation [SEP]"]
[Init] best perm rec loss: 1.2686342000961304 for ["[CLS] victory speeding'year obama u whether mere heel japanesequin heroin his door buenos could else distinguished final authority authored grace nehru recreation horton lower disgust gregory pronunciation thoughlushing as chojevic base [SEP]"]
[Init] best perm rec loss: 1.2669504880905151 for ["[CLS] lower disgust his buenos as speeding year authored obama base mere distinguished grace victory whether though final heelshing heroin doorjevic nehru gregory u japanese'authority couldquin elselu pronunciation recreation cho horton [SEP]"]
[Init] best perm rec loss: 1.261819839477539 for ["[CLS] year buenos japanese though'u couldquin distinguished obama else door heel nehru authority final speeding cho whether victory as base pronunciation disgust heroinshing gregory grace hortonjevic lower recreation authoredlu his mere [SEP]"]
[Init] best perm rec loss: 1.257513165473938 for ["[CLS] grace distinguished could horton base his heroin though final victory else lower heelquin gregory obama door authority u authored'mere as speeding buenosjevic year cho whetherlu japanese recreation nehru disgust pronunciationshing [SEP]"]
[Init] best perm rec loss: 1.2564736604690552 for ["[CLS] base lower buenos gregory authority year pronunciation recreationquin nehru though disgust u speeding japanese cho victory authored grace door as could distinguished heroinshinglujevic final heel horton obama whether'his mere else [SEP]"]
[Init] best perm rec loss: 1.2494480609893799 for ["[CLS] victory u distinguished disgust gregory heroin authoredshing buenos grace else year lower heel mere finallu pronunciation'cho recreation though whetherquin door could base japanese obama his as nehrujevic speeding authority horton [SEP]"]
Nsteps: 500
[  50/ 500] tot_loss=2.662 (perp=11.725, rec=0.315, cos=0.002), tot_loss_proj:3.130 [t=0.26s]
prediction: ["[CLS] ze attempted wait shit lecture fraud could kiss torture no forgotten. worse pronouns. invitation movie worst'film battalion turned everything thomas might waste ran thinking coming film whenova a radiation cause deaths [SEP]"]
Attempt swap
Swapped tokens
[ 100/ 500] tot_loss=2.512 (perp=11.385, rec=0.233, cos=0.002), tot_loss_proj:3.233 [t=0.26s]
prediction: ["[CLS] example'bother pronouns lecture fuss liked triple teasing no forgotten but horrible shit, ticket playbackssing'film battalion fei fun engineering di ban walked really these film di cost a radiation government death [SEP]"]
[ 150/ 500] tot_loss=2.311 (perp=10.539, rec=0.201, cos=0.002), tot_loss_proj:2.818 [t=0.25s]
prediction: ["[CLS] out'coughed pronounsigate'fun di fun nothing forgotten but horrible ', ticket playbackssing ` ticket battalion fei fun equations di di walkedssing the film that cost the radiation armies the [SEP]"]
Attempt swap
Moved token
[ 200/ 500] tot_loss=2.463 (perp=11.419, rec=0.178, cos=0.001), tot_loss_proj:3.017 [t=0.25s]
prediction: ["[CLS] out saying muttering ` walkedailed ` fun di terrible of him but horrible ', ticket mindedssing ` ticket battalion fei fun equations di di gave that film'cost the radiationpartisan the [SEP]"]
Attempt swap
Swapped tokens
[ 250/ 500] tot_loss=2.343 (perp=10.961, rec=0.149, cos=0.001), tot_loss_proj:2.938 [t=0.27s]
prediction: ["[CLS] out muttering muttering ` walked cost ` fun di terrible of him but horrible `'ticket mindssing ` ticket appearance turned fun horrible di not had that film theailed the radiation worst the [SEP]"]
[ 300/ 500] tot_loss=2.264 (perp=10.666, rec=0.129, cos=0.002), tot_loss_proj:2.862 [t=0.28s]
prediction: ["[CLS] out muttering muttering ` walked cost ` fun di terrible of him but horrible `'ticket mindssing ` cost principle turning fun horrible di not had so film the bill that they they the [SEP]"]
Attempt swap
Swapped tokens
[ 350/ 500] tot_loss=2.347 (perp=11.104, rec=0.125, cos=0.001), tot_loss_proj:3.080 [t=0.26s]
prediction: ["[CLS] out muttering film ` walked why ` fun di terrible of i but horrible `'ticket mindssing ` cost habit turns fun horrible di not had so muttering thevius that they they the [SEP]"]
Attempt swap
Swapped tokens
[ 400/ 500] tot_loss=2.310 (perp=10.935, rec=0.122, cos=0.001), tot_loss_proj:3.026 [t=0.27s]
prediction: ["[CLS] out muttering film ` walked why ` minded di terrible of i but horrible `'ticket mindssing ` cost habit turning fun horrible di didn they so muttering the bigger that they had the [SEP]"]
[ 450/ 500] tot_loss=2.241 (perp=10.680, rec=0.104, cos=0.000), tot_loss_proj:2.992 [t=0.25s]
prediction: ["[CLS] out muttering film ` walked why ` minded ` terrible means i but horrible `'ticket mindssing ` cost habit turning fun horrible di t they so muttering the bigger that they had the [SEP]"]
Attempt swap
Moved token
[ 500/ 500] tot_loss=2.222 (perp=10.529, rec=0.116, cos=0.000), tot_loss_proj:2.852 [t=0.26s]
prediction: ['[CLS] out muttering film ` walked why ` minded ` terrible than0 but horrible `, ticket mindssing ` cost fun turning fun so horrible di t they muttering thevius that they had did [SEP]']
Done with input #99 of 100.
reference: 
========================
[CLS] walked out muttering words like ` ` horrible'' and ` ` terrible,'' but had so much fun dissing the film that they didn't mind the ticket cost [SEP]
========================
predicted: 
========================
[CLS] out muttering film ` walked why ` minded ` terrible than0 but horrible `, ticket mindssing ` cost fun turning fun so horrible di t they muttering thevius that they had did [SEP]
========================
[Curr input metrics]:
rouge1     | fm: 61.818 | p: 58.621 | r: 65.385
rouge2     | fm: 7.547 | p: 7.143 | r: 8.000
rougeL     | fm: 32.727 | p: 31.034 | r: 34.615
rougeLsum  | fm: 32.727 | p: 31.034 | r: 34.615
r1fm+r2fm = 69.365

[Aggregate metrics]:
rouge1     | fm: 80.779 | p: 79.915 | r: 82.059
rouge2     | fm: 42.049 | p: 41.679 | r: 42.527
rougeL     | fm: 68.840 | p: 68.235 | r: 69.783
rougeLsum  | fm: 68.941 | p: 68.300 | r: 69.883
r1fm+r2fm = 122.828

input #99 time: 0:03:29 | total time: 5:54:23


Average Cosine Similarity: 0.9992812683158312
Done with all.
